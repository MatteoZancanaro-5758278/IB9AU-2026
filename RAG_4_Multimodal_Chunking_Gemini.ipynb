{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/RAG_4_Multimodal_Chunking_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfeea611"
      },
      "source": [
        "This notebook demonstrates a **Page-Wise Multimodal Retrieval Augmented Generation (RAG)** system using Gemini-Flash and LlamaIndex. The core idea addresses the limitations of even very large context windows when dealing with massive documents or collections of documents, by switching from text-chunk retrieval to **visual page retrieval**.\n",
        "\n",
        "### The Challenge: Beyond Long Context\n",
        "Traditional RAG systems retrieve text chunks. While Large Language Models (LLMs) now boast impressive context windows (like Gemini's 2-million tokens), real-world enterprise scenarios often involve processing thousands of pages (e.g., all S&P 500 reports). In such cases, even large context windows can be insufficient, requiring a more intelligent retrieval strategy.\n",
        "\n",
        "### The Solution: Page-Wise Visual Retrieval\n",
        "Instead of sending just text, this approach extracts and sends *entire relevant pages* (including their visual layout, charts, and images) to a Vision Model. This allows the model to 'see' the information exactly as a human would, leading to more accurate and context-rich responses, especially for questions requiring interpretation of visual data.\n",
        "\n",
        "### Architecture Overview:\n",
        "1.  **Index**: We create a standard text index (using LlamaIndex) but crucially store the **page number as metadata** for each text chunk.\n",
        "2.  **Search**: A standard text search query identifies the most relevant text snippets within the documents.\n",
        "3.  **Router**: Based on the metadata, the system identifies the exact page(s) where the relevant information is located.\n",
        "4.  **Extract & Solve**: The identified page is then extracted as a mini-PDF (preserving its original visual integrity) and sent exclusively to a multimodal LLM (Gemini Flash) along with the user's query.\n",
        "\n",
        "This notebook walks through the setup, data indexing, and query process for this advanced RAG architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a628f9cb"
      },
      "source": [
        "First, we need to install all the necessary Python libraries for this project. This includes `google-genai` for interacting with the Gemini API, `llama-index-core` and related LlamaIndex packages for indexing and retrieval, and `pypdf` for PDF manipulation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Google GenAI SDK for the multimodal part\n",
        "!pip install -qU google-genai\n",
        "\n",
        "# Install LlamaIndex & Dependencies\n",
        "!pip install -qU llama-index-core llama-index-readers-file llama-index-llms-gemini llama-index-embeddings-gemini pypdf\n",
        "\n",
        "# Fix jedi version conflict flagged by pip's dependency resolver\n",
        "!pip install -qU jedi>=0.16\n",
        "\n",
        "print(\"\\u2705 Installation Complete.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LyKgtG1zbiXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1c2af8e"
      },
      "source": [
        "Next, we import the required modules from these libraries. This sets up our environment for creating the LlamaIndex, configuring the LLM and embedding models, and handling PDF files."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Imports & Custom Embedding Class\n",
        "import os\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
        "from llama_index.core.embeddings import BaseEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google import genai\n",
        "from typing import List\n",
        "from pydantic import PrivateAttr\n",
        "\n",
        "class GenAIEmbedding(BaseEmbedding):\n",
        "    _client: genai.Client = PrivateAttr()\n",
        "    _model_name: str = PrivateAttr()\n",
        "    _requests_per_minute: int = PrivateAttr()\n",
        "    _request_count: int = PrivateAttr()\n",
        "    _window_start: float = PrivateAttr()\n",
        "\n",
        "    def __init__(self, model_name: str = \"gemini-embedding-001\", requests_per_minute: int = 10, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "        self._model_name = model_name\n",
        "        self._requests_per_minute = requests_per_minute\n",
        "        self._request_count = 0\n",
        "        self._window_start = time.time()\n",
        "\n",
        "    def _rate_limit(self):\n",
        "        \"\"\"Simple rate limiter: pause if we've hit the per-minute request cap.\"\"\"\n",
        "        self._request_count += 1\n",
        "        elapsed = time.time() - self._window_start\n",
        "        if self._request_count >= self._requests_per_minute:\n",
        "            sleep_time = max(0, 10 - elapsed)\n",
        "            if sleep_time > 0:\n",
        "                print(f\"   \\u23f3 Rate limit: sleeping {sleep_time:.1f}s...\")\n",
        "                time.sleep(sleep_time)\n",
        "            # Reset window\n",
        "            self._request_count = 0\n",
        "            self._window_start = time.time()\n",
        "\n",
        "    def _get_embedding(self, text: str) -> List[float]:\n",
        "        self._rate_limit()\n",
        "        result = self._client.models.embed_content(\n",
        "            model=self._model_name,\n",
        "            contents=text\n",
        "        )\n",
        "        return result.embeddings[0].values\n",
        "\n",
        "    def _get_text_embedding(self, text: str) -> List[float]:\n",
        "        return self._get_embedding(text)\n",
        "\n",
        "    def _get_query_embedding(self, query: str) -> List[float]:\n",
        "        return self._get_embedding(query)\n",
        "\n",
        "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
        "        return self._get_embedding(query)\n",
        "\n",
        "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
        "        return self._get_embedding(text)\n",
        "\n",
        "print(\"\\u2705 Imports and GenAIEmbedding class defined.\")"
      ],
      "metadata": {
        "id": "L9RgEZ_Abqjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34182f69"
      },
      "source": [
        "To interact with Google's GenAI and LlamaIndex with Gemini models, you need to provide your Google API Key. This key is securely loaded from Colab's user data secrets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Set API Key\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Set API key in environment for LlamaIndex to pick up automatically\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "print(\"\\u2705 API Key loaded.\")"
      ],
      "metadata": {
        "id": "9X9e90lmVewG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69c4032d"
      },
      "source": [
        "Here, we configure LlamaIndex to use Google's Gemini models for both language generation (`Gemini`) and embeddings (`GenAIEmbedding`). We set these globally so all subsequent LlamaIndex operations use these powerful models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Configure LlamaIndex with Gemini Flash + Rate-Limited Embeddings\n",
        "Settings.llm = Gemini(model_name=\"models/gemini-2.0-flash\")\n",
        "\n",
        "# requests_per_minute=10 is safe for the free tier (limit is ~15 RPM).\n",
        "# If you're on a paid plan, you can raise this to 50 or higher.\n",
        "Settings.embed_model = GenAIEmbedding(model_name=\"gemini-embedding-001\", requests_per_minute=10)\n",
        "print(\"\\u2705 LlamaIndex configured.\")"
      ],
      "metadata": {
        "id": "_NYUS9gxbqdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51ab7f7a"
      },
      "source": [
        "Now, we load the earnings report PDF. The `SimpleDirectoryReader` automatically splits the PDF into individual pages, treating each page as a separate document. This is crucial for our page-wise retrieval strategy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Load & Index Data\n",
        "print(\"\\U0001f4da Loading Goldman Sachs Report (This automatically separates pages)...\")\n",
        "# SimpleDirectoryReader reads the file and creates a 'Document' for every page\n",
        "documents = SimpleDirectoryReader(input_files=[\"GS-2024-q4-earnings.pdf\"]).load_data()\n",
        "\n",
        "print(f\"   Loaded {len(documents)} pages.\")\n",
        "print(f\"   Sample Metadata: {documents[0].metadata}\")"
      ],
      "metadata": {
        "id": "x-hlHO0ddXLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Build Vector Index\n",
        "print(\"\\U0001f9e0 Building Vector Index...\")\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "retriever = index.as_retriever(similarity_top_k=1)  # Retrieve the single best page\n",
        "\n",
        "print(\"\\u2705 Index Ready!\")"
      ],
      "metadata": {
        "id": "G-QGF_6AbqZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cfd2280"
      },
      "source": [
        "This is the core of our **Visual RAG Orchestrator**. The `query_visual_rag` function performs the following steps:\n",
        "1.  **Retrieve**: Uses LlamaIndex to find the most relevant text snippet.\n",
        "2.  **Locate**: Extracts the page number metadata from the retrieved snippet.\n",
        "3.  **Extract**: Uses `pypdf` to create a mini-PDF containing only the identified page.\n",
        "4.  **Vision**: Uploads this mini-PDF to Gemini Flash, enabling the model to 'see' the entire page visually.\n",
        "5.  **Generate**: Prompts Gemini Flash to answer the user's question based on the visual and textual content of that single page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq-hYK0QS67S"
      },
      "source": [
        "# @title 6. The Visual RAG Orchestrator\n",
        "# FIX: Use genai.types.Part(text=...) instead of the deprecated Part.from_text()\n",
        "# The google-genai SDK changed the API: from_text() no longer accepts positional args.\n",
        "\n",
        "from pypdf import PdfReader, PdfWriter\n",
        "import time\n",
        "\n",
        "# Initialise the google-genai client\n",
        "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "def query_visual_rag(query_text):\n",
        "    print(f\"\\n\\u2385 Querying LlamaIndex for: '{query_text}'...\")\n",
        "\n",
        "    # 1. RETRIEVE: Ask LlamaIndex to find the best text match\n",
        "    nodes = retriever.retrieve(query_text)\n",
        "\n",
        "    if not nodes:\n",
        "        return \"\\u274c No relevant information found in the index.\"\n",
        "\n",
        "    # 2. LOCATE: Extract metadata from the best node\n",
        "    best_node = nodes[0]\n",
        "    page_label = best_node.metadata.get('page_label')\n",
        "\n",
        "    if page_label:\n",
        "        page_index = int(page_label) - 1\n",
        "    else:\n",
        "        print(\"\\u26a0\\ufe0f Page metadata missing, defaulting to Page 1\")\n",
        "        page_index = 0\n",
        "        page_label = \"1\"\n",
        "\n",
        "    print(f\"Found answer on Page {page_label} (Score: {best_node.score:.4f})\")\n",
        "    print(f\"   Context Snippet: {best_node.text[:100]}...\")\n",
        "\n",
        "    # 3. EXTRACT: Slice that specific page as a mini-PDF\n",
        "    reader = PdfReader(\"GS-2024-q4-earnings.pdf\")\n",
        "    writer = PdfWriter()\n",
        "    writer.add_page(reader.pages[page_index])\n",
        "\n",
        "    temp_filename = \"temp_visual_context.pdf\"\n",
        "    with open(temp_filename, \"wb\") as f:\n",
        "        writer.write(f)\n",
        "\n",
        "    # 4. VISION: Upload the page PDF with the google-genai SDK\n",
        "    print(\"\\U0001f680 Sending Page Visuals to Gemini Flash...\")\n",
        "    with open(temp_filename, \"rb\") as f:\n",
        "        upload_file = client.files.upload(\n",
        "            file=f,\n",
        "            config=genai.types.UploadFileConfig(mime_type=\"application/pdf\")\n",
        "        )\n",
        "\n",
        "    # Wait for processing\n",
        "    while upload_file.state.name == \"PROCESSING\":\n",
        "        time.sleep(1)\n",
        "        upload_file = client.files.get(name=upload_file.name)\n",
        "\n",
        "    # 5. GENERATE: Ask the model to look at the page\n",
        "    prompt_text = (\n",
        "        f\"You are an expert financial analyst. You are looking at Page {page_label} \"\n",
        "        \"of the GS Earnings Report. \"\n",
        "        \"Answer the user's question based on the visual charts, tables, and text on this page. \"\n",
        "        \"If the answer is in a chart, explicitly describe the visual trend.\"\n",
        "    )\n",
        "\n",
        "    # FIX: Use genai.types.Part(text=...) constructor instead of the broken\n",
        "    # Part.from_text() which changed behaviour in newer google-genai SDK versions.\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        contents=[\n",
        "            genai.types.Part(text=prompt_text),\n",
        "            genai.types.Part(\n",
        "                file_data=genai.types.FileData(\n",
        "                    file_uri=upload_file.uri,\n",
        "                    mime_type=\"application/pdf\"\n",
        "                )\n",
        "            ),\n",
        "            genai.types.Part(text=query_text)\n",
        "        ]\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "print(\"\\u2705 query_visual_rag() function defined and ready.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bafcd3c9"
      },
      "source": [
        "Let's test our Visual RAG system with a query that specifically benefits from visual information. This query asks about AUS by region, which requires interpreting a pie chart on the page."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Run Tests\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Test 1: AUS by Region\n",
        "q1 = \"Describe the AUS by region.\"\n",
        "display(Markdown(f\"### Q1: {q1}\"))\n",
        "display(Markdown(query_visual_rag(q1)))"
      ],
      "metadata": {
        "id": "H7rykjB3eOVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dab7f524"
      },
      "source": [
        "This second test query challenges the model to extract structured information from a complex table. A traditional text-only RAG might struggle, but with visual context, Gemini Flash can accurately interpret the table layout and extract the requested data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2: Summarise financial results\n",
        "q2 = \"Summarize the financial results\"\n",
        "display(Markdown(f\"### Q2: {q2}\"))\n",
        "display(Markdown(query_visual_rag(q2)))"
      ],
      "metadata": {
        "id": "chqoP3hheJ8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60877513"
      },
      "source": [
        "Finally, we run an 'Audit Mode' query designed for verification. This prompt specifically asks for revenue figures and requires citing the exact page numbers, including whether the information was derived from a chart. This helps confirm the system's accuracy and ability to pinpoint information within the visual context."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. Audit Mode (Verification)\n",
        "\n",
        "audit_prompt = \"\"\"\n",
        "Extract the exact revenue figures for FY24/25.\n",
        "For every figure you provide, cite the PAGE NUMBER where it appears.\n",
        "If the number comes from a chart, state \\\"Derived from Chart on Page X\\\".\n",
        "\"\"\"\n",
        "display(Markdown(f\"### Q: {audit_prompt}\"))\n",
        "display(Markdown(query_visual_rag(audit_prompt)))"
      ],
      "metadata": {
        "id": "nzpR_WWuZR39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}