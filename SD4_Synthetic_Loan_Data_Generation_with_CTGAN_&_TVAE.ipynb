{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPjdSsZBtpXfnQNh4vM+CZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/SD4_Synthetic_Loan_Data_Generation_with_CTGAN_%26_TVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will evaluate two techniques to generate structured synthetic data: **Tabular GAN** and **Tabular Variational Autoencoder**."
      ],
      "metadata": {
        "id": "0HZOwHSOV1YJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1: Tabular GANs\n",
        "Tabular GANs are a type of Generative Adversarial Network (GAN) specifically designed to generate synthetic tabular data (data organized in rows and columns, like a spreadsheet or a Pandas DataFrame) that closely resembles a real-world dataset. Traditional GANs were initially more successful in generating continuous data like images. Tabular data presents unique challenges due to the presence of:\n",
        "* Mixed Data Types: Tables often contain both numerical (continuous or discrete) and categorical features.\n",
        "* Complex Correlations: Features in a table can have intricate linear and non-linear relationships.\n",
        "* Unbalanced Categories: Categorical features can have classes with highly varying frequencies.\n",
        "* Discrete Values: Even numerical columns might represent discrete quantities.\n",
        "\n",
        "\n",
        "CTGAN (Conditional Tabular Generative Adversarial Network) addresses these challenges through several key innovations built upon the standard GAN architecture:\n",
        "* Generator (G):\n",
        "Takes random noise as input.\n",
        "Its goal is to generate synthetic data samples that the discriminator cannot distinguish from real data.\n",
        "It uses neural networks (typically Multi-Layer Perceptrons or MLPs) to transform the noise into synthetic tabular data.\n",
        "* Discriminator (D):\n",
        "Takes a batch of data as input, which can be a mix of real data samples from the original dataset and synthetic data samples generated by the generator.\n",
        "Its goal is to correctly classify each input sample as either \"real\" or \"synthetic.\"\n",
        "It also uses neural networks (MLPs) for this classification task.\n",
        "* Adversarial Training:\n",
        "The generator and discriminator are trained in an adversarial manner.\n",
        "The generator tries to fool the discriminator by producing increasingly realistic synthetic data.\n",
        "The discriminator tries to become better at distinguishing real from synthetic data.\n",
        "This competition drives both networks to improve, ideally leading the generator to produce synthetic data that is statistically very similar to the real data.\n",
        "\n",
        "\n",
        "In essence, CTGAN aims to learn the underlying data generation process of  tabular dataset by training a generator to produce synthetic data that fools a discriminator trained to distinguish it from the real data."
      ],
      "metadata": {
        "id": "G356022i4Yub"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QGbFOPi401WX"
      },
      "outputs": [],
      "source": [
        "!pip install sdv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "from scipy.stats import ks_2samp\n",
        "from sklearn.neighbors import NearestNeighbors\n"
      ],
      "metadata": {
        "id": "bZzZs8CA10Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load and inspect `AnguloM/loan_data`\n",
        "\n",
        "The dataset is a LendingClub‑style consumer loan dataset hosted on Hugging Face.  \n",
        "Key fields (from the dataset card):\n",
        "\n",
        "- `not.fully.paid`: **outcome** – 1 if the loan was *not* fully repaid (default/charge‑off), 0 otherwise. [web:111]\n",
        "- `credit.policy`: 1 if the customer meets LendingClub's underwriting criteria.\n",
        "- `purpose`: loan purpose (debt_consolidation, credit_card, etc.).\n",
        "- Numeric features: `int.rate`, `installment`, `log.annual.inc`, `dti`, `fico`, `days.with.cr.line`, `revol.bal`, `revol.util`, `inq.last.6mths`, `delinq.2yrs`, `pub.rec`. [web:109][web:142]\n",
        "\n",
        "We will:\n",
        "1. Load the dataset.\n",
        "2. Inspect schema and basic statistics.\n",
        "3. Confirm class balance of `not.fully.paid`.\n"
      ],
      "metadata": {
        "id": "s0ReNhCg1-av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loan_ds = load_dataset(\"AnguloM/loan_data\")\n",
        "df = loan_ds[\"train\"].to_pandas()\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "vp6HBjf72B0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n"
      ],
      "metadata": {
        "id": "2KqGekO62L-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"not.fully.paid\"].value_counts(normalize=True)\n"
      ],
      "metadata": {
        "id": "6QPWkwxH2Pmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing with `not.fully.paid` as outcome\n",
        "\n",
        "Our prediction / label variable is:\n",
        "\n",
        "- `not.fully.paid` (1 = default / not fully repaid, 0 = fully paid).\n",
        "\n",
        "We define three groups of columns:\n",
        "\n",
        "- **Target:** `not.fully.paid`\n",
        "- **Categorical features:** `purpose`, `credit.policy` (treated as discrete category).\n",
        "- **Numeric features:** rate, installment, income, FICO, etc.\n",
        "\n",
        "Steps:\n",
        "1. Drop rows with missing values (dataset is usually clean, but we are defensive).\n",
        "2. Split into features `X` and target `y`.\n"
      ],
      "metadata": {
        "id": "MnZaMW9l2XnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop any NA rows to simplify the lab\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "target_col = \"not.fully.paid\"\n",
        "\n",
        "cat_cols = [\"purpose\", \"credit.policy\"]\n",
        "num_cols = [\n",
        "    \"int.rate\",\n",
        "    \"installment\",\n",
        "    \"log.annual.inc\",\n",
        "    \"dti\",\n",
        "    \"fico\",\n",
        "    \"days.with.cr.line\",\n",
        "    \"revol.bal\",\n",
        "    \"revol.util\",\n",
        "    \"inq.last.6mths\",\n",
        "    \"delinq.2yrs\",\n",
        "    \"pub.rec\"\n",
        "]\n",
        "\n",
        "X = df[cat_cols + num_cols].copy()\n",
        "y = df[target_col].astype(int)\n",
        "\n",
        "df[[target_col]].value_counts(normalize=True)\n"
      ],
      "metadata": {
        "id": "4glgYzdE2ZEf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train/test split on real data\n",
        "\n",
        "We will later:\n",
        "\n",
        "- Train a classifier on **real data** (baseline).\n",
        "- Train a classifier on **synthetic data** and test on **real data** (TSTR).\n",
        "\n",
        "To do this, we split the real dataset into `train` and `test` with stratification on `not.fully.paid`.\n"
      ],
      "metadata": {
        "id": "lEh2LreE2mkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real_train, real_test = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[target_col]\n",
        ")\n",
        "\n",
        "real_train.shape, real_test.shape\n"
      ],
      "metadata": {
        "id": "Ittourl92mQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train CTGAN to generate synthetic loans\n",
        "\n",
        "We will use **CTGAN**, a GAN variant designed specifically for mixed‑type tabular data.\n",
        "\n",
        "Key design choices:\n",
        "\n",
        "- Input to CTGAN: all feature columns **plus** the outcome `not.fully.paid`.\n",
        "- `discrete_columns` parameter: includes all categorical fields and integer count features, including the binary outcome.\n",
        "\n",
        "This lets us later **condition** on `not.fully.paid` if we want to oversample defaulted loans.\n"
      ],
      "metadata": {
        "id": "zLrWdiey2vSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ctgan import CTGAN\n",
        "\n",
        "ctgan_data = df[cat_cols + num_cols + [target_col]].copy()\n",
        "\n",
        "discrete_cols = cat_cols + [\"inq.last.6mths\", \"delinq.2yrs\", \"pub.rec\", target_col]\n",
        "\n",
        "ctgan = CTGAN(\n",
        "    epochs=300,       # increase for better quality if you have time/compute\n",
        "    batch_size=500,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "ctgan.fit(ctgan_data, discrete_columns=discrete_cols)"
      ],
      "metadata": {
        "id": "0-87TpZX2yt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Generate synthetic data\n",
        "\n",
        "Let's generate 10,000 synthetic loan records.\n",
        "\n",
        "We will inspect:\n",
        "\n",
        "- Schema (column names, dtypes).\n",
        "- Class balance for `not.fully.paid` in synthetic data vs. real data.\n"
      ],
      "metadata": {
        "id": "6u5bm5na3e6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_synth = 10_000\n",
        "synthetic = ctgan.sample(n_synth)\n",
        "synthetic.head()\n"
      ],
      "metadata": {
        "id": "ZYioX8kO3gge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic.info()\n"
      ],
      "metadata": {
        "id": "u_SzFnXK3mEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Real outcome distribution:\")\n",
        "print(df[target_col].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nSynthetic outcome distribution:\")\n",
        "print(synthetic[target_col].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "qS4GB4y_3pI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Optional: condition on defaults (`not.fully.paid = 1`)\n",
        "\n",
        "We can ask CTGAN to specifically generate records where the loan is **not fully paid**, which is useful for oversampling the rare default class.\n"
      ],
      "metadata": {
        "id": "ymEahmU13y4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a larger number of synthetic samples with a strong bias towards the condition\n",
        "\n",
        "large_synthetic_sample = ctgan.sample(5000)\n",
        "\n",
        "# Filter to keep only the records where 'not.fully.paid' is 1\n",
        "# Then sample 2000 from this filtered set. Using replace=True to handle cases where fewer than 2000 are initially generated.\n",
        "synthetic_defaults = large_synthetic_sample[large_synthetic_sample[target_col] == 1].sample(n=2000, random_state=42, replace=True)\n",
        "\n",
        "synthetic_defaults[target_col].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "u_i2Pbdo30ul",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Is the Synthetic Data Trustworthy?\n",
        "\n",
        "We will evaluate the generated synthetic data set along the three pillars of quality.\n",
        "\n",
        "1. **Fidelity**: Does the synthetic data statistically resemble the real data?\n",
        "2. **Utility**: Can a machine learning algorithm trained on synthetic data perform well on real data?\n",
        "3. **Privacy**: Can we guarantee that the synthetic data does not expose sentitive information from the real data?"
      ],
      "metadata": {
        "id": "ldUluR-4Urii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Fidelity: do synthetic and real loans look statistically similar?\n",
        "\n",
        "We evaluate fidelity for numeric columns based on individual columns and pairwise correlations\n",
        "\n",
        "#### 6.1.1 Univariate Fidelity:\n",
        "\n",
        "- For each numeric column, run a **Kolmogorov–Smirnov (KS) test** comparing real vs synthetic samples.\n",
        "- KS statistic near 0 ⇒ distributions are similar.\n",
        "- Higher KS ⇒ synthetic deviates from real.\n",
        "\n",
        "We do this feature by feature.\n"
      ],
      "metadata": {
        "id": "2quU17l839Qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real = ctgan_data\n",
        "syn = synthetic\n",
        "\n",
        "ks_results = {}\n",
        "\n",
        "for col in num_cols:\n",
        "    r = real[col].sample(min(5000, len(real)), random_state=42)\n",
        "    s = syn[col].sample(min(5000, len(syn)), random_state=42)\n",
        "    stat, pval = ks_2samp(r, s)\n",
        "    ks_results[col] = {\"ks_stat\": stat, \"p_value\": pval}\n",
        "\n",
        "ks_df = pd.DataFrame(ks_results).T.sort_values(\"ks_stat\")\n",
        "ks_df\n"
      ],
      "metadata": {
        "id": "h7qFRCtu3-QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "\n",
        "- Which features have the **lowest** KS (best‑matched distributions)?\n",
        "- Which features are hardest for CTGAN to mimic (highest KS)?\n",
        "\n"
      ],
      "metadata": {
        "id": "2HX2MkPj4Bby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###' 6.1.2 Correlation Structure\n",
        "\n",
        "We now compare **pairwise correlations** between numerical features in real vs synthetic data.\n",
        "\n",
        "- Compute correlation matrices for real and synthetic numeric features.\n",
        "- Look at absolute differences between them.\n"
      ],
      "metadata": {
        "id": "4PzrqI9G4MDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real_corr = real[num_cols].corr()\n",
        "syn_corr = syn[num_cols].corr()\n",
        "\n",
        "corr_diff = (real_corr - syn_corr).abs()\n",
        "corr_diff\n"
      ],
      "metadata": {
        "id": "b65E-FYs4O1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average absolute difference in correlation per feature\n",
        "corr_diff.mean().sort_values(ascending=False)\n"
      ],
      "metadata": {
        "id": "AyHIVaOL4R-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Utility: can synthetic data train a useful default model?\n",
        "\n",
        "We evaluate **utility** using the TSTR protocol:\n",
        "\n",
        "1. Fit a classifier on **synthetic** data.\n",
        "2. Test on **held‑out real** data.\n",
        "3. Compare performance with a classifier trained on **real** data (upper bound).\n",
        "\n",
        "Metrics:\n",
        "\n",
        "- ROC AUC\n",
        "- F1‑score (for imbalanced classification)\n"
      ],
      "metadata": {
        "id": "1ta_q8V94ZKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.1 Create encoders/scaler on REAL training data\n",
        "\n",
        "We will:\n",
        "\n",
        "- Fit **OneHotEncoder** and **StandardScaler** only on the **real training** subset.\n",
        "- Apply exactly the same transformations to synthetic data and real test data.\n"
      ],
      "metadata": {
        "id": "Zj3kmrA44g7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit on REAL TRAINING DATA ONLY\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "scaler = StandardScaler()\n",
        "\n",
        "ohe.fit(real_train[cat_cols])\n",
        "scaler.fit(real_train[num_cols])\n",
        "\n",
        "def preprocess_for_model(df_subset):\n",
        "    X_cat = df_subset[cat_cols]\n",
        "    X_num = df_subset[num_cols]\n",
        "    y_out = df_subset[target_col].astype(int)\n",
        "\n",
        "    X_cat_enc = ohe.transform(X_cat)\n",
        "    X_num_scaled = scaler.transform(X_num)\n",
        "\n",
        "    X_all = np.hstack([X_cat_enc, X_num_scaled])\n",
        "    return X_all, y_out\n"
      ],
      "metadata": {
        "id": "Tr5X1fB34d1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.2 Baseline: Train on REAL, Test on REAL\n",
        "\n",
        "This is our **upper bound** for performance.\n"
      ],
      "metadata": {
        "id": "Jpd5mWyp4l05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_real_train, y_real_train = preprocess_for_model(real_train)\n",
        "X_real_test, y_real_test = preprocess_for_model(real_test)\n",
        "\n",
        "rf_real = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_real.fit(X_real_train, y_real_train)\n",
        "\n",
        "y_proba_real = rf_real.predict_proba(X_real_test)[:, 1]\n",
        "y_pred_real = (y_proba_real >= 0.5).astype(int)\n",
        "\n",
        "auc_real = roc_auc_score(y_real_test, y_proba_real)\n",
        "f1_real = f1_score(y_real_test, y_pred_real)\n",
        "\n",
        "auc_real, f1_real\n"
      ],
      "metadata": {
        "id": "d8ncW0VW4tLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.3 TSTR: Train on SYNTHETIC, Test on REAL\n",
        "\n",
        "We now:\n",
        "\n",
        "- Use our `synthetic` dataframe as training data.\n",
        "- Evaluate on the **same real test set** as above.\n",
        "\n",
        "This tells us how good models trained purely on synthetic data are for predicting `not.fully.paid` on real loans.\n"
      ],
      "metadata": {
        "id": "_KvLzhMx4xJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_syn_train, y_syn_train = preprocess_for_model(synthetic)\n",
        "\n",
        "rf_syn = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_syn.fit(X_syn_train, y_syn_train)\n",
        "\n",
        "y_proba_syn = rf_syn.predict_proba(X_real_test)[:, 1]\n",
        "y_pred_syn = (y_proba_syn >= 0.5).astype(int)\n",
        "\n",
        "auc_syn = roc_auc_score(y_real_test, y_proba_syn)\n",
        "f1_syn = f1_score(y_real_test, y_pred_syn)\n",
        "\n",
        "auc_syn, f1_syn\n"
      ],
      "metadata": {
        "id": "5HIebzAh4yxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.4 Compare utility: TRTR vs TSTR\n"
      ],
      "metadata": {
        "id": "2clpv_7d42M3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(\n",
        "    {\n",
        "        \"AUC\": [auc_real, auc_syn],\n",
        "        \"F1\": [f1_real, f1_syn]\n",
        "    },\n",
        "    index=[\"Train REAL, Test REAL\", \"Train SYNTHETIC, Test REAL\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "YEoUUNVw47sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Privacy: Approximate Memorization Check\n",
        "\n",
        "CTGAN can overfit and memorize real rows, which is a privacy risk.\n",
        "\n",
        "A simple heuristic:\n",
        "- Take numeric features from synthetic data.\n",
        "- For each synthetic point, compute the distance to the **nearest real point**.\n",
        "- If many synthetic points are at extremely small distance, it may indicate memorization.\n",
        "\n",
        "This is not a formal privacy guarantee, but a useful metric.\n"
      ],
      "metadata": {
        "id": "mBQzfmpq5D8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample down for speed\n",
        "real_num = real[num_cols].sample(5000, random_state=42)\n",
        "syn_num = syn[num_cols].sample(5000, random_state=42)\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=1)\n",
        "nn.fit(real_num)\n",
        "\n",
        "distances, indices = nn.kneighbors(syn_num)\n",
        "distances = distances.flatten()\n",
        "\n",
        "pd.Series(distances).describe()\n"
      ],
      "metadata": {
        "id": "93H2X0fK5FSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "\n",
        "- Very small distances (e.g., many < 1e-6 after scaling) might indicate memorization.\n",
        "- Larger distances suggest synthetic records are not exact copies.\n",
        "\n",
        "For a production‑grade system, you would combine such checks with more formal privacy metrics (e.g., membership inference tests, differential privacy variants of CTGAN).\n"
      ],
      "metadata": {
        "id": "NUwqmU7F5JLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. What did we learn?\n",
        "\n",
        "In this notebook we:\n",
        "\n",
        "1. Treated **`not.fully.paid` as the key outcome** for loan default risk.\n",
        "2. Trained **CTGAN** on mixed‑type loan data to generate synthetic loans.\n",
        "3. Evaluated **fidelity**:\n",
        "   - KS test per numeric feature.\n",
        "   - Correlation structure differences.\n",
        "4. Evaluated **utility** via **Train‑Synthetic‑Test‑Real (TSTR)** against a real‑trained baseline.\n",
        "5. Ran a simple **privacy heuristic** using nearest‑neighbor distances.\n",
        "\n"
      ],
      "metadata": {
        "id": "vFxR6qU25RQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2 -  TVAE: Tabular Variational Autoencoder\n",
        "\n",
        "We now train **TVAE**, another SDV model for tabular data.  \n",
        "TVAE models the joint distribution using a variational autoencoder instead of an adversarial game.\n",
        "\n",
        "We will:\n",
        "\n",
        "1. Train TVAE on the same columns as CTGAN.\n",
        "2. Generate synthetic loans.\n",
        "3. Reuse the same evaluation pipeline (fidelity + TSTR utility).\n"
      ],
      "metadata": {
        "id": "7bnX1b5p6DwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sdv.single_table import TVAESynthesizer"
      ],
      "metadata": {
        "id": "c9ERImAN6IB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sdv.metadata import SingleTableMetadata\n",
        "\n",
        "tvae_data = df[cat_cols + num_cols + [target_col]].copy()\n",
        "\n",
        "discrete_cols = cat_cols + [\"inq.last.6mths\", \"delinq.2yrs\", \"pub.rec\", target_col]\n",
        "\n",
        "# Create metadata from the training data\n",
        "metadata = SingleTableMetadata()\n",
        "metadata.detect_from_dataframe(tvae_data)\n",
        "\n",
        "# Explicitly set discrete columns in the metadata\n",
        "for col in discrete_cols:\n",
        "    metadata.update_column(column_name=col, sdtype='categorical')\n",
        "\n",
        "tvae = TVAESynthesizer(\n",
        "    metadata=metadata,\n",
        "    epochs=300,         # similar budget to CTGAN for fairness\n",
        "    batch_size=500\n",
        ")\n",
        "\n",
        "tvae.fit(tvae_data)"
      ],
      "metadata": {
        "id": "b2NJr5F96I0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate synthetic loans with TVAE\n",
        "\n",
        "We generate the same number of records (10,000) to make comparisons fair.\n"
      ],
      "metadata": {
        "id": "xEtVoXNR6Nsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_synth = 10_000\n",
        "synthetic_tvae = tvae.sample(n_synth)\n",
        "\n",
        "synthetic_tvae.head()\n"
      ],
      "metadata": {
        "id": "2RVupk986OiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TVAE synthetic outcome distribution:\")\n",
        "print(synthetic_tvae[target_col].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "3OAA_-gc6RIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fidelity: CTGAN vs TVAE (KS test)\n",
        "\n",
        "We compute the KS statistic per numeric feature for:\n",
        "\n",
        "- Real vs **CTGAN** synthetic\n",
        "- Real vs **TVAE** synthetic\n",
        "\n",
        "Lower KS ⇒ closer match to real marginal distribution. [web:42][web:140]\n"
      ],
      "metadata": {
        "id": "6I6eXxAe6UG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real = df[cat_cols + num_cols + [target_col]].copy()\n",
        "\n",
        "def ks_per_feature(real_df, syn_df, num_cols):\n",
        "    results = {}\n",
        "    for col in num_cols:\n",
        "        r = real_df[col].sample(min(5000, len(real_df)), random_state=42)\n",
        "        s = syn_df[col].sample(min(5000, len(syn_df)), random_state=42)\n",
        "        stat, pval = ks_2samp(r, s)\n",
        "        results[col] = {\"ks_stat\": stat, \"p_value\": pval}\n",
        "    return pd.DataFrame(results).T\n",
        "\n",
        "ks_ctgan = ks_per_feature(real, synthetic, num_cols)\n",
        "ks_tvae  = ks_per_feature(real, synthetic_tvae, num_cols)\n",
        "\n",
        "ks_compare = pd.DataFrame({\n",
        "    \"KS_CTGAN\": ks_ctgan[\"ks_stat\"],\n",
        "    \"KS_TVAE\": ks_tvae[\"ks_stat\"]\n",
        "}).sort_values(\"KS_CTGAN\")\n",
        "\n",
        "ks_compare\n"
      ],
      "metadata": {
        "id": "qUF9R9986Xl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can quickly see:\n",
        "\n",
        "- Which model better fits each numeric feature.\n",
        "- Whether one model tends to systematically have lower KS across features.\n"
      ],
      "metadata": {
        "id": "szQBugbc6eSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation structure: CTGAN vs TVAE\n",
        "\n",
        "We compare correlation matrices as before, now for both synthesizers. [web:143]\n"
      ],
      "metadata": {
        "id": "YB0QK7oe6hlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real_corr = real[num_cols].corr()\n",
        "ctgan_corr = synthetic[num_cols].corr()\n",
        "tvae_corr  = synthetic_tvae[num_cols].corr()\n",
        "\n",
        "ctgan_corr_diff = (real_corr - ctgan_corr).abs()\n",
        "tvae_corr_diff  = (real_corr - tvae_corr).abs()\n",
        "\n",
        "corr_compare = pd.DataFrame({\n",
        "    \"mean_abs_diff_CTGAN\": ctgan_corr_diff.mean(),\n",
        "    \"mean_abs_diff_TVAE\": tvae_corr_diff.mean()\n",
        "}).sort_values(\"mean_abs_diff_CTGAN\")\n",
        "\n",
        "corr_compare\n"
      ],
      "metadata": {
        "id": "wYh4R9-26kb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility: TSTR for CTGAN vs TVAE\n",
        "\n",
        "We reuse the same **Train‑Synthetic‑Test‑Real** pipeline: [web:39][web:148]\n",
        "\n",
        "- Encoders (`ohe`) and `scaler` were fit on **real_train**.\n",
        "- `preprocess_for_model` converts any dataframe to model‑ready `X`, `y`.\n"
      ],
      "metadata": {
        "id": "pX4E6cv86oML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tvae_train, y_tvae_train = preprocess_for_model(synthetic_tvae)\n",
        "\n",
        "rf_tvae = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_tvae.fit(X_tvae_train, y_tvae_train)\n",
        "\n",
        "y_proba_tvae = rf_tvae.predict_proba(X_real_test)[:, 1]\n",
        "y_pred_tvae = (y_proba_tvae >= 0.5).astype(int)\n",
        "\n",
        "auc_tvae = roc_auc_score(y_real_test, y_proba_tvae)\n",
        "f1_tvae = f1_score(y_real_test, y_pred_tvae)\n",
        "\n",
        "auc_tvae, f1_tvae\n"
      ],
      "metadata": {
        "id": "W_5orSGZ6phH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare TRTR, CTGAN‑TSTR, TVAE‑TSTR\n"
      ],
      "metadata": {
        "id": "_2Nb5FJP6xN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "utility_df = pd.DataFrame(\n",
        "    {\n",
        "        \"AUC\": [auc_real, auc_syn, auc_tvae],\n",
        "        \"F1\":  [f1_real,  f1_syn,  f1_tvae]\n",
        "    },\n",
        "    index=[\n",
        "        \"Train REAL, Test REAL\",\n",
        "        \"Train CTGAN, Test REAL\",\n",
        "        \"Train TVAE, Test REAL\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "utility_df\n"
      ],
      "metadata": {
        "id": "Wz3ML-wg6zxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion prompts:\n",
        "\n",
        "- Which synthesizer gives a classifier whose AUC/F1 is closer to the **real‑trained** baseline?\n",
        "- Are there differences in calibration or class balance that might explain performance? [web:148][web:143]\n"
      ],
      "metadata": {
        "id": "bKMCFROn62fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Privacy heuristic: nearest‑neighbor distance (CTGAN vs TVAE)\n",
        "\n",
        "We reuse the **nearest‑neighbor distance** approach to compare memorization risk for the two models. [web:140][web:146]\n"
      ],
      "metadata": {
        "id": "UtimivDz65bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample real numeric subset for reference\n",
        "real_num = real[num_cols].sample(5000, random_state=42)\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=1)\n",
        "nn.fit(real_num)\n",
        "\n",
        "# CTGAN\n",
        "syn_ctgan_num = synthetic[num_cols].sample(5000, random_state=42)\n",
        "dist_ctgan, _ = nn.kneighbors(syn_ctgan_num)\n",
        "dist_ctgan = dist_ctgan.flatten()\n",
        "\n",
        "# TVAE\n",
        "syn_tvae_num = synthetic_tvae[num_cols].sample(5000, random_state=42)\n",
        "dist_tvae, _ = nn.kneighbors(syn_tvae_num)\n",
        "dist_tvae = dist_tvae.flatten()\n",
        "\n",
        "privacy_df = pd.DataFrame(\n",
        "    {\n",
        "        \"CTGAN_dist\": dist_ctgan,\n",
        "        \"TVAE_dist\": dist_tvae\n",
        "    }\n",
        ")\n",
        "\n",
        "privacy_df.describe()\n"
      ],
      "metadata": {
        "id": "4m5L3-pH66au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "\n",
        "- Higher typical nearest‑neighbor distances suggest less memorization.\n",
        "- If one model has consistently much smaller distances, it may be overfitting more to specific real points. [web:140][web:146]\n"
      ],
      "metadata": {
        "id": "vJcfjfAI6_fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CTGAN vs TVAE: what to observe\n",
        "\n",
        "\n",
        "1. **Fidelity (KS & correlation):**\n",
        "   - For which features does CTGAN best mimic the real distribution?\n",
        "   - Where does TVAE do better?\n",
        "   - Is one model more consistent across features?\n",
        "\n",
        "2. **Utility (TSTR AUC & F1):**\n",
        "   - Which synthetic dataset produces more useful classifiers for predicting `not.fully.paid`?\n",
        "   - How close are they to the real‑trained baseline?\n",
        "\n",
        "3. **Privacy heuristic (NN distances):**\n",
        "   - Does either model appear to memorize real records more?\n",
        "\n",
        "This gives a concrete, model‑agnostic way to discuss **fidelity–utility–privacy trade‑offs** across two popular tabular synthesizers on a realistic loan default problem.\n"
      ],
      "metadata": {
        "id": "XWEMw3m17DoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Required Task 13\n",
        "Load the file `fraud_transactions.csv` and create a synthetic data set of 5000 records. Evaluate the quality of the synthetic data created."
      ],
      "metadata": {
        "id": "OLrbF3ap9m2B"
      }
    }
  ]
}