{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/RAG_5_Multimodal_Chunking_OpenSourced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_md"
      },
      "source": [
        "This notebook demonstrates a **Page-Wise Multimodal Retrieval Augmented Generation (RAG)** system using open-source HuggingFace models and LlamaIndex — no API keys or rate limits required.\n",
        "\n",
        "### The Challenge: Beyond Long Context\n",
        "Traditional RAG systems retrieve text chunks. Even with large context windows, processing thousands of pages (e.g., entire document collections) requires a smarter retrieval strategy.\n",
        "\n",
        "### The Solution: Page-Wise Visual Retrieval\n",
        "Instead of sending just text, this approach extracts and sends *entire relevant pages* as images to a Vision-Language Model (VLM). This allows the model to 'see' charts, tables, and layout exactly as a human analyst would.\n",
        "\n",
        "### Architecture:\n",
        "1. **Index** — LlamaIndex embeds each page's text with a local sentence-transformer model.\n",
        "2. **Search** — Semantic search identifies the best matching page for a query.\n",
        "3. **Render** — `pdf2image` converts that page to a PIL image.\n",
        "4. **VLM** — Qwen2.5-VL-3B-Instruct reads the image and answers the question.\n",
        "\n",
        "> ⚠️ **IMPORTANT**: This notebook requires a **T4 GPU** runtime.  \n",
        "> Go to **Runtime → Change runtime type → T4 GPU** before running any cells.  \n",
        "> Running on CPU will cause generation to take 5–10 minutes per query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_md"
      },
      "source": [
        "## ⚠️ Step 1: Upgrade Pillow and Restart Runtime\n",
        "\n",
        "Colab ships with an outdated version of Pillow that causes an `ImportError: cannot import name '_Ink'` when loading the HuggingFace embedding model. **You must upgrade Pillow first and then restart the runtime before running any other cells.**\n",
        "\n",
        "**Instructions:**\n",
        "1. Run the cell below.\n",
        "2. Go to **Runtime → Restart session**.\n",
        "3. Then continue running cells from Step 2 onwards — do **not** re-run this cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Upgrade Pillow FIRST — restart runtime after this cell\n",
        "!pip install -qU Pillow\n",
        "print(\"\\u2705 Pillow upgraded. Now go to Runtime \\u2192 Restart session, then continue from Step 2.\")"
      ],
      "metadata": {
        "id": "step1_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_md"
      },
      "source": [
        "## Step 2: Verify GPU and Install Dependencies\n",
        "\n",
        "After restarting the runtime, run this cell. It first **verifies a GPU is available** — if not, it will warn you before wasting time loading a 3B model onto CPU.\n",
        "\n",
        "Then it installs:\n",
        "- `transformers` + `accelerate` — for loading Qwen2.5-VL\n",
        "- `llama-index-core`, `llama-index-readers-file`, `llama-index-embeddings-huggingface` — for indexing\n",
        "- `pdf2image` + `poppler-utils` — for rendering PDF pages as images\n",
        "- `pypdf` — for reading PDF metadata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX 1: Check GPU is available BEFORE loading the model.\n",
        "# On CPU, Qwen2.5-VL-3B generates ~1-2 tokens/sec = 5-10 min per query.\n",
        "# On T4 GPU, it generates ~30-50 tokens/sec = ~15 sec per query.\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"\\u2705 GPU detected: {gpu_name} ({vram_gb:.1f} GB VRAM)\")\n",
        "else:\n",
        "    print(\"\\u274c NO GPU DETECTED!\")\n",
        "    print(\"   Go to Runtime \\u2192 Change runtime type \\u2192 T4 GPU\")\n",
        "    print(\"   Running on CPU will make each query take 5-10 minutes.\")\n",
        "    raise RuntimeError(\"GPU required. Please change runtime type to T4 GPU and re-run.\")\n",
        "\n",
        "!pip install -qU transformers accelerate\n",
        "!pip install -qU llama-index-core llama-index-readers-file llama-index-embeddings-huggingface pypdf\n",
        "!pip install -qU pdf2image\n",
        "!apt-get install -q poppler-utils\n",
        "\n",
        "print(\"\\u2705 Installation Complete.\")"
      ],
      "metadata": {
        "id": "step2_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_md"
      },
      "source": [
        "## Step 3: Load the Embedding Model\n",
        "\n",
        "`all-MiniLM-L6-v2` is a compact sentence-transformer that converts text into 384-dimensional embeddings for semantic similarity search. It runs on CPU and requires no API key."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "print(\"\\u2705 Embedding model loaded.\")"
      ],
      "metadata": {
        "id": "step3_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_md"
      },
      "source": [
        "## Step 4: Load the Vision-Language Model (VLM)\n",
        "\n",
        "`Qwen2.5-VL-3B-Instruct` is a 3B open-source vision-language model. At `float16` it fits on a T4 GPU (~7.5 GB VRAM) and generates responses in ~15 seconds per query.\n",
        "\n",
        "The key fix here is explicitly setting `device_map='cuda'` to ensure the model always loads onto the GPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "\n",
        "VLM_MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "print(f\"\\u23f3 Loading {VLM_MODEL_ID}...\")\n",
        "print(f\"   Device: {'CUDA - ' + torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU (WARNING: will be very slow)'}\")\n",
        "\n",
        "# FIX 2: Explicitly use 'cuda' instead of 'auto' to guarantee GPU placement.\n",
        "# 'device_map=\"auto\"' can silently fall back to CPU if VRAM seems insufficient,\n",
        "# causing generation to take 5-10 minutes per query with no error or warning.\n",
        "vlm_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    VLM_MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\"   # <-- FIXED: was 'auto', now explicitly 'cuda'\n",
        ")\n",
        "vlm_processor = AutoProcessor.from_pretrained(VLM_MODEL_ID)\n",
        "\n",
        "# Confirm where the model actually landed\n",
        "model_device = next(vlm_model.parameters()).device\n",
        "print(f\"\\u2705 VLM loaded on: {model_device}\")\n",
        "if str(model_device) == 'cpu':\n",
        "    print(\"\\u26a0\\ufe0f  WARNING: Model is on CPU. Each query will take 5-10 minutes!\")"
      ],
      "metadata": {
        "id": "step4_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_md"
      },
      "source": [
        "## Step 5: Configure LlamaIndex Settings\n",
        "\n",
        "`Settings.llm = None` because LlamaIndex's built-in LLM is only used for query synthesis — our visual QA is handled directly by the Qwen VLM."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.embed_model = embed_model\n",
        "Settings.llm = None\n",
        "print(\"\\u2705 LlamaIndex settings configured.\")"
      ],
      "metadata": {
        "id": "step5_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6_md"
      },
      "source": [
        "## Step 6: Load the PDF\n",
        "\n",
        "`SimpleDirectoryReader` reads the PDF and creates one `Document` per page, each carrying page number metadata (e.g. `{'page_label': '5'}`). This page label is how we know which page to render for the VLM."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_FILE = \"GS-2024-q4-earnings.pdf\"\n",
        "\n",
        "print(f\"\\U0001f4da Loading {PDF_FILE}...\")\n",
        "documents = SimpleDirectoryReader(input_files=[PDF_FILE]).load_data()\n",
        "\n",
        "print(f\"   Loaded {len(documents)} pages.\")\n",
        "print(f\"   Sample Metadata: {documents[0].metadata}\")"
      ],
      "metadata": {
        "id": "step6_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7_md"
      },
      "source": [
        "## Step 7: Build the Vector Index\n",
        "\n",
        "LlamaIndex embeds each page's text using `all-MiniLM-L6-v2` and stores the vectors in-memory. The retriever is configured to return the single best-matching page per query."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\U0001f9e0 Building Vector Index...\")\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "retriever = index.as_retriever(similarity_top_k=1)\n",
        "print(\"\\u2705 Index Ready!\")"
      ],
      "metadata": {
        "id": "step7_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8_md"
      },
      "source": [
        "## Step 8: The Visual RAG Orchestrator\n",
        "\n",
        "The `query_visual_rag` function ties everything together:\n",
        "1. **Retrieve** — LlamaIndex finds the best matching page by text similarity.\n",
        "2. **Locate** — The page number is extracted from metadata.\n",
        "3. **Render** — `pdf2image` renders that PDF page to a PIL image at 150 DPI.\n",
        "4. **Prompt** — Image + question are formatted into a Qwen2.5-VL chat prompt.\n",
        "5. **Generate** — The VLM generates an answer, with a token limit to prevent runaway generation.\n",
        "\n",
        "**Key fixes applied:**\n",
        "- `device_map='cuda'` ensures GPU is always used (see Step 4).\n",
        "- `max_new_tokens=256` keeps generation fast — increase if you need longer answers.\n",
        "- A device check prints a warning if somehow CPU is being used.\n",
        "- Fixed the prompt: was incorrectly labelled 'JLR Annual Report', now 'GS Earnings Report'."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader, PdfWriter\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "def query_visual_rag(query_text, max_new_tokens=256):\n",
        "    print(f\"\\n\\U0001f50e Querying LlamaIndex for: '{query_text}'...\")\n",
        "\n",
        "    # FIX 3: Warn if model is on CPU before spending time on generation.\n",
        "    model_device = next(vlm_model.parameters()).device\n",
        "    if str(model_device) == 'cpu':\n",
        "        print(\"\\u26a0\\ufe0f  WARNING: VLM is running on CPU. This will be very slow (5-10 min).\")\n",
        "        print(\"   Change runtime to T4 GPU and restart for fast inference.\")\n",
        "\n",
        "    # 1. RETRIEVE: semantic search over page text\n",
        "    nodes = retriever.retrieve(query_text)\n",
        "    if not nodes:\n",
        "        return \"\\u274c No relevant information found in the index.\"\n",
        "\n",
        "    # 2. LOCATE: get page number from metadata\n",
        "    best_node = nodes[0]\n",
        "    page_label = best_node.metadata.get('page_label')\n",
        "    page_index = int(page_label) - 1 if page_label else 0\n",
        "\n",
        "    print(f\"\\U0001f4cd Found answer on Page {page_label} (Score: {best_node.score:.4f})\")\n",
        "    print(f\"   Context Snippet: {best_node.text[:100]}...\")\n",
        "\n",
        "    # 3. RENDER: convert that PDF page to an image\n",
        "    print(\"\\U0001f5bc\\ufe0f  Rendering page as image...\")\n",
        "    pages = convert_from_path(\n",
        "        PDF_FILE,\n",
        "        first_page=page_index + 1,\n",
        "        last_page=page_index + 1,\n",
        "        dpi=150\n",
        "    )\n",
        "    page_image = pages[0]\n",
        "\n",
        "    # 4. VISION: build the prompt and run Qwen2.5-VL\n",
        "    print(f\"\\U0001f680 Sending page image to Qwen2.5-VL (max {max_new_tokens} tokens)...\")\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": page_image},\n",
        "                {\"type\": \"text\", \"text\": (\n",
        "                    f\"You are an expert financial analyst reviewing Page {page_label} \"\n",
        "                    # FIX 4: Corrected report name — was 'JLR Annual Report'\n",
        "                    \"of the Goldman Sachs Q4 2024 Earnings Report. \"\n",
        "                    \"Answer the following question based on the text, tables, and \"\n",
        "                    \"charts visible on this page. \"\n",
        "                    \"If the answer involves a chart, describe the visual trend.\\n\\n\"\n",
        "                    f\"Question: {query_text}\"\n",
        "                )}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    text_prompt = vlm_processor.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    inputs = vlm_processor(\n",
        "        text=[text_prompt],\n",
        "        images=[page_image],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(vlm_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = vlm_model.generate(\n",
        "            **inputs,\n",
        "            # FIX 5: Reduced max_new_tokens from 512 to 256.\n",
        "            # 512 tokens on T4 GPU takes ~15-20 sec; on CPU it takes ~5-10 MIN.\n",
        "            # 256 tokens covers all typical financial QA answers in ~8-10 sec on GPU.\n",
        "            # Caller can pass max_new_tokens=512 if longer answers are needed.\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            temperature=None,   # FIX 6: Remove temperature when do_sample=False\n",
        "            top_p=None,         # to suppress the 'not valid' warning\n",
        "        )\n",
        "\n",
        "    # Decode only the newly generated tokens\n",
        "    generated = output_ids[:, inputs['input_ids'].shape[1]:]\n",
        "    answer = vlm_processor.batch_decode(generated, skip_special_tokens=True)[0]\n",
        "    return answer\n",
        "\n",
        "print(\"\\u2705 query_visual_rag() function defined and ready.\")"
      ],
      "metadata": {
        "id": "step8_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step9_md"
      },
      "source": [
        "## Step 9: Test 1\n",
        "\n",
        "This query requires the VLM to read a table and extract a value — something text-only RAG cannot do reliably."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "q1 = \"What are the net revenues from Equity underwriting?\"\n",
        "display(Markdown(f\"### Q1: {q1}\"))\n",
        "display(Markdown(query_visual_rag(q1)))"
      ],
      "metadata": {
        "id": "step9_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step10_md"
      },
      "source": [
        "## Step 10: Test 2\n",
        "\n",
        "This query asks the model to extract structured financial data from a complex table. Tables are notoriously difficult for text-extraction-based RAG — column alignment is often lost. By passing the rendered page image, the VLM can interpret the table layout visually."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2 = \"Give me the details for Investment banking fees\"\n",
        "display(Markdown(f\"### Q2: {q2}\"))\n",
        "display(Markdown(query_visual_rag(q2)))"
      ],
      "metadata": {
        "id": "step10_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required Task 14\n",
        "Build a Page-Wise Visual RAG (Retrieval-Augmented Generation) system to analyse AstraZeneca's FY and Q4 2025 earnings report. Rather than simply reading the PDF as text, your system will identify the most relevant page for a given query, render it as an image, and use a Vision-Language Model (VLM) to extract and interpret the information visually — just as a financial analyst would.\n",
        "\n",
        "This mirrors a real-world analyst workflow: locate the right section of a report, then read it carefully to extract structured insights.\n",
        "\n",
        "## Setup\n",
        "Use the notebook structure from the lab session. Your system should use:\n",
        "\n",
        "**Embeddings**: sentence-transformers/all-MiniLM-L6-v2 (local, CPU)\n",
        "\n",
        "**VLM**: Qwen/Qwen2.5-VL-3B-Instruct (local, T4 GPU)\n",
        "\n",
        "**PDF**: AstraZeneca-Q4-2025-earnings.pdf\n",
        "\n",
        "**Runtime**: Google Colab with T4 GPU\n",
        "\n",
        "\n",
        "## Tasks\n",
        "### Task 1 — Revenue Table Extraction\n",
        "Use your Visual RAG system to answer the following query:\n",
        "\n",
        "\"What were AstraZeneca's total Product Sales and Alliance Revenue for FY 2025, and how did each change compared to FY 2024?\"\n",
        "\n",
        "### Task 2 — Regional Revenue Breakdown\n",
        "Issue the following query:\n",
        "\n",
        "\"Which geographic region had the highest Total Revenue growth in FY 2025, and what was the growth rate at constant exchange rates?\"\n",
        "\n",
        "### Task 3 — R&D Pipeline Interpretation\n",
        "Issue the following query:\n",
        "\n",
        "\"Which medicines received regulatory approvals in the US between November 2025 and February 2026, and for what indications?\"\n",
        "\n",
        "### Task 4 — Audit Mode Query\n",
        "Design your own audit-style prompt — one that demands precise numerical figures with an explicit instruction to cite the page or table the number came from. Run it through your system and report:\n",
        "\n",
        "Your chosen query\n",
        "The page retrieved\n",
        "The VLM's response\n",
        "A verification of at least two specific figures against the source document\n",
        "\n",
        "Your prompt should target a different section of the report from Tasks 1–3. Good candidates include the cash flow statement (Table 12), the Reported-to-Core reconciliation (Table 10), or the currency sensitivity table (Table 16)."
      ],
      "metadata": {
        "id": "task14_md"
      }
    }
  ]
}