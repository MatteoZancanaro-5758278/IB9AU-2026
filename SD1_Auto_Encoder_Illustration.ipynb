{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNknMkN1USQw8lV5K2BC4k0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/SD1_Auto_Encoder_Illustration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Auto Encoding with images"
      ],
      "metadata": {
        "id": "DXmAFhNeSASV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d78a30d"
      },
      "source": [
        "Autoencoders are a type of neural network used for unsupervised learning, specifically for learning efficient data codings (representations). They aim to reconstruct their inputs, which means they learn to copy their input to their output. Internally, they have a hidden layer that describes a code used to represent the input. The network consists of two parts: an encoder, which compresses the input into a latent-space representation, and a decoder, which reconstructs the input from this latent-space representation. This notebook demonstrates a simple autoencoder using the MNIST dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a676168"
      },
      "source": [
        "This cell imports the necessary libraries for building and training our autoencoder. `numpy` is for numerical operations, `matplotlib.pyplot` for plotting, and `tensorflow.keras` for building the neural network."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, losses"
      ],
      "metadata": {
        "id": "X1aWDMVOSKZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d2663f7"
      },
      "source": [
        "Here, we define key configuration parameters such as the size of the latent space (`LATENT_DIM_AE`), the flattened image size (`IMAGE_SIZE`), and training parameters (`EPOCHS`, `BATCH_SIZE`). We then load the MNIST dataset, which consists of handwritten digit images. For autoencoders, we only need the image data (`x_train`, `x_test`) as the input is also the target output. The pixel values are normalized to a `[0, 1]` range and the 28x28 images are flattened into a 784-dimensional vector."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration ---\n",
        "LATENT_DIM_AE = 32  # Size of the compressed representation\n",
        "IMAGE_SIZE = 784   # 28x28 pixels flattened\n",
        "EPOCHS = 10        # Number of training epochs (keep low for quick demo)\n",
        "BATCH_SIZE = 128   # Number of samples per gradient update\n",
        "\n",
        "# Load and Preprocess MNIST Data ---\n",
        "print(\"Loading MNIST data...\")\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data() # We don't need labels (y) for standard AE\n",
        "\n",
        "# Normalize pixel values to [0, 1] and flatten images\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), IMAGE_SIZE))\n",
        "x_test = x_test.reshape((len(x_test), IMAGE_SIZE))\n",
        "print(f\"Training data shape: {x_train.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}\")"
      ],
      "metadata": {
        "id": "KlOuwsbqSQ1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e71e577"
      },
      "source": [
        "This section defines the architecture of our autoencoder. It's composed of three main parts:\n",
        "\n",
        "*   **Encoder:** Takes the `IMAGE_SIZE` input, passes it through a dense layer, and compresses it into the `LATENT_DIM_AE` (latent space) representation. This bottleneck forces the encoder to learn a compressed, meaningful representation of the input data.\n",
        "*   **Decoder:** Takes the `LATENT_DIM_AE` latent-space representation, expands it through a dense layer, and reconstructs the original `IMAGE_SIZE` output. The `sigmoid` activation ensures the output pixel values are also within the `[0, 1]` range.\n",
        "*   **Autoencoder Model:** Combines the encoder and decoder. It takes the original input, encodes it, and then decodes the latent representation back into a reconstruction. The model is then compiled with the `adam` optimizer and `BinaryCrossentropy` loss function, suitable for reconstructing pixel values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Standard Autoencoder (AE) ---\n",
        "\n",
        "# Encoder Network\n",
        "# Input -> Dense Layer -> Bottleneck (Latent Space)\n",
        "ae_encoder_inputs = keras.Input(shape=(IMAGE_SIZE,), name='encoder_input')\n",
        "x = layers.Dense(128, activation='relu')(ae_encoder_inputs)\n",
        "ae_latent_space = layers.Dense(LATENT_DIM_AE, activation='relu', name='ae_latent_space')(x)\n",
        "ae_encoder = models.Model(ae_encoder_inputs, ae_latent_space, name='ae_encoder')\n",
        "print(\"\\nEncoder Architecture:\")\n",
        "ae_encoder.summary()\n",
        "\n",
        "# Decoder Network\n",
        "# Bottleneck (Latent Space) -> Dense Layer -> Output (Reconstruction)\n",
        "ae_latent_inputs = keras.Input(shape=(LATENT_DIM_AE,), name='decoder_input')\n",
        "x = layers.Dense(128, activation='relu')(ae_latent_inputs)\n",
        "ae_outputs = layers.Dense(IMAGE_SIZE, activation='sigmoid', name='decoder_output')(x) # Sigmoid for [0,1] pixel output\n",
        "ae_decoder = models.Model(ae_latent_inputs, ae_outputs, name='ae_decoder')\n",
        "print(\"\\nDecoder Architecture:\")\n",
        "ae_decoder.summary()\n",
        "\n",
        "# Autoencoder Model (Connecting Encoder and Decoder)\n",
        "ae_model_outputs = ae_decoder(ae_encoder(ae_encoder_inputs))\n",
        "autoencoder = models.Model(ae_encoder_inputs, ae_model_outputs, name='autoencoder')\n",
        "print(\"\\nFull Autoencoder Architecture:\")\n",
        "autoencoder.summary()\n",
        "\n",
        "# --- 3. Compile the Autoencoder ---\n",
        "# Use binary_crossentropy loss for comparing pixel probabilities (values between 0 and 1)\n",
        "autoencoder.compile(optimizer='adam', loss=losses.BinaryCrossentropy())\n"
      ],
      "metadata": {
        "id": "enzawG9nSecu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4b365ce"
      },
      "source": [
        "This cell trains the autoencoder model using the `x_train` data as both input and target output. The model learns to reconstruct the input images over a specified number of `EPOCHS`, with `validation_data` used to monitor performance on unseen data (`x_test`). The `loss` values indicate how well the autoencoder is performing its reconstruction task, with lower values being better."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Autoencoder ---\n",
        "print(\"\\n--- Training Autoencoder ---\")\n",
        "history_ae = autoencoder.fit(x_train, x_train, # Input data is used as both input and target output\n",
        "                             epochs=EPOCHS,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=True,\n",
        "                             validation_data=(x_test, x_test), # Evaluate on test set\n",
        "                             verbose=1) # Set verbose=1 or 2 to see progress\n"
      ],
      "metadata": {
        "id": "C70jOMkGSt0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1310e7a"
      },
      "source": [
        "Finally, this cell visualizes the autoencoder's performance. The `plot_reconstructions` function takes a few images from the test set, encodes them into their latent representation, and then decodes them back into reconstructed images. By comparing the original and reconstructed images side-by-side, we can qualitatively assess how well the autoencoder has learned to compress and decompress the image data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9apBkLkI3Dw4"
      },
      "outputs": [],
      "source": [
        "# Visualize Results ---\n",
        "print(\"\\n--- Visualizing AE Reconstructions ---\")\n",
        "\n",
        "def plot_reconstructions(encoder, decoder, x_data, n=10):\n",
        "    \"\"\"Plots original and reconstructed images.\"\"\"\n",
        "    # Encode and decode some digits\n",
        "    encoded_imgs = encoder.predict(x_data[:n])\n",
        "    decoded_imgs = decoder.predict(encoded_imgs)\n",
        "\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    plt.suptitle(\"AE: Original vs Reconstructed Images\", fontsize=16)\n",
        "    for i in range(n):\n",
        "        # Display original image\n",
        "        ax = plt.subplot(2, n, i + 1)\n",
        "        plt.imshow(x_data[i].reshape(28, 28), cmap='gray')\n",
        "        plt.title(\"Original\")\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "        # Display reconstructed image\n",
        "        ax = plt.subplot(2, n, i + 1 + n)\n",
        "        plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')\n",
        "        plt.title(\"Reconstructed\")\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "    plt.show()\n",
        "\n",
        "# Select some images from the test set to display\n",
        "plot_reconstructions(ae_encoder, ae_decoder, x_test, n=10)\n",
        "\n",
        "print(\"\\nStandard Autoencoder program finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fcc3eee"
      },
      "source": [
        "# Visualizing the Latent Vector\n",
        "Let usv isualize the 32-dimensional latent vectors generated by the autoencoder for several examples of each MNIST digit (0-9) from the test set, displaying the original image alongside its corresponding latent vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af95d357"
      },
      "source": [
        "Reload the MNIST dataset to obtain the `y_test` labels, which are necessary to categorize the latent vectors by digit.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53d7de14"
      },
      "source": [
        "print(\"Reloading MNIST data to get y_test...\")\n",
        "(_, _), (_, y_test) = keras.datasets.mnist.load_data() # Only interested in y_test\n",
        "\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "print(f\"First 5 y_test labels: {y_test[:5]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4709ec98"
      },
      "source": [
        "## Generate Latent Vectors\n",
        "\n",
        "Use the previously trained `ae_encoder` model to transform the `x_test` images into their 32-dimensional latent representations. This will give us the 'bottleneck' vectors for each test image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "b635b8fb"
      },
      "source": [
        "print(\"Generating latent vectors for x_test using ae_encoder...\")\n",
        "x_test_encoded = ae_encoder.predict(x_test)\n",
        "print(f\"Shape of generated latent vectors (x_test_encoded): {x_test_encoded.shape}\")\n",
        "print(f\"First 5 latent vectors:\\n{x_test_encoded[:5]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b9a65a6"
      },
      "source": [
        "The following shows the visualization function that displays an original image and its corresponding 32-dimensional latent vector. This function will be used to show how each digit is represented in the latent space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4307607b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_latent_representation(original_image, latent_vector, digit, index):\n",
        "    \"\"\"Plots the original image and its 32-dimensional latent vector.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    fig.suptitle(f\"Digit: {digit}, Sample: {index}\", fontsize=16)\n",
        "\n",
        "    # Plot original image\n",
        "    axes[0].imshow(original_image.reshape(28, 28), cmap='gray')\n",
        "    axes[0].set_title('Original Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Plot latent vector\n",
        "    axes[1].bar(range(len(latent_vector)), latent_vector)\n",
        "    axes[1].set_title('32-dim Latent Vector')\n",
        "    axes[1].set_xlabel('Latent Dimension')\n",
        "    axes[1].set_ylabel('Activation Value')\n",
        "    axes[1].set_xticks([]) # Remove x-axis ticks for cleaner visualization of the vector\n",
        "    axes[1].set_ylim(0, max(latent_vector) * 1.1) # Set y-limit based on max value for consistent scaling\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "print(\"Defined plot_latent_representation function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fce8db3"
      },
      "source": [
        "For each digit, we will select a few examples from `x_test` and their corresponding latent vectors from `x_test_encoded`. Then, we use the `plot_latent_representation` function to visualize each original image alongside its latent vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "1a7b2581"
      },
      "source": [
        "print(\"Visualizing latent representations for each digit...\")\n",
        "\n",
        "# Number of samples to show for each digit\n",
        "samples_per_digit = 3\n",
        "\n",
        "# Get unique digits\n",
        "unique_digits = np.unique(y_test)\n",
        "\n",
        "for digit in unique_digits:\n",
        "    print(f\"\\n--- Displaying samples for Digit: {digit} ---\")\n",
        "    # Find indices for the current digit\n",
        "    indices = np.where(y_test == digit)[0]\n",
        "\n",
        "    # Select a few random samples for visualization\n",
        "    # Ensure we don't try to select more samples than available\n",
        "    num_samples_to_show = min(samples_per_digit, len(indices))\n",
        "    if num_samples_to_show == 0:\n",
        "        print(f\"No samples found for digit {digit}\")\n",
        "        continue\n",
        "\n",
        "    # Randomly pick indices to ensure variety if samples_per_digit is small\n",
        "    # For larger samples_per_digit, it might be better to pick first few for consistency\n",
        "    chosen_indices = np.random.choice(indices, num_samples_to_show, replace=False)\n",
        "\n",
        "    for i, idx in enumerate(chosen_indices):\n",
        "        original_image = x_test[idx]\n",
        "        latent_vector = x_test_encoded[idx]\n",
        "        plot_latent_representation(original_image, latent_vector, digit, i + 1)\n",
        "\n",
        "print(\"Visualization complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7164a25b"
      },
      "source": [
        "## Summary of Latent Space Representation\n",
        "\n",
        "*   **Digit-Specific Patterns**:\n",
        "    *   Some digits might show a strong activation in one particular dimension, while others show distributed activations across several dimensions.\n",
        "    *   The overall `shape` of the bar chart (the pattern of activated and inactive dimensions) appears to be somewhat consistent within samples of the same digit, suggesting that the autoencoder has learned characteristic features for each digit.\n",
        "\n",
        "*   **Variability within a Digit**: Even within the same digit, there is some variability in the latent vectors. This is expected, as different handwritten samples of the same digit will have slight variations in style, thickness, and orientation. The autoencoder captures these nuances, but the core 'digit identity' is likely preserved by a consistent underlying pattern in the latent space.\n",
        "\n",
        "In conclusion, the autoencoder effectively transforms complex image data into a compressed, digit-specific latent representation, although the exact semantic meaning of each latent dimension remains abstract."
      ]
    }
  ]
}