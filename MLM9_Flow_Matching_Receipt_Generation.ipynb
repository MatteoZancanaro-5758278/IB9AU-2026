{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/MLM9_Flow_Matching_Receipt_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a1cd2ad"
      },
      "source": [
        "# Flow Matching for Receipt Generation\n",
        "\n",
        "This notebook demonstrates how to train a Flow Matching model to generate synthetic receipt images. We'll cover the entire process from data preparation to model training and image generation. The goal is to create a generative model that can produce realistic-looking receipts based on a small dataset of real receipts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4bf0cab"
      },
      "source": [
        "## Setup and Hyperparameters\n",
        "\n",
        "This section imports necessary libraries and defines global hyperparameters (`HPARAMS`) that configure our model and training process. Understanding these parameters is crucial for customizing the model's behavior and performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "decd4650"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Check for GPU availability and set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define Hyperparameters\n",
        "HPARAMS = {\n",
        "    \"img_size\": 256,         # Size of the generated images (e.g., 256x256 pixels)\n",
        "    \"inference_steps\": 100,  # Number of steps for the ODE solver during sampling (higher = better quality, slower)\n",
        "    \"batch_size\": 32,        # Number of images processed per training step\n",
        "    \"lr\": 1e-4,              # Learning rate for the Adam optimizer\n",
        "    \"epochs\": 50,            # Number of full passes through the training dataset\n",
        "    \"channels\": 1,           # Number of image channels (1 for grayscale, 3 for RGB)\n",
        "    \"num_classes\": 1         # Number of distinct classes the model should generate (here, only 'receipts')\n",
        "}\n",
        "print(\"Hyperparameters:\", HPARAMS)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7217c108"
      },
      "source": [
        "## Data Extraction\n",
        "\n",
        "Before we can train our model, we need to extract the receipt images from the provided ZIP file. This step uses a shell command to decompress the archive into a designated directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e41c3679",
        "collapsed": true
      },
      "source": [
        "!unzip /content/receipts_sample-20260102T154712Z-3-001.zip -d receipt_images\n",
        "print(\"Zip file extracted to 'receipt_images' directory.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79789b62"
      },
      "source": [
        "## Model Architecture and Flow Matching Logic\n",
        "\n",
        "This section defines the core components of our generative model: the `ConditionalUNet` architecture, which is a neural network designed for image generation tasks, and the `FlowMatching` class, which implements the training and sampling logic for flow-based generative models. It also includes our custom `ReceiptDataset` to handle loading and transforming the image data.\n",
        "\n",
        "### SinusoidalPositionEmbeddings\n",
        "This class creates sinusoidal positional embeddings for time steps. In generative models like Flow Matching, these embeddings help the network understand the 'progress' of the generation process (from noise to data) by providing a unique, continuous representation for each time step.\n",
        "\n",
        "### Block\n",
        "The `Block` class represents a fundamental building block of the U-Net architecture. It consists of convolutional layers, batch normalization, and ReLU activations. It also integrates time embeddings, allowing the model to condition its output on the current time step in the flow. The `transform` layer handles downsampling (Conv2d with stride 2) or upsampling (ConvTranspose2d) within the U-Net.\n",
        "\n",
        "### ConditionalUNet\n",
        "The `ConditionalUNet` is the main neural network architecture. It's a U-Net variant that processes images and is 'conditional' because it takes both the image, the current time step (`t`), and a class label (though here we only have one class) as input. This allows the model to learn to generate specific types of images. It consists of a series of `Block`s for downsampling (`downs`) and upsampling (`ups`), with skip connections (residuals) to preserve detail.\n",
        "\n",
        "### FlowMatching\n",
        "This class encapsulates the core logic of the Flow Matching generative process. Unlike diffusion models that iteratively add and remove noise, Flow Matching learns a continuous-time vector field that smoothly transports a simple prior distribution (e.g., Gaussian noise, `x_0`) to a complex target distribution (the real data, `x_1`).\n",
        "\n",
        "-   **`compute_loss`**: Calculates the loss for training. It samples `x_0` (noise), `x_1` (real data), and a random time `t`. It then computes an intermediate point `x_t` on the straight line path between `x_0` and `x_1`, and asks the model to predict the velocity vector (`v_target`) that points from `x_0` to `x_1`. The loss is the mean squared error between the model's predicted velocity (`v_pred`) and the true target velocity.\n",
        "-   **`sample`**: Implements an Euler ODE solver for generating new images. Starting from random noise (`x_0` at `t=0`), it iteratively updates the image by following the velocity field predicted by the trained `ConditionalUNet`. This process effectively traces a path from noise to a generated image (`x_1` at `t=1`).\n",
        "\n",
        "### ReceiptDataset\n",
        "This custom `torch.utils.data.Dataset` class is responsible for loading our receipt images. It reads image files from the specified directory, converts them to grayscale, resizes them to the model's input dimensions, and transforms them into PyTorch tensors. Since we are only generating receipts, it assigns a single class label (0) to all images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f907d90"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "# Re-define device and HPARAMS as they were not in the current execution context\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "HPARAMS = {\n",
        "    \"img_size\": 256,\n",
        "    \"inference_steps\": 100,\n",
        "    \"batch_size\": 32,\n",
        "    \"lr\": 1e-4,\n",
        "    \"epochs\": 50,\n",
        "    \"channels\": 1,\n",
        "    \"num_classes\": 1 # Updated for receipts only\n",
        "}\n",
        "\n",
        "# Re-define model architecture (ConditionalUNet, Block, SinusoidalPositionEmbeddings)\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = np.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
        "        if up:\n",
        "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
        "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
        "        time_emb = self.relu(self.time_mlp(t))\n",
        "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
        "        h = h + time_emb\n",
        "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
        "        return self.transform(h)\n",
        "\n",
        "class ConditionalUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        img_channels = HPARAMS[\"channels\"]\n",
        "        down_channels = (32, 64, 128)\n",
        "        up_channels = (128, 64, 32)\n",
        "        out_dim = img_channels\n",
        "        time_emb_dim = 32\n",
        "        classes = HPARAMS[\"num_classes\"]\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.class_emb = nn.Embedding(classes, time_emb_dim)\n",
        "        self.conv0 = nn.Conv2d(img_channels, down_channels[0], 3, padding=1)\n",
        "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], time_emb_dim) for i in range(len(down_channels)-1)])\n",
        "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], time_emb_dim, up=True) for i in range(len(up_channels)-1)])\n",
        "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
        "\n",
        "    def forward(self, x, t_float, class_label):\n",
        "        t = self.time_mlp(t_float)\n",
        "        c = self.class_emb(class_label)\n",
        "        t = t + c\n",
        "        x = self.conv0(x)\n",
        "        residuals = []\n",
        "        for down in self.downs:\n",
        "            x = down(x, t)\n",
        "            residuals.append(x)\n",
        "        for up in self.ups:\n",
        "            residual = residuals.pop()\n",
        "            x = torch.cat((x, residual), dim=1)\n",
        "            x = up(x, t)\n",
        "        return self.output(x)\n",
        "\n",
        "# Re-define FlowMatching logic\n",
        "class FlowMatching:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def compute_loss(self, model, x_1, labels):\n",
        "        b = x_1.shape[0]\n",
        "        x_0 = torch.randn_like(x_1)\n",
        "        t = torch.rand(b, device=x_1.device)\n",
        "        t_view = t.view(b, 1, 1, 1)\n",
        "        x_t = (1 - t_view) * x_0 + t_view * x_1\n",
        "        v_target = x_1 - x_0\n",
        "        v_pred = model(x_t, t, labels)\n",
        "        return F.mse_loss(v_pred, v_target)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, model, n_samples, class_label_idx, size, steps=50):\n",
        "        model.eval()\n",
        "        x = torch.randn((n_samples, 1, size, size)).to(device)\n",
        "        labels = torch.full((n_samples,), class_label_idx, dtype=torch.long).to(device)\n",
        "        dt = 1.0 / steps\n",
        "        for i in range(steps):\n",
        "            t_curr = torch.ones(n_samples).to(device) * (i / steps)\n",
        "            v_pred = model(x, t_curr, labels)\n",
        "            x = x + v_pred * dt\n",
        "        model.train()\n",
        "        return x\n",
        "\n",
        "# ReceiptDataset definition (already correct from previous successful execution)\n",
        "class ReceiptDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, size=(64, 64), transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.size = size\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "\n",
        "        # Corrected: image_folder should now be 'receipts_sample' based on unzip output\n",
        "        image_folder = os.path.join(root_dir, 'receipts_sample')\n",
        "        for ext in ['jpg', 'jpeg', 'png']:\n",
        "            self.image_paths.extend(glob.glob(os.path.join(image_folder, f'*.{ext}')))\n",
        "\n",
        "        if not self.image_paths:\n",
        "            raise RuntimeError(f\"No images found in {image_folder}. Please check the path and file types.\")\n",
        "\n",
        "        print(f\"Found {len(self.image_paths)} receipt images in {image_folder}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = Image.open(img_path).convert('L') # Convert to grayscale\n",
        "        label = 0 # Assign a single label for receipts\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# save_model function (copy from original notebook)\n",
        "def save_model(model, filename=\"fintech_flow_model.pth\"):\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    print(f\"‚úÖ Model saved to {filename}\")\n",
        "\n",
        "\n",
        "def train_receipt_model():\n",
        "    # 1. Prepare Data using ReceiptDataset\n",
        "    receipt_dataset = ReceiptDataset(\n",
        "        root_dir='receipt_images',\n",
        "        size=(HPARAMS[\"img_size\"], HPARAMS[\"img_size\"]),\n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((HPARAMS[\"img_size\"], HPARAMS[\"img_size\"])),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "    )\n",
        "    receipt_dataloader = DataLoader(receipt_dataset, batch_size=HPARAMS[\"batch_size\"], shuffle=True)\n",
        "\n",
        "    # 2. Initialize Model and Flow Matching (ConditionalUNet uses HPARAMS[\"num_classes\"] internally)\n",
        "    model = ConditionalUNet().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=HPARAMS[\"lr\"])\n",
        "    flow = FlowMatching() # Use Flow Matching manager\n",
        "\n",
        "    print(f\"Starting Flow Matching training for {HPARAMS['epochs']} epochs on Receipts...\")\n",
        "    for epoch in range(HPARAMS['epochs']):\n",
        "        pbar = tqdm(receipt_dataloader)\n",
        "        epoch_loss = 0\n",
        "        for step, (images, labels) in enumerate(pbar):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute Flow Matching Loss\n",
        "            loss = flow.compute_loss(model, images, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_description(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model, flow\n",
        "\n",
        "def generate_receipt_grid(model, flow, n_samples_per_row=4, total_rows=3):\n",
        "    print(\"\\nGenerating Grid of Synthetic Receipts (Euler Step)...\")\n",
        "    steps = HPARAMS[\"inference_steps\"]\n",
        "    # For a single class (receipts), all samples will have the same label (0)\n",
        "    generated_receipts = flow.sample(model, n_samples=n_samples_per_row * total_rows,\n",
        "                                     class_label_idx=0, # Fixed label for receipts\n",
        "                                     size=HPARAMS[\"img_size\"], steps=steps)\n",
        "\n",
        "    grid = make_grid(generated_receipts, nrow=n_samples_per_row, padding=2, normalize=True)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "    plt.title(f\"Generated Receipts (Single Class: {n_samples_per_row*total_rows} samples)\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# --- Main execution block for the new task ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Train the Flow Matching model specifically for receipts\n",
        "    trained_receipt_model, receipt_flow_manager = train_receipt_model()\n",
        "\n",
        "    # 2. Save the trained model\n",
        "    save_model(trained_receipt_model, \"fintech_receipt_flow_model.pth\")\n",
        "\n",
        "    # 3. Visualize the generated receipts\n",
        "    generate_receipt_grid(trained_receipt_model, receipt_flow_manager, n_samples_per_row=1, total_rows=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f2238c2"
      },
      "source": [
        "## 5. Generate New Receipt Image\n",
        "\n",
        "Now that the model has been trained, we can load it and use it to generate new, synthetic receipt images. This section loads the saved model and utilizes the `generate_single_document` function to create and display a new receipt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3762b7c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "# Re-define device and HPARAMS as they were not in the current execution context\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "HPARAMS = {\n",
        "    \"img_size\": 256,\n",
        "    \"inference_steps\": 100,\n",
        "    \"batch_size\": 32,\n",
        "    \"lr\": 1e-4,\n",
        "    \"epochs\": 50,\n",
        "    \"channels\": 1,\n",
        "    \"num_classes\": 1 # Updated for receipts only\n",
        "}\n",
        "\n",
        "# Re-define model architecture (ConditionalUNet, Block, SinusoidalPositionEmbeddings)\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = np.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
        "        if up:\n",
        "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
        "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
        "        time_emb = self.relu(self.time_mlp(t))\n",
        "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
        "        h = h + time_emb\n",
        "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
        "        return self.transform(h)\n",
        "\n",
        "class ConditionalUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        img_channels = HPARAMS[\"channels\"]\n",
        "        down_channels = (32, 64, 128)\n",
        "        up_channels = (128, 64, 32)\n",
        "        out_dim = img_channels\n",
        "        time_emb_dim = 32\n",
        "        classes = HPARAMS[\"num_classes\"]\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.class_emb = nn.Embedding(classes, time_emb_dim)\n",
        "        self.conv0 = nn.Conv2d(img_channels, down_channels[0], 3, padding=1)\n",
        "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], time_emb_dim) for i in range(len(down_channels)-1)])\n",
        "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], time_emb_dim, up=True) for i in range(len(up_channels)-1)])\n",
        "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
        "\n",
        "    def forward(self, x, t_float, class_label):\n",
        "        t = self.time_mlp(t_float)\n",
        "        c = self.class_emb(class_label)\n",
        "        t = t + c\n",
        "        x = self.conv0(x)\n",
        "        residuals = []\n",
        "        for down in self.downs:\n",
        "            x = down(x, t)\n",
        "            residuals.append(x)\n",
        "        for up in self.ups:\n",
        "            residual = residuals.pop()\n",
        "            x = torch.cat((x, residual), dim=1)\n",
        "            x = up(x, t)\n",
        "        return self.output(x)\n",
        "\n",
        "# Re-define FlowMatching logic\n",
        "class FlowMatching:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def compute_loss(self, model, x_1, labels):\n",
        "        b = x_1.shape[0]\n",
        "        x_0 = torch.randn_like(x_1)\n",
        "        t = torch.rand(b, device=x_1.device)\n",
        "        t_view = t.view(b, 1, 1, 1)\n",
        "        x_t = (1 - t_view) * x_0 + t_view * x_1\n",
        "        v_target = x_1 - x_0\n",
        "        v_pred = model(x_t, t, labels)\n",
        "        return F.mse_loss(v_pred, v_target)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, model, n_samples, class_label_idx, size, steps=50):\n",
        "        model.eval()\n",
        "        x = torch.randn((n_samples, 1, size, size)).to(device)\n",
        "        labels = torch.full((n_samples,), class_label_idx, dtype=torch.long).to(device)\n",
        "        dt = 1.0 / steps\n",
        "        for i in range(steps):\n",
        "            t_curr = torch.ones(n_samples).to(device) * (i / steps)\n",
        "            v_pred = model(x, t_curr, labels)\n",
        "            x = x + v_pred * dt\n",
        "        model.train()\n",
        "        return x\n",
        "\n",
        "# Re-define load_model function (copy from original notebook)\n",
        "def load_model(filename=\"fintech_receipt_flow_model.pth\"):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"‚ùå Error: {filename} not found.\")\n",
        "        return None\n",
        "\n",
        "    # Ensure HPARAMS['num_classes'] is set correctly before initializing model\n",
        "    # We know from previous steps it was set to 1 for receipts\n",
        "    model = ConditionalUNet().to(device)\n",
        "    model.load_state_dict(torch.load(filename, map_location=device))\n",
        "    model.eval()\n",
        "    print(f\"‚úÖ Model loaded from {filename}\")\n",
        "    return model\n",
        "\n",
        "# Re-define generate_single_document function (copy from original notebook)\n",
        "def generate_single_document(model, flow, doc_type='receipt'):\n",
        "    \"\"\"Generates a single image of the requested type.\"\"\"\n",
        "    # For this specific task, we only generate 'receipt' and its label is 0\n",
        "    label_map = {'receipt': 0}\n",
        "\n",
        "    if doc_type not in label_map:\n",
        "        print(f\"‚ùå Unknown type: {doc_type}. Currently only 'receipt' is supported for single generation in this context.\")\n",
        "        return\n",
        "\n",
        "    print(f\"üé® Generating new {doc_type} with ODE Solver...\")\n",
        "    sample_tensor = flow.sample(model, n_samples=1, class_label_idx=label_map[doc_type],\n",
        "                                size=HPARAMS[\"img_size\"], steps=HPARAMS[\"inference_steps\"])\n",
        "\n",
        "    # Convert tensor to displayable image\n",
        "    img = sample_tensor[0].cpu().permute(1, 2, 0).numpy()\n",
        "    img = (img - img.min()) / (img.max() - img.min()) # Normalize to 0-1\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(f\"Generated {doc_type.capitalize()}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# --- Execution for generating a single receipt ---\n",
        "\n",
        "# Load the previously trained model\n",
        "loaded_receipt_model = load_model(\"fintech_receipt_flow_model.pth\")\n",
        "\n",
        "# Initialize FlowMatching class (no state needed, just methods)\n",
        "receipt_flow_manager = FlowMatching()\n",
        "\n",
        "# Generate and display a single receipt\n",
        "if loaded_receipt_model:\n",
        "    generate_single_document(loaded_receipt_model, receipt_flow_manager, doc_type='receipt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Required Task 11\n",
        "\n",
        "Download the zip file `floorplans_v2-20251223T170650Z-3-001.zip` which contains a large sample of floorplan images. Your task is to train flow matching model based on these images. Train the model on the floorplan images and create code to generate new synthetic floorplans."
      ],
      "metadata": {
        "id": "3kZT7yWxqE3p"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}