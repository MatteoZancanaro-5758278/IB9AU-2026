{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOfdjYAW4uZothjPIZO33iU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/L5_LLMs_Structured_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab7ed1fb"
      },
      "source": [
        "# Predicting Creditworthiness: A FinTech Application\n",
        "\n",
        "In this notebook, we'll explore the power of structured data and its critical role in various industries, especially in the rapidly evolving world of FinTech.\n",
        "\n",
        "## The Ubiquity of Structured Data\n",
        "\n",
        "Despite the rise of unstructured data sources like text and images, structured data remains the backbone of many analytical and operational systems. This type of data, organized into tables with rows and columns, is found everywhere, from financial transactions and customer records to scientific measurements and inventory management. Its clear, predefined format makes it ideal for traditional database systems and powerful machine learning algorithms.\n",
        "\n",
        "## Structured Data in FinTech\n",
        "\n",
        "In FinTech (Financial Technology), structured data is paramount. Banks, lending institutions, and investment firms rely heavily on structured datasets to:\n",
        "*   **Assess Credit Risk:** Determine the likelihood of a borrower defaulting on a loan.\n",
        "*   **Detect Fraud:** Identify suspicious patterns in transactions.\n",
        "*   **Personalize Services:** Offer tailored financial products to customers.\n",
        "*   **Optimize Investments:** Analyze market trends and portfolio performance.\n",
        "\n",
        "## Case Study: German Credit Dataset\n",
        "\n",
        "We will dive into the **German Credit Dataset**, a classic dataset from Kaggle that is widely used for credit risk assessment. This dataset contains 20 features describing individuals applying for credit, along with a target variable indicating their creditworthiness.\n",
        "\n",
        "Our objective is to build a predictive model to classify applicants as either:\n",
        "*   **0: Bad Credit Risk**\n",
        "*   **1: Good Credit Risk**\n",
        "\n",
        "## Our Approach: Random Forest Classifier\n",
        "\n",
        "To achieve this, we will employ a traditional yet highly effective machine learning algorithm: the **Random Forest Classifier**. This ensemble learning method combines multiple decision trees to produce a more robust and accurate prediction. By leveraging this algorithm, we aim to demonstrate how structured data and machine learning can be used to make informed decisions in a real-world FinTech scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d2afaa2"
      },
      "source": [
        "First, we need to install the `ucimlrepo` library, which allows us to easily fetch datasets from the UCI Machine Learning Repository, where the German Credit Dataset is hosted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oSKJuUjCfNUO"
      },
      "outputs": [],
      "source": [
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8922ed0e"
      },
      "source": [
        "Now, we'll use the `ucimlrepo` library to fetch the German Credit Dataset. We'll separate the features (`X`) from the target variable (`y`). We'll also print the dataset's metadata and variable information to understand its structure and contents better."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "statlog_german_credit_data = fetch_ucirepo(id=144)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = statlog_german_credit_data.data.features\n",
        "y = statlog_german_credit_data.data.targets\n",
        "\n",
        "# metadata\n",
        "print(statlog_german_credit_data.metadata)\n",
        "\n",
        "# variable information\n",
        "print(statlog_german_credit_data.variables)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pQ3sWL9VfYhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "121c1f8d"
      },
      "source": [
        "Before training our Random Forest model, we need to preprocess the data. This involves:\n",
        "1.  **Identifying Categorical Columns:** Finding columns with non-numeric (object) data types.\n",
        "2.  **One-Hot Encoding:** Converting these categorical columns into a numerical format that machine learning algorithms can understand. We use `drop_first=True` to avoid multicollinearity.\n",
        "3.  **Splitting Data:** Dividing the dataset into training and testing sets to evaluate the model's performance on unseen data.\n",
        "4.  **Training the Model:** Initializing and fitting a `RandomForestClassifier` to our training data.\n",
        "5.  **Making Predictions:** Using the trained model to predict creditworthiness on the test set.\n",
        "6.  **Evaluating the Model:** Assessing the model's accuracy and generating a classification report to understand its precision, recall, and F1-score for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ce36bc16"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Identify categorical columns in X\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Apply one-hot encoding to categorical features\n",
        "X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9Vg3GE4O0jqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TabPFN (Tabular Prior-Data Fitted Network)\n",
        "TabPFN is a foundation model for tabular data that fundamentally changes how machine learning models are trained and deployed on structured datasets. Unlike traditional methods (such as Gradient-Boosted Decision Trees) that must be trained from scratch on every new dataset, TabPFN is a pre-trained Transformer that generates predictions in a single forward pass.\n",
        "\n",
        "Link to learn more: https://github.com/PriorLabs/TabPFN"
      ],
      "metadata": {
        "id": "oOS7jAC9gbRv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c848bba0"
      },
      "source": [
        "Next, we'll install `tabpfn-client` and `tabpfn-extensions`. TabPFN is a novel deep learning model for tabular data that acts as an in-context learner. It can often provide strong performance without extensive hyperparameter tuning, making it an interesting alternative to traditional models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TabPFN Installation optimized for Google Colab\n",
        "# Install the TabPFN Client library\n",
        "!uv pip install tabpfn-client\n",
        "\n",
        "# Install TabPFN extensions for additional functionalities\n",
        "!uv pip install tabpfn-extensions[all]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Cnul0ow_gdY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a792043e"
      },
      "source": [
        "With the TabPFN libraries installed, we can now import the `TabPFNClassifier` for use in our notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple import for TabPFN\n",
        "from tabpfn_client import TabPFNClassifier\n",
        "\n",
        "# Now you can use it like any other sklearn classifier\n",
        "# model = TabPFNClassifier()\n",
        "print(\"TabPFNClassifier imported successfully.\")"
      ],
      "metadata": {
        "id": "oBIA7ApigqAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abdc258d"
      },
      "source": [
        "Similar to the Random Forest model, we'll train and evaluate the `TabPFNClassifier`. This involves:\n",
        "1.  **Splitting Data:** We'll split the original `X` and `y` data again to ensure a clean split for TabPFN.\n",
        "2.  **Training the Classifier:** Initializing and fitting the `TabPFNClassifier` to the training data.\n",
        "3.  **Making Predictions:** Generating both probability predictions (`predict_proba`) and class predictions (`predict`) on the test set.\n",
        "4.  **Evaluating the Model:** Calculating the ROC AUC score, accuracy, and a classification report to understand TabPFN's performance, especially for binary classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***First time you run this, you will be asked to create an account.***"
      ],
      "metadata": {
        "id": "_FJlT7tnltP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTHzDedu3DKR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tabpfn_client import TabPFNClassifier\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42\n",
        ")\n",
        "\n",
        "# Train and evaluate the TabPFN classifier\n",
        "tabpfn_classifier = TabPFNClassifier(random_state=42)\n",
        "tabpfn_classifier.fit(X_train, y_train)\n",
        "y_pred_proba = tabpfn_classifier.predict_proba(X_test)\n",
        "y_pred = tabpfn_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the ROC AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "print(f\"TabPFN ROC AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Required Task 6\n",
        "The code below gets a dataset to predict default. The outcome variable of interest is `default.payment.next.month`. Use two traditional machine learning algorithms (random forest, XGboost, etc.) and TabPFN to predict the outcome. Use a test set of 25% of the data. How well does TabPFN perform in comparison to machine learning algorithms?"
      ],
      "metadata": {
        "id": "PV0vobWQyc6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "dataset_path = kagglehub.dataset_download(\"uciml/default-of-credit-card-clients-dataset\")\n",
        "\n",
        "# List contents of the downloaded dataset directory to find the data file\n",
        "files_in_dataset = os.listdir(dataset_path)\n",
        "print(f\"Files in the dataset directory: {files_in_dataset}\")\n",
        "\n",
        "# Assuming the main data file is a CSV and is directly in the downloaded path\n",
        "# You might need to adjust the filename if it's different or in a subdirectory\n",
        "# For this dataset, it's typically 'UCI_Credit_Card.csv'\n",
        "\n",
        "# Construct the full path to the CSV file\n",
        "csv_file_name = 'UCI_Credit_Card.csv'\n",
        "full_csv_path = os.path.join(dataset_path, csv_file_name)\n",
        "\n",
        "# Load the CSV into a pandas DataFrame\n",
        "df = pd.read_csv(full_csv_path)\n",
        "\n",
        "print(\"Data loaded into DataFrame 'df' successfully.\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "paUnvVDUx5c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_syyw-JRyANG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}