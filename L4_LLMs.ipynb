{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "mount_file_id": "1OYK7P4WMtz17KcJzlOn4y-4n0kn2ctKh",
      "authorship_tag": "ABX9TyO92TrM1SQSFsEgIiyGPWWv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/L4_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Make sure you have your API keys for Gemini and HuggingFace***\n",
        "\n"
      ],
      "metadata": {
        "id": "TnryJDgOhrwj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b5c7984"
      },
      "source": [
        "## Overview of LLMs: Proprietary vs. Open Source\n",
        "\n",
        "Large Language Models (LLMs) are a type of artificial intelligence program capable of generating human-like text, understanding context, and performing various language-related tasks. They are trained on vast amounts of text data.\n",
        "\n",
        "LLMs can generally be categorized into two main types:\n",
        "\n",
        "### 1. Proprietary Models\n",
        "\n",
        "These are models developed and owned by private companies. They are often closed-source, meaning their internal architecture, training data, and weights are not publicly accessible. Users typically interact with these models through APIs provided by the developers.\n",
        "\n",
        "**Characteristics:**\n",
        "*   **Controlled Access:** Access is usually via API keys or paid subscriptions.\n",
        "*   **High Performance:** Often trained on massive datasets with significant computational resources, leading to state-of-the-art performance.\n",
        "*   **Ease of Use:** Generally well-documented APIs and managed infrastructure make them easy to integrate and use.\n",
        "*   **Less Transparency:** The inner workings are typically opaque.\n",
        "*   **Examples:** Google's Gemini, OpenAI's GPT series, Anthropic's Claude.\n",
        "\n",
        "### 2. Open Source Models\n",
        "\n",
        "These models are released with their code, weights, and sometimes training data publicly available. This allows researchers and developers to inspect, modify, and deploy them freely.\n",
        "\n",
        "**Characteristics:**\n",
        "*   **Transparency:** Full access to the model's architecture and weights encourages research and understanding.\n",
        "*   **Customization:** Can be fine-tuned or adapted for specific tasks and datasets.\n",
        "*   **Community Support:** Benefit from a vibrant community that contributes improvements, tools, and documentation.\n",
        "*   **Deployment Flexibility:** Can be hosted on various infrastructures, from local machines to cloud services, giving users more control over data privacy and costs.\n",
        "*   **Examples:** HuggingFace's Transformers library models (e.g., GPT-2, Llama, Falcon), Mistral, Gemma.\n",
        "\n",
        "This notebook demonstrates how to interact with both proprietary models (specifically Google's Gemini) and open-source models (using HuggingFace Transformers)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1 - Proprietary Models"
      ],
      "metadata": {
        "id": "6tK61UOCqisy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26f25707"
      },
      "source": [
        "### API Key Setup\n",
        "\n",
        "To interact with proprietary models like Google Gemini, you often need an API key. This key authenticates your requests and manages your usage. In Google Colab, it's good practice to store sensitive information like API keys in `Userdata` secrets for security.\n",
        "\n",
        "*   `import os` and `from google.colab import userdata`: These lines import necessary modules for interacting with the operating system and Colab's user data, respectively.\n",
        "*   `os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GEMINI_API_KEY')`: This retrieves your API key, named 'GEMINI_API_KEY', from Colab secrets and sets it as an environment variable.\n",
        "*   `genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])`: This configures the `google.generativeai` library with your API key, allowing you to make authenticated requests to the Gemini API."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup (Use the standard library)\n",
        "# Get your API key from https://aistudio.google.com/\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "C6fwZe_lpOT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a412b972"
      },
      "source": [
        "### Listing Available Models\n",
        "\n",
        "The `genai.list_models()` function allows you to see all the available Generative AI models that you can access through the API. The loop then filters these models to display only those that support the `generateContent` method, which indicates they are suitable for text generation tasks (LLMs)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Available LLM Models:\")\n",
        "# Iterate through all models and filter\n",
        "for m in genai.list_models():\n",
        "    # 'generateContent' indicates it is a text-generation model (LLM)\n",
        "    if 'generateContent' in m.supported_generation_methods:\n",
        "        print(f\"- {m.name}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YOHPCBxgmVrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "lg_us24Czhov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a824bb15"
      },
      "source": [
        "### Basic Text Generation\n",
        "\n",
        "This section demonstrates a basic text generation request using the Gemini API:\n",
        "\n",
        "1.  `model = genai.GenerativeModel(\"gemini-flash-latest\")`: This line initializes a `GenerativeModel` object, specifying the particular Gemini model you want to use (`gemini-flash-latest` in this case).\n",
        "2.  `response = model.generate_content(...)`: This makes a call to the model to generate content. You pass a list of strings as input (your prompt).\n",
        "3.  `print(response.text)`: The generated text content from the model's response is then printed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the Question\n",
        "model = genai.GenerativeModel(\"gemini-flash-latest\")\n",
        "\n",
        "response = model.generate_content(\n",
        "    [\n",
        "        \"Explain what AI is\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"\\n--- ANSWER ---\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nSBuRWCVqFnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xf-UJt0z05Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e2fba6e"
      },
      "source": [
        "### Controlling Generation Parameters\n",
        "\n",
        "When generating content, you can often control aspects of the output using `generation_config` parameters:\n",
        "\n",
        "*   `\"temperature\"`: This parameter controls the randomness of the output. A higher temperature (e.g., 1.0) leads to more creative and diverse responses, while a lower temperature (e.g., 0.1-0.5) makes the output more deterministic and focused. Here, `0.7` provides a moderate level of creativity.\n",
        "*   `\"max_output_tokens\"`: This parameter sets a limit on the number of tokens (words or sub-word units) the model will generate in its response. This is useful for controlling response length and managing API costs. Here, the output is capped at `100` tokens."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the Question\n",
        "model = genai.GenerativeModel(\"gemini-flash-latest\")\n",
        "\n",
        "response = model.generate_content(\n",
        "    [\n",
        "        \"Explain what AI is\"\n",
        "    ],\n",
        "    generation_config={\n",
        "        \"temperature\": 0.7, # Controls randomness; lower values make output more deterministic.\n",
        "        \"max_output_tokens\": 100 # Sets the maximum number of tokens to generate.\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"\\n--- ANSWER ---\")\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "tB1pplUgpHOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2 - Open Source Models"
      ],
      "metadata": {
        "id": "kkZc78JIqny-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9210408"
      },
      "source": [
        "### HuggingFace Transformers\n",
        "\n",
        "HuggingFace Transformers is a powerful library that provides thousands of pre-trained models for various tasks in Natural Language Processing (NLP), Computer Vision, and Audio. It allows you to quickly download and use state-of-the-art models for tasks like text generation, sentiment analysis, translation, and more. It supports popular deep learning frameworks like PyTorch, TensorFlow, and JAX.\n",
        "\n",
        "This section will explore how to use different `pipeline` functions from the Transformers library for various NLP tasks with open-source models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Transformers\n",
        "The Transformers library is already installed Google Colab. You just have to import it."
      ],
      "metadata": {
        "id": "3itfVTs3DmZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "ijA-Inl8zjBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models available on HuggingFace: [Models](https://huggingface.co/models)"
      ],
      "metadata": {
        "id": "ql8cZrTdCXgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Resource](https://huggingface.co/learn/llm-course/chapter1/1?fw=pt)\n"
      ],
      "metadata": {
        "id": "sXjoeXTsyuyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sI_VuDmEzRzq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6de7055"
      },
      "source": [
        "### Understanding the `pipeline()` Function\n",
        "\n",
        "The `pipeline()` function from the HuggingFace Transformers library is a high-level API designed to make it easy to use pre-trained models for common NLP tasks. It abstracts away the complexities of tokenization, model loading, and post-processing, allowing you to quickly get results with just a few lines of code.\n",
        "\n",
        "When you use `from transformers import pipeline`, you're importing this versatile function. It acts as a wrapper around models, making them accessible for specific tasks like text generation, sentiment analysis, or summarization, without needing to delve into the intricate details of each model's architecture or how to prepare the input data for it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pipelines\n",
        "\n",
        "The `pipeline()` function connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer.\n",
        "\n",
        "There are three main steps involved when you pass some text to a pipeline:\n",
        "\n",
        "1. The text is preprocessed into a format the model can understand.\n",
        "2. The preprocessed inputs are passed to the model.\n",
        "3. The predictions of the model are post-processed, so you can make sense of them.\n",
        "\n",
        "\n",
        "\n",
        "Some of the currently available pipelines are:\n",
        "\n",
        "* feature-extraction (get the vector representation of a text)\n",
        "\n",
        "* fill-mask\n",
        "\n",
        "* ner (named entity recognition)\n",
        "\n",
        "* question-answering\n",
        "\n",
        "* sentiment-analysis\n",
        "\n",
        "* summarization\n",
        "\n",
        "* text-generation\n",
        "\n",
        "* translation\n",
        "\n",
        "* zero-shot-classification"
      ],
      "metadata": {
        "id": "4KVO-8PB0epv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "C0HOtyO7vTJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "bLfMt2E4wcJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59f4865a"
      },
      "source": [
        "### Text Generation with Open-Source Models\n",
        "\n",
        "The `text-generation` pipeline allows you to generate new text based on a given prompt. You can specify different pre-trained models from the HuggingFace Hub.\n",
        "\n",
        "#### GPT-2\n",
        "\n",
        "*   `generator = pipeline('text-generation', model='gpt2')`: This initializes a text generation pipeline using the `gpt2` model. GPT-2 is a widely recognized transformer-based language model by OpenAI, known for its ability to generate coherent and diverse text.\n",
        "*   The `max_length` parameter controls the maximum number of tokens in the generated output, and `num_return_sequences` specifies how many different output sequences should be generated.\n",
        "\n",
        "#### LiquidAI\n",
        "\n",
        "*   `generator = pipeline(\"text-generation\", model=\"LiquidAI/LFM2-2.6B-Exp\")`: This initializes another text generation pipeline, but this time using the `LiquidAI/LFM2-2.6B-Exp` model. This demonstrates how you can easily switch between different models available on HuggingFace by just changing the `model` parameter. This particular model is an experimental one from LiquidAI, showcasing the diversity of available open-source models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GPT2"
      ],
      "metadata": {
        "id": "s_EVjnG6vGoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', model='gpt2')"
      ],
      "metadata": {
        "id": "9YUyoVk618-T",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = generator(\n",
        "    \"When you are on a diet\",\n",
        "    max_length=50,\n",
        "    num_return_sequences=1,\n",
        ")\n",
        "output"
      ],
      "metadata": {
        "id": "hcQ6-HPu8205",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the generated text from the list of dictionaries\n",
        "generated_texts = [item['generated_text'] for item in output]\n",
        "# Join the generated texts into a single string\n",
        "display(Markdown(\"\\n\".join(generated_texts)))"
      ],
      "metadata": {
        "id": "zQddqUl6wn4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LiquidAI"
      ],
      "metadata": {
        "id": "dB3xqVm-vc2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=\"LiquidAI/LFM2-2.6B-Exp\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iRZbV0PYvjjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = generator(\n",
        "    \"When you are on a diet\",\n",
        "    max_length=50,\n",
        ")\n",
        "\n",
        "# Extract the generated text from the list of dictionaries\n",
        "generated_texts = [item['generated_text'] for item in output]\n",
        "# Join the generated texts into a single string\n",
        "display(Markdown(\"\\n\".join(generated_texts)))"
      ],
      "metadata": {
        "id": "CfdKvS8awDwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "672b890d"
      },
      "source": [
        "### Sentiment Analysis\n",
        "\n",
        "Sentiment analysis is the task of classifying the emotional tone of text, typically as positive, negative, or neutral. The `sentiment-analysis` pipeline is perfect for this.\n",
        "\n",
        "*   `classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")`: This line initializes a sentiment analysis pipeline. The model specified, `distilbert-base-uncased-finetuned-sst-2-english`, is a DistilBERT model fine-tuned on the SST-2 dataset, which is designed for sentiment classification. It's a smaller, faster version of BERT, making it efficient for such tasks.\n",
        "*   When you pass text to the `classifier`, it returns a list of dictionaries, each containing the predicted `label` (e.g., 'POSITIVE', 'NEGATIVE') and a `score` indicating the confidence of that prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id3r1YOXyeFJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"This is quite difficult\")"
      ],
      "metadata": {
        "id": "kxSLO2MEz0Hu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b5c9d05"
      },
      "source": [
        "### Batch Processing for Sentiment Analysis (Give this a try)\n",
        "\n",
        "This section demonstrates how to apply the sentiment analysis pipeline to multiple pieces of text efficiently, often referred to as batch processing. Instead of processing one sentence at a time, you can pass a list of texts to the pipeline.\n",
        "\n",
        "*   `import pandas as pd`: Imports the pandas library, which is commonly used for data manipulation, especially with tabular data.\n",
        "*   `df = pd.read_csv('...', nrows=100)`: Reads the first 100 rows of the `sms_spam.csv` dataset into a pandas DataFrame. This dataset likely contains text messages that can be analyzed for sentiment.\n",
        "*   `results = classifier(df['text'].tolist())`: This is the key line for batch processing. It takes the 'text' column from the DataFrame, converts it into a list, and passes this list directly to the `classifier` pipeline. The pipeline processes all texts in the list simultaneously.\n",
        "*   The subsequent lines `labels = [item['label'] for item in results]` and `scores = [item['score'] for item in results]` extract the sentiment labels and scores from the results.\n",
        "*   `df['sentiment_label'] = labels` and `df['sentiment_score'] = scores`: These lines add the extracted labels and scores as new columns to the DataFrame, associating each text message with its predicted sentiment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/RDGopal/IB9LQ0-GenAI/main/Data/sms_spam.csv',nrows=100)"
      ],
      "metadata": {
        "id": "ysli7eIXzy4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "LdaMF4-g6U_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    results = classifier(df['text'].tolist())\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "labels = [item['label'] for item in results]\n",
        "scores = [item['score'] for item in results]\n",
        "\n",
        "df['sentiment_label'] = labels\n",
        "df['sentiment_score'] = scores"
      ],
      "metadata": {
        "id": "uF0PcKzN6IYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "BWtHHV1q66fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25637d70"
      },
      "source": [
        "### Zero-Shot Classification\n",
        "\n",
        "Zero-shot classification is a powerful NLP technique where a model can classify text into categories it hasn't explicitly been trained on. Instead, it classifies based on a natural language description of the categories.\n",
        "\n",
        "*   `classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")`: This initializes a zero-shot classification pipeline using the `facebook/bart-large-mnli` model. This model is fine-tuned for natural language inference (MNLI), which allows it to understand the relationship between a premise (your text) and a hypothesis (your candidate labels).\n",
        "*   When calling the `classifier`, you provide both the `sequence` (the text to be classified) and a list of `candidate_labels`. The model then determines how likely the sequence belongs to each of the provided labels, even if it has never seen these specific labels during its training for this task. The output includes the `sequence`, `labels`, and corresponding `scores`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
      ],
      "metadata": {
        "id": "TXaB1Hl91S6j",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\n",
        "    \"This is a discussion about world history\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ],
      "metadata": {
        "id": "vEPXEEL18jry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1049df71"
      },
      "source": [
        "### Fill-Mask\n",
        "\n",
        "The `fill-mask` pipeline is used to predict missing words (masked tokens) in a sentence. This is a common task used to pre-train language models.\n",
        "\n",
        "*   `unmasker = pipeline(\"fill-mask\", model = \"distilroberta-base\")`: This initializes a fill-mask pipeline using the `distilroberta-base` model. DistilRoBERTa is a distilled version of RoBERTa, which itself is an optimized BERT model. It's designed for efficiency while retaining much of the performance.\n",
        "*   When you pass a sentence with a `<mask>` token to the `unmasker`, the model predicts the most probable words that could fill that mask. The `top_k` parameter specifies how many of the top predictions you want to see. The output provides the `score` (confidence), `token` (ID), `token_str` (the predicted word), and the `sequence` with the mask filled."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker = pipeline(\"fill-mask\", model = \"distilroberta-base\")"
      ],
      "metadata": {
        "id": "KceQ4oX0_zFI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker(\"This course will teach you all about <mask> models.\", top_k=5)"
      ],
      "metadata": {
        "id": "J2UsfL559PQy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33383943"
      },
      "source": [
        "### Named Entity Recognition (NER)\n",
        "\n",
        "Named Entity Recognition (NER) is the task of identifying and classifying named entities (like persons, organizations, locations, dates, etc.) in text. The `ner` pipeline helps extract these entities.\n",
        "\n",
        "*   `ner = pipeline(\"ner\", grouped_entities=True, model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")`: This initializes an NER pipeline. The `dbmdz/bert-large-cased-finetuned-conll03-english` model is a BERT-based model fine-tuned on a dataset (CoNLL-2003) specifically for NER tasks in English.\n",
        "*   `grouped_entities=True` is a useful parameter that combines sub-word tokens belonging to the same entity into a single entity. For example, \"New York\" might be tokenized as two separate tokens but would be grouped as a single 'LOC' (location) entity.\n",
        "*   The output shows the `entity_group` (e.g., 'PER' for Person, 'ORG' for Organization, 'LOC' for Location), a `score` (confidence), the `word` itself, and its `start` and `end` character positions in the original text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline(\"ner\", grouped_entities=True, model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")"
      ],
      "metadata": {
        "id": "vtiZ6p0tEhhZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner(\"My name is Ram and I work at WBS in Coventry.\")"
      ],
      "metadata": {
        "id": "2uqBUzLE9k-x",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f6fbb00"
      },
      "source": [
        "### Summarization\n",
        "\n",
        "Text summarization is the task of creating a shorter, coherent, and fluent summary of a longer text document. The `summarization` pipeline helps condense information.\n",
        "\n",
        "*   `summarizer = pipeline(\"summarization\", model = \"sshleifer/distilbart-cnn-12-6\")`: This initializes a summarization pipeline using the `sshleifer/distilbart-cnn-12-6` model. This model is a DistilBART variant fine-tuned for summarization. BART (Bidirectional and Auto-Regressive Transformers) is known for its effectiveness in generation tasks like summarization.\n",
        "*   You pass a longer piece of text to the `summarizer`, and it returns a dictionary containing the `summary_text`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model = \"sshleifer/distilbart-cnn-12-6\")"
      ],
      "metadata": {
        "id": "QEgzjs3_HP3F",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = summarizer(\n",
        "    \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of\n",
        "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
        "    the premier American universities engineering curricula now concentrate on\n",
        "    and encourage largely the study of engineering science. As a result, there\n",
        "    are declining offerings in engineering subjects dealing with infrastructure,\n",
        "    the environment, and related issues, and greater concentration on high\n",
        "    technology subjects, largely supporting increasingly complex scientific\n",
        "    developments. While the latter is important, it should not be at the expense\n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other\n",
        "    industrial countries in Europe and Asia, continue to encourage and advance\n",
        "    the teaching of engineering. Both China and India, respectively, graduate\n",
        "    six and eight times as many traditional engineers as does the United States.\n",
        "    Other industrial countries at minimum maintain their output, while America\n",
        "    suffers an increasingly serious decline in the number of engineering graduates\n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\"\n",
        ")\n",
        "display(Markdown(output[0]['summary_text']))"
      ],
      "metadata": {
        "id": "Ctm4xaiv-xpB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50319212"
      },
      "source": [
        "### Translation\n",
        "\n",
        "The `translation` pipeline allows you to translate text from one language to another. You need to specify the source and target languages in the pipeline name.\n",
        "\n",
        "*   `trans = pipeline(\"translation_en_to_fr\", model = \"t5-base\")`: This initializes a translation pipeline specifically for translating from English (`en`) to French (`fr`). The `t5-base` model is a T5 (Text-to-Text Transfer Transformer) model, which frames all NLP problems as text-to-text tasks, making it versatile for translation, summarization, and more.\n",
        "*   When you pass an English sentence to `trans`, it returns a list of dictionaries, each containing the `translation_text` in French."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans = pipeline(\"translation_en_to_fr\", model = \"t5-base\")"
      ],
      "metadata": {
        "id": "BQASSWWAHtPv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = trans(\"good evening\")\n",
        "list(y)"
      ],
      "metadata": {
        "id": "bkGwMuBY_RF7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rJAlnzTnLTDq"
      }
    }
  ]
}