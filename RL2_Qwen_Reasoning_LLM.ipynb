{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPcEzwKYyy5sqPrjxUc0ENC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/RL2_Qwen_Reasoning_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Qwen3-0.6B 'Thinking' LLM\n"
      ],
      "metadata": {
        "id": "P-EqZ_YW7z8n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8f63504"
      },
      "source": [
        "A 'Thinking' or 'Reasoning' Large Language Model (LLM) refers to a model that exhibits an ability to process information beyond simple token prediction, often by generating intermediate thoughts or steps before producing a final answer. This internal 'thought process' can make the model's reasoning more transparent and improve the quality of its responses by allowing it to break down complex problems, evaluate options, and refine its logic. In the context of the Qwen3-0.6B model used in this notebook, the `enable_thinking=True` parameter activates this mode, allowing the model to generate an internal 'thought' block (`<think>...</think>`) before its main response. This can be particularly useful for understanding how the model arrives at its conclusions, aiding in debugging, and improving its performance on tasks requiring multi-step reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c35c463"
      },
      "source": [
        "This cell loads the Qwen3-0.6B model and its corresponding tokenizer from the Hugging Face Transformers library. `AutoTokenizer` and `AutoModelForCausalLM` are used to automatically select the correct tokenizer and model architecture based on the provided model name. `torch_dtype='auto'` optimizes memory usage, and `device_map='auto'` intelligently distributes the model across available devices (like GPUs) for efficient loading and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMQzGAch3rvF",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "# load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4c50549"
      },
      "source": [
        "In this cell, the input for the language model is prepared. A `prompt` defines the user's query. This prompt is then formatted into a list of messages (simulating a chat conversation). The `tokenizer.apply_chat_template` function converts these messages into a format the model understands, and importantly, `enable_thinking=True` activates the model's 'thinking' mode, instructing it to generate an internal thought process before its final answer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the model input\n",
        "prompt = \"Tell me how the Q-learning algorithm for Reinforcement Learning works\" #@param {type:\"string\"}\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
      ],
      "metadata": {
        "id": "6HRAtreR3-fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15d9598c"
      },
      "source": [
        "This cell performs the core text completion task. The `model.generate()` method is called with the prepared model inputs. `max_new_tokens` is set to a high value to ensure a comprehensive response. The generated IDs are then processed to extract only the newly generated tokens, excluding the input prompt tokens."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=32768\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()"
      ],
      "metadata": {
        "id": "8AecnHZE4IkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91be2df5"
      },
      "source": [
        "This cell is responsible for parsing the generated output. Since the model operates in 'thinking' mode, its output includes a `<think>...</think>` block. This code identifies the end of the thinking block (using a special token ID 151668 for `</think>`) and then decodes the content before and after this marker into `thinking_content` and `content` variables, respectively. This allows for separate analysis of the model's internal reasoning and its final answer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")"
      ],
      "metadata": {
        "id": "GrluICWc4UNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee64777c"
      },
      "source": [
        "This cell imports `Markdown` from `IPython.display`. This utility is used to render markdown-formatted strings directly in the Jupyter/Colab output, making the display of the model's output (both thinking and final content) more readable and structured."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "AxXuclP64tJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31d5e459"
      },
      "source": [
        "This cell displays the `thinking_content` extracted in the previous parsing step. Viewing this output helps understand the model's internal reasoning process, how it breaks down the problem, and what steps it considers before formulating its final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f473c489"
      },
      "source": [
        "This cell displays the final `content` generated by the Qwen3-0.6B model. This is the ultimate answer or explanation provided by the LLM, following its internal thought process."
      ]
    },
    {
      "source": [
        "# Display thinking content\n",
        "display(Markdown(thinking_content))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WCIsW93H5Bzk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Display content\n",
        "display(Markdown(content))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dsvYzROm4v5p",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "728fdf73"
      },
      "source": [
        "### Try - Fintech Application of LLM Reasoning\n",
        "\n",
        "**Scenario:** A fintech startup wants to use an LLM to provide basic, personalized financial advice or explain complex financial concepts to users. They want to ensure the LLM's responses are transparent and well-reasoned.\n",
        "\n",
        "**Task:** Modify the existing code cells in this notebook to perform the following:\n",
        "\n",
        "1.  **Change the `prompt`**: Update the `prompt` variable  to ask the Qwen3-0.6B model to explain a common fintech concept. Examples include:\n",
        "    *   \"Explain the concept of 'Decentralized Finance (DeFi)' and its potential impact on traditional banking.\"\n",
        "    *   \"Describe how blockchain technology is used in supply chain finance.\"\n",
        "    *   \"What are the pros and cons of algorithmic trading in financial markets?\"\n",
        "\n",
        "2.  **Analyze Thinking Content**: After running the cells, examine the `thinking_content` output. How does the model's internal 'thought process' contribute to the clarity or accuracy of its final explanation?\n",
        "\n",
        "3.  **Evaluate Clarity and Accuracy**: Based on the `content` output, how well does the Qwen3-0.6B model explain the chosen fintech concept? If there are any areas for improvement, think about how the prompt could be refined to get a better answer."
      ]
    }
  ]
}