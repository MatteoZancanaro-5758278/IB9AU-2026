{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOd6GWFJ2cwgwjmf1JS7cbV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/RAG_2_Long_Context_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f749fd84"
      },
      "source": [
        "## 1. Setup Environment\n",
        "\n",
        "This first section handles the installation of necessary libraries and imports modules required for our application. We'll install `gradio` to build our web interface and import `google.generativeai` to interact with the Gemini API.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBs4BVZl8eBb"
      },
      "outputs": [],
      "source": [
        "# 1. Install Gradio\n",
        "!pip install -q gradio\n",
        "\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "import time\n",
        "import os\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3d8a984"
      },
      "source": [
        "## 2. Configure Gemini API\n",
        "\n",
        "Before using Google's Generative AI models, we need to set up the API key. This cell retrieves your `GEMINI_API_KEY` from Colab's user data secrets and configures the `genai` library. Make sure you have your API key stored as 'GEMINI_API_KEY' in the Colab secrets manager.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup (Use the standard library)\n",
        "# Get your API key from https://aistudio.google.com/\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "C6fwZe_lpOT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54c4df59"
      },
      "source": [
        "## 3. Core Application Logic (Functions)\n",
        "\n",
        "This section defines the main functions that power our RAG (Retrieval-Augmented Generation) application:\n",
        "\n",
        "*   **`active_gemini_files`**: A global list to store file objects uploaded to the Gemini API.\n",
        "*   **`upload_and_process_files(files)`**: This function takes file paths from the Gradio interface, uploads them to the Gemini API, monitors their processing status, and returns a log of the results.\n",
        "*   **`chat_response(message, history)`**: This is the heart of the chat. It receives the user's message and chat history, constructs a prompt (including an 'auditor' system prompt to enforce citation from provided documents), and uses the `gemini-flash-latest` model to generate a response. It ensures that all claims are backed by source blocks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global list to store the uploaded file objects from Gemini\n",
        "active_gemini_files = []\n",
        "\n",
        "# --- FUNCTIONS ---\n",
        "\n",
        "def upload_and_process_files(files):\n",
        "    \"\"\"\n",
        "    Takes a list of file paths (from Gradio), uploads them to Gemini,\n",
        "    waits for processing, and returns a status message.\n",
        "    \"\"\"\n",
        "    global active_gemini_files\n",
        "    active_gemini_files = [] # Clear previous files\n",
        "\n",
        "    if not files:\n",
        "        return \"No files uploaded.\"\n",
        "\n",
        "    status_log = \"\"\n",
        "\n",
        "    for file_path in files:\n",
        "        # Upload to Gemini\n",
        "        print(f\"Uploading {file_path}...\")\n",
        "        g_file = genai.upload_file(file_path)\n",
        "\n",
        "        # Wait for processing\n",
        "        while g_file.state.name == \"PROCESSING\":\n",
        "            time.sleep(1)\n",
        "            g_file = genai.get_file(g_file.name)\n",
        "\n",
        "        if g_file.state.name == \"FAILED\":\n",
        "            status_log += f\"‚ùå Failed to process: {file_path}\\n\"\n",
        "        else:\n",
        "            active_gemini_files.append(g_file)\n",
        "            status_log += f\"‚úÖ Ready: {g_file.display_name}\\n\"\n",
        "\n",
        "    return status_log\n",
        "\n",
        "def chat_response(message, history):\n",
        "    \"\"\"\n",
        "    The chat function.\n",
        "    - message: The user's current question.\n",
        "    - history: Previous chat context (Gradio handles this).\n",
        "    \"\"\"\n",
        "    if not active_gemini_files:\n",
        "        return \"‚ö†Ô∏è Please upload and process documents first.\"\n",
        "\n",
        "    # The Auditor Prompt (Enforcing Citations)\n",
        "    auditor_prompt = \"\"\"\n",
        "    You are a strict Compliance Auditor.\n",
        "    Answer the user's question using ONLY the provided documents.\n",
        "\n",
        "    CRITICAL INSTRUCTION:\n",
        "    For every claim you make, you must provide a \"Source Block\" in this exact format:\n",
        "    [Source: Document Name | Page/Section | Exact Quote]\n",
        "\n",
        "    If the answer is not in the documents, state \"Information not found in provided records.\"\n",
        "    \"\"\"\n",
        "\n",
        "    # We combine the System Prompt + User Message + All Active Files\n",
        "    # Note: We pass the *list* of file objects directly to the model\n",
        "    input_content = [auditor_prompt, message] + active_gemini_files\n",
        "\n",
        "    try:\n",
        "        # Using the Flash model for large context window\n",
        "        model = genai.GenerativeModel(\"gemini-flash-latest\")\n",
        "        response = model.generate_content(input_content)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {str(e)}\""
      ],
      "metadata": {
        "id": "-lD-psVU9C_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25a2b9a7"
      },
      "source": [
        "## 4. Build the Web Interface with Gradio\n",
        "\n",
        "Here, we construct the user interface using the Gradio library. `gr.Blocks` allows for more flexible layouts. We create:\n",
        "\n",
        "*   A **File Uploader**: For users to select and upload documents.\n",
        "*   An **Upload Button**: To trigger the `upload_and_process_files` function.\n",
        "*   A **Status Output**: To display the processing status of the uploaded files.\n",
        "*   A **Chat Interface**: To interact with the AI model, including example questions.\n",
        "\n",
        "The `upload_btn.click` line links the button click event to our `upload_and_process_files` function, passing the uploaded files as input and updating the status output.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BUILD THE WEB UI ---\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# üè¶ HR RAG\")\n",
        "    gr.Markdown(\"Upload any number of documents (PDFs, Images, CSVs) and ask questions. The AI will cite its sources.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # File Uploader\n",
        "            file_input = gr.File(\n",
        "                label=\"1. Upload Documents\",\n",
        "                file_count=\"multiple\",\n",
        "                file_types=[\".pdf\", \".csv\", \".txt\", \".png\", \".jpg\"]\n",
        "            )\n",
        "            upload_btn = gr.Button(\"Process Files\", variant=\"primary\")\n",
        "            status_output = gr.Textbox(label=\"Status\", interactive=False, lines=4)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            # Chat Interface\n",
        "            chatbot = gr.ChatInterface(\n",
        "                fn=chat_response,\n",
        "                examples=[\"What is the dental deductible?\", \"Who approved the FMLA request?\", \"Summarize the drug testing policy.\"],\n",
        "                title=\"2. Chat\"\n",
        "            )\n",
        "\n",
        "    # Link the Upload Button to the processing function\n",
        "    upload_btn.click(\n",
        "        fn=upload_and_process_files,\n",
        "        inputs=file_input,\n",
        "        outputs=status_output\n",
        "    )\n"
      ],
      "metadata": {
        "id": "BvGNTX6Y8fEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2824b060"
      },
      "source": [
        "## 5. Launch the Application\n",
        "\n",
        "Finally, this cell launches the Gradio web application. The `demo.launch(debug=True)` command starts the server, making the interface accessible via a public URL provided by Gradio. `debug=True` is useful for seeing detailed logs during development."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the app\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "sNxmc8pP8fBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}