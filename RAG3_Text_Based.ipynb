{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "G4",
      "authorship_tag": "ABX9TyNKGS4LRdUwED+wNMESsuAa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/RAG3_Text_Based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d588cfff"
      },
      "source": [
        "## Building a RAG Solution\n",
        "\n",
        "###Step 1 - Install Required Packages\n",
        "\n",
        "Before we can run any of the code in this notebook, we need to make sure all necessary software libraries are installed. This step uses `pip`, Python's package installer, to download and install several libraries that are crucial for building our RAG (Retrieval Augmented Generation) system:\n",
        "\n",
        "*   **`llama-index`**: A data framework for LLM (Large Language Model) applications, used for data ingestion, indexing, and querying.\n",
        "*   **`llama-index-llms-huggingface`**: LlamaIndex integration for Hugging Face Large Language Models.\n",
        "*   **`llama-index-embeddings-huggingface`**: LlamaIndex integration for Hugging Face embedding models.\n",
        "*   **`llama-index-vector-stores-chroma`**: LlamaIndex integration for the Chroma vector database.\n",
        "*   **`chromadb`**: A fast, in-memory vector database used to store and search our document embeddings.\n",
        "*   **`pypdf`**: A library to work with PDF files, enabling us to read and extract text from our documents.\n",
        "*   **`sentence-transformers`**: Provides state-of-the-art pre-trained models for creating text embeddings.\n",
        "*   **`torch`**: PyTorch, a powerful open-source machine learning framework, essential for running the language models.\n",
        "*   **`accelerate`**: A Hugging Face library that simplifies using PyTorch models on different hardware (like GPUs) efficiently.\n",
        "*   **`bitsandbytes`**: A library for efficient 8-bit quantization of neural networks, which helps reduce memory usage and speed up inference for large models.\n",
        "\n",
        "The `-q` flag means 'quiet' (less output), and `-U` means 'upgrade' (ensure the latest version is installed). This ensures our environment has all the tools needed for the RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: INSTALL REQUIRED PACKAGES ---\n",
        "!pip install -q -U llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-vector-stores-chroma chromadb pypdf sentence-transformers torch accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "Q_T0poqI0hun",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "736aaef4"
      },
      "source": [
        "### Step 2 - Imports\n",
        "\n",
        "This section imports all the necessary modules and classes from the installed libraries. Each import plays a specific role in building our RAG system:\n",
        "\n",
        "*   **`os`**: Provides a way to interact with the operating system, for example, to create directories for storing documents.\n",
        "*   **`torch`**: The PyTorch library, essential for running and managing machine learning models, especially the LLM and embedding models.\n",
        "*   **`llama_index.core` components**:\n",
        "    *   **`VectorStoreIndex`**: The core class from LlamaIndex for creating and querying a vector index, which stores our document embeddings.\n",
        "    *   **`SimpleDirectoryReader`**: Used to easily load documents (like PDFs) from a specified directory.\n",
        "    *   **`StorageContext`**: Manages the storage backend for the LlamaIndex, telling it where to store the index data (in our case, ChromaDB).\n",
        "    *   **`Settings`**: A global configuration object for LlamaIndex, allowing us to set default models for embeddings and LLMs, as well as document parsing strategies.\n",
        "*   **`llama_index.core.node_parser.SentenceSplitter`**: A tool to break down large documents into smaller, manageable chunks (nodes) based on sentences, which is crucial for effective retrieval.\n",
        "*   **`llama_index.embeddings.huggingface.HuggingFaceEmbedding`**: Allows us to use pre-trained embedding models from Hugging Face to convert text into numerical vector representations.\n",
        "*   **`llama_index.llms.huggingface.HuggingFaceLLM`**: Enables the integration of Large Language Models (LLMs) available on Hugging Face directly into our LlamaIndex application.\n",
        "*   **`llama_index.vector_stores.chroma.ChromaVectorStore`**: The LlamaIndex adapter to use ChromaDB as the persistent vector store for our embeddings.\n",
        "*   **`chromadb`**: The client library for Chroma, used to directly interact with the Chroma vector database (e.g., to create or get collections)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 2: IMPORTS ---\n",
        "import os\n",
        "import torch\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, Settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "2B9n36J40qii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cdea723"
      },
      "source": [
        "### Step 3 - Global Settings (Embeddings and Chunking)\n",
        "\n",
        "This step is crucial for configuring how our RAG system processes and understands documents. We set up two main global settings for LlamaIndex:\n",
        "\n",
        "1.  **Embedding Model (`Settings.embed_model`)**:\n",
        "    *   `Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")`:\n",
        "        *   **Purpose**: An embedding model converts text into numerical vectors (embeddings). These vectors capture the semantic meaning of the text, allowing the RAG system to find relevant document chunks based on the similarity of their embeddings to the query's embedding.\n",
        "        *   **Model Choice**: We're using `\"sentence-transformers/all-MiniLM-L6-v2\"`. This is a popular, efficient, and effective pre-trained model from Hugging Face designed to create good quality sentence embeddings. It's a good balance of performance and computational cost.\n",
        "\n",
        "2.  **Node Parser / Chunking Strategy (`Settings.node_parser`)**:\n",
        "    *   `Settings.node_parser = SentenceSplitter(chunk_size=500, chunk_overlap=50)`:\n",
        "        *   **Purpose**: Large documents need to be broken down into smaller, manageable pieces (called 'nodes' or 'chunks') before they can be effectively stored and retrieved. This is because LLMs have a limited context window, and providing smaller, relevant chunks improves retrieval accuracy and reduces computational load.\n",
        "        *   **`SentenceSplitter`**: This specific parser splits documents into chunks based on sentence boundaries, which often leads to more coherent and meaningful chunks compared to splitting purely by character count.\n",
        "        *   **`chunk_size=500`**: Each chunk will aim to contain approximately 500 tokens (or words/subwords, depending on the tokenizer). This is a common size, balancing detail with brevity.\n",
        "        *   **`chunk_overlap=50`**: This creates an overlap of 50 tokens between consecutive chunks. Overlap helps ensure that context isn't lost when a piece of information spans across two chunks. For example, if a sentence is at the very end of one chunk, the beginning of that sentence will also appear in the next chunk, providing continuity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 3: GLOBAL SETTINGS (Embeddings and Chunking) ---\n",
        "# Embedding model (same as your original)\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Chunk size and overlap (equivalent to your RecursiveCharacterTextSplitter)\n",
        "Settings.node_parser = SentenceSplitter(chunk_size=500, chunk_overlap=50)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qkne9bWb0qcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e068e938"
      },
      "source": [
        "### Step 4 - Load Phi-3 Mini (Local LLM)\n",
        "\n",
        "This step initializes and configures the Large Language Model (LLM) that our RAG system will use to generate answers based on the retrieved documents. We are utilizing a powerful, open-source model from Hugging Face, specifically **Phi-3-mini-4k-instruct**.\n",
        "\n",
        "*   **`model_id = \"microsoft/Phi-3-mini-4k-instruct\"`**: This line specifies the exact model we want to load from the Hugging Face model hub. Phi-3-mini is a lightweight yet capable model developed by Microsoft, designed for instruction-following tasks, making it suitable for a RAG system.\n",
        "\n",
        "*   **`llm = HuggingFaceLLM(...)`**: We instantiate the `HuggingFaceLLM` class from LlamaIndex, which acts as a wrapper to easily integrate Hugging Face models.\n",
        "    *   **`model_name=model_id`**: Points to the model we defined earlier.\n",
        "    *   **`model_kwargs={\"torch_dtype\": torch.bfloat16}`**: This sets the data type for the model's computations to `bfloat16`. Using `bfloat16` (Brain Floating Point) is a common optimization for large models, as it reduces memory usage and speeds up calculations on compatible hardware (like modern GPUs) while maintaining sufficient precision.\n",
        "    *   **`device_map=\"auto\"`**: This crucial setting tells the `accelerate` library (which `HuggingFaceLLM` uses internally) to automatically distribute the model across available devices (e.g., GPU, CPU) in the most efficient way. For large models, this helps manage memory and ensures the model runs as fast as possible.\n",
        "    *   **`max_new_tokens=256`**: This parameter limits the maximum number of tokens the LLM will generate in its response. This helps control the length of the answers and manage computational resources.\n",
        "    *   **`generate_kwargs={}`**: This is a dictionary where you can pass additional arguments directly to the model's `generate` method (e.g., `temperature`, `top_p` for controlling creativity/randomness in generation).\n",
        "    *   **`tokenizer_name=model_id`**: Specifies that the tokenizer (which converts text into numerical tokens for the model) associated with the `model_id` should be loaded.\n",
        "\n",
        "*   **`Settings.llm = llm`**: Finally, we set this configured `llm` object as the default Large Language Model for all subsequent operations within our LlamaIndex application. This means any query engine or other LlamaIndex component will automatically use this `Phi-3-mini-4k-instruct` model for generating responses."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 4: LOAD PHI-3 MINI (Local LLM) ---\n",
        "print(\"Loading Phi-3-mini-4k-instruct (this takes ~1-2 minutes)...\")\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=model_id,\n",
        "    model_kwargs={\n",
        "        \"torch_dtype\": torch.bfloat16,\n",
        "    },\n",
        "    device_map=\"auto\",\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={},\n",
        "    tokenizer_name=model_id,\n",
        ")\n",
        "Settings.llm = llm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "i4ODxW0O06lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e894af0c"
      },
      "source": [
        "### Step 5 - Ingest PDFs\n",
        "\n",
        "This crucial step is responsible for loading your company's onboarding documents (in PDF format) into the RAG system. Here's a breakdown:\n",
        "\n",
        "1.  **Create Document Directory (`os.makedirs`)**:\n",
        "    *   `os.makedirs(\"onboarding_docs\", exist_ok=True)`: This line uses Python's `os` module to create a directory named `onboarding_docs` in the current working directory. The `exist_ok=True` argument ensures that if the directory already exists, no error is raised, making the script robust to multiple runs.\n",
        "    *   **Purpose**: This folder will be where you place all the PDF documents you want your RAG assistant to be able to answer questions about.\n",
        "\n",
        "2.  **Load Documents with `SimpleDirectoryReader`**:\n",
        "    *   `reader = SimpleDirectoryReader(input_dir=\"onboarding_docs\", required_exts=[\".pdf\"])`: We initialize `SimpleDirectoryReader` from `llama_index.core`. This class is designed to easily load documents from a specified directory.\n",
        "        *   `input_dir=\"onboarding_docs\"`: Tells the reader to look for files within the `onboarding_docs` folder.\n",
        "        *   `required_exts=[\".pdf\"]`: Specifies that only files with the `.pdf` extension should be considered for loading.\n",
        "    *   `documents = reader.load_data()`: This method executes the loading process. It reads all specified PDF files in the directory and converts their content into a format that LlamaIndex can process. Each page of a PDF typically becomes a separate 'document' object in the `documents` list.\n",
        "\n",
        "3.  **Verify Document Loading**:\n",
        "    *   The subsequent `if not documents:` block (`1TJKY1251BN6`) checks if any documents were actually loaded. If the `documents` list is empty, it prints a warning, prompting the user to upload PDFs. Otherwise, it confirms how many document pages were loaded.\n",
        "\n",
        "In summary, this step sets up the document storage, reads your PDF content, and prepares it for further processing by the RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 5: INGEST PDFs ---\n",
        "os.makedirs(\"onboarding_docs\", exist_ok=True)\n",
        "print(\"ðŸ‘‰ Please upload your PDFs to the 'onboarding_docs' folder now (if not already there).\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vZs3DrYW18Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = SimpleDirectoryReader(input_dir=\"onboarding_docs\", required_exts=[\".pdf\"])\n",
        "documents = reader.load_data()"
      ],
      "metadata": {
        "id": "m7fT1n4B_vHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not documents:\n",
        "    print(\"âš ï¸ No files found! Please upload PDFs to the folder.\")\n",
        "else:\n",
        "    print(f\"Loaded {len(documents)} document pages.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1TJKY1251BN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a931d7e1"
      },
      "source": [
        "### Step 6 - Build or Load Chroma Vector Index\n",
        "\n",
        "This step is where our processed documents are stored and indexed, allowing for efficient retrieval later. We use ChromaDB, a lightweight vector database, to store the numerical embeddings of our document chunks.\n",
        "\n",
        "1.  **Define Persistence Directory (`persist_dir`)**:\n",
        "    *   `persist_dir = \"./chroma_db\"`: This sets a local directory named `chroma_db` where ChromaDB will store its data. This ensures that our vector index is saved to disk and can be reused between Colab sessions, avoiding the need to re-index documents every time.\n",
        "\n",
        "2.  **Initialize Chroma Client and Collection**:\n",
        "    *   `chroma_client = chromadb.PersistentClient(path=persist_dir)`: We create a `PersistentClient` for ChromaDB, pointing it to our `persist_dir`. This client will manage the interaction with the database.\n",
        "    *   `chroma_collection = chroma_client.get_or_create_collection(\"onboarding_rag\")`: We get (if it exists) or create (if it doesn't) a collection named `onboarding_rag`. A collection in ChromaDB is where our document embeddings and their associated metadata are stored.\n",
        "\n",
        "3.  **Configure LlamaIndex Storage Context**:\n",
        "    *   `vector_store = ChromaVectorStore(chroma_collection=chroma_collection)`: We create a `ChromaVectorStore` instance, linking it to our `onboarding_rag` collection. This makes ChromaDB compatible with LlamaIndex's vector store interface.\n",
        "    *   `storage_context = StorageContext.from_defaults(vector_store=vector_store)`: We then create a `StorageContext` for LlamaIndex, telling it to use our configured `ChromaVectorStore` for all storage operations.\n",
        "\n",
        "4.  **Build or Load Index Logic**:\n",
        "    *   `if chroma_collection.count() == 0:`: This condition checks if the `onboarding_rag` collection is empty. If it is (meaning no embeddings have been stored yet), we proceed to build a new index.\n",
        "        *   `index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, show_progress=True)`: If the collection is empty, a new `VectorStoreIndex` is built from the `documents` (loaded in Step 5). LlamaIndex processes these documents, creates embeddings using the `Settings.embed_model` (from Step 3), and stores them in the `chroma_collection` using the `storage_context`.\n",
        "        *   `print(\"âœ… New index built and persisted.\")`: Confirms that a new index was created.\n",
        "    *   `else:`: If the `chroma_collection` is not empty (i.e., it already contains embeddings from a previous run).\n",
        "        *   `index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)`: LlamaIndex loads the existing index directly from our `ChromaVectorStore`. This saves time and computational resources as the embeddings don't need to be re-generated.\n",
        "        *   `print(\"âœ… Loaded existing index.\")`: Confirms that an existing index was loaded.\n",
        "\n",
        "This step ensures that our document content is efficiently stored and ready for retrieval during the query process."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 6: BUILD OR LOAD CHROMA VECTOR INDEX ---\n",
        "print(\"Building Vector Index (this takes a bit of time)...\")\n",
        "persist_dir = \"./chroma_db\"\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"onboarding_rag\")\n",
        "\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Build index if not exists, otherwise load\n",
        "if chroma_collection.count() == 0:\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents,\n",
        "        storage_context=storage_context,\n",
        "        show_progress=True\n",
        "    )\n",
        "    print(\"âœ… New index built and persisted.\")\n",
        "else:\n",
        "    index = VectorStoreIndex.from_vector_store(\n",
        "        vector_store,\n",
        "        storage_context=storage_context\n",
        "    )\n",
        "    print(\"âœ… Loaded existing index.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SGtgUuiA1GDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa70bf9d"
      },
      "source": [
        "### Step 7 - Create Query Engine (RAG Pipeline)\n",
        "\n",
        "This step is where we create the **query engine**, which is the core component that allows us to interact with our RAG system. The query engine takes a user's question, retrieves relevant information from our indexed documents, and then uses the LLM to generate a coherent answer.\n",
        "\n",
        "*   **`query_engine = index.as_query_engine(similarity_top_k=3)`**:\n",
        "    *   **`index.as_query_engine()`**: This method on our previously built `VectorStoreIndex` (from Step 6) creates a query engine. This engine orchestrates the entire RAG process: it receives a query, converts it into an embedding, uses that embedding to search the `ChromaVectorStore` for the most similar document chunks, and then passes those chunks along with the original query to the LLM for answer generation.\n",
        "    *   **`similarity_top_k=3`**: This is a crucial parameter for the retrieval part of RAG. It tells the query engine to retrieve the **top 3 most similar document chunks** (or nodes) from our vector store based on the similarity of their embeddings to the query's embedding. A higher `k` means more context is provided to the LLM, potentially leading to more comprehensive answers but also increasing processing time and token usage. A lower `k` focuses on the most relevant bits.\n",
        "\n",
        "After this step, our RAG pipeline is fully configured and ready to answer questions interactively!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 7: CREATE QUERY ENGINE (RAG Pipeline) ---\n",
        "# Top-k retrieval (same as your k=3)\n",
        "query_engine = index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "print(\"âœ… RAG Pipeline Ready!\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "v2DKVCUl1NCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "919b78b6"
      },
      "source": [
        "### Step 8 - Test the RAG Pipeline\n",
        "\n",
        "This final step demonstrates how to use the `query_engine` we built in Step 7 to ask questions and receive answers, along with the source documents that informed the answer. This is where you can interact with your fully functional RAG system.\n",
        "\n",
        "1.  **Define a Query**:\n",
        "    *   `query = \"What is the deductible for Dental and Vision?\"`: We define a natural language question that we want our RAG assistant to answer. This query will be passed to the `query_engine`.\n",
        "\n",
        "2.  **Execute the Query**:\n",
        "    *   `response = query_engine.query(query)`: This line sends the `query` to the `query_engine`. The engine then performs the following actions internally:\n",
        "        *   Converts the query into an embedding.\n",
        "        *   Uses this embedding to retrieve the most relevant document chunks from the Chroma vector store (based on `similarity_top_k=3` set in Step 7).\n",
        "        *   Passes these retrieved chunks and the original query to the loaded LLM (Phi-3 Mini from Step 4) to generate a coherent answer.\n",
        "\n",
        "3.  **Display the Answer**:\n",
        "    *   `print(\"ðŸ¤– Answer:\")` and `print(response.response)`: The generated answer from the LLM is extracted from the `response` object and printed. This is the natural language answer to your question.\n",
        "\n",
        "4.  **Display Source Documents**:\n",
        "    *   `print(\"ðŸ“„ Source Documents:\")` and the subsequent loop: This section iterates through `response.source_nodes`. Each `node` represents a chunk of text retrieved from your documents that was used by the LLM to formulate its answer.\n",
        "    *   `file_name = node.metadata.get(\"file_name\", \"Unknown file\")` and `page_label = node.metadata.get(\"page_label\", \"N/A\")`: For each source node, we extract metadata such as the original `file_name` and the `page_label` from where the chunk was extracted. This is crucial for transparency and allowing users to verify the information. If the metadata isn't available, default values are used.\n",
        "    *   This provides a clear trail of where the information came from, enhancing the trustworthiness and interpretability of the RAG system's output.\n",
        "\n",
        "By running this cell, you can see the RAG pipeline in action, demonstrating its ability to retrieve relevant information and synthesize it into an answer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 8: TEST IT ---\n",
        "query = \"What is the deductible for Dental and Vision?\"\n",
        "query = \"How much time off do I get?\"\n",
        "\n",
        "print(f\"\\nâ“ Asking: {query}\\n\")\n",
        "\n",
        "response = query_engine.query(query)\n",
        "\n",
        "print(\"ðŸ¤– Answer:\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(response.response)\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "print(\"\\nðŸ“„ Source Documents:\")\n",
        "print(\"--------------------------------------------------\")\n",
        "if response.source_nodes:\n",
        "    for i, node in enumerate(response.source_nodes):\n",
        "        file_name = node.metadata.get(\"file_name\", \"Unknown file\")\n",
        "        page_label = node.metadata.get(\"page_label\", \"N/A\")\n",
        "        print(f\"  {i+1}. File: {file_name}, Page: {page_label}\")\n",
        "else:\n",
        "    print(\"  No source documents found.\")\n",
        "print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a-vbqkxW0hro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "970012a0"
      },
      "source": [
        "## Building an User Interface for RAG\n",
        "\n",
        "We will use `gradio` (Construct the Gradio interface using `gradio.Interface) to create the interface that provides a text input for questions and text outputs for the answer and source documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "f6216d3c"
      },
      "source": [
        "print(\"Installing Gradio...\")\n",
        "!pip install -q gradio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1963866"
      },
      "source": [
        "## Define RAG Query Function\n",
        "\n",
        "We will need to create a Python function that takes a user query, calls the existing `query_engine`, and formats the response (answer and source documents) into a displayable string for the Gradio interface.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "04375e80"
      },
      "source": [
        "def query_rag(question):\n",
        "    \"\"\"\n",
        "    Queries the RAG pipeline with the given question and formats the response.\n",
        "    \"\"\"\n",
        "    response = query_engine.query(question)\n",
        "\n",
        "    # Extract the answer\n",
        "    answer = response.response\n",
        "\n",
        "    # Format source documents\n",
        "    source_docs_str = \"\\n\\nðŸ“„ Source Documents:\\n--------------------------------------------------\"\n",
        "    if response.source_nodes:\n",
        "        for i, node in enumerate(response.source_nodes):\n",
        "            file_name = node.metadata.get(\"file_name\", \"Unknown file\")\n",
        "            page_label = node.metadata.get(\"page_label\", \"N/A\")\n",
        "            source_docs_str += f\"\\n  {i+1}. File: {file_name}, Page: {page_label}\"\n",
        "    else:\n",
        "        source_docs_str += \"\\n  No source documents found.\"\n",
        "    source_docs_str += \"\\n--------------------------------------------------\"\n",
        "\n",
        "    # Combine answer and source documents\n",
        "    formatted_output = f\"ðŸ¤– Answer:\\n--------------------------------------------------\\n{answer}\\n--------------------------------------------------{source_docs_str}\"\n",
        "\n",
        "    return formatted_output\n",
        "\n",
        "print(\"Defined query_rag function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "febe6fa6"
      },
      "source": [
        "### Build the Gradio interface\n",
        "This involves importing Gradio, defining the interface with the `query_rag` function, and specifying textboxes for input and output, along with a title.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "22b7fe79"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "print(\"Building Gradio interface...\")\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=query_rag,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Type your question here...\"),\n",
        "    outputs=gr.Textbox(label=\"RAG Response\", lines=10), # Increased lines for output\n",
        "    title=\"Onboarding RAG Assistant\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Gradio interface built.\")\n",
        "\n",
        "# To launch the interface in a new cell later:\n",
        "# iface.launch(share=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac2960f7"
      },
      "source": [
        "### Launch the Interface\n",
        "The Gradio interface has been built; the next step is to launch it to allow interactive querying.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "06f78cc0"
      },
      "source": [
        "print(\"Launching Gradio interface...\")\n",
        "\n",
        "iface.launch(share=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EVALS\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPESvzpSCjpS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef6d1668"
      },
      "source": [
        "To effectively evaluate the quality of a RAG solution, an evaluation dataset is essential. This dataset typically consists of question-answer pairs, context snippets, and metadata like category and difficulty. These datasets can be constructed by human experts for high precision, or in some cases, a powerful Large Language Model (LLM) can be leveraged to generate a synthetic evaluation dataset. We will explore the latter approach, utilizing an LLM to construct our evaluation dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prompt to give an LLM\n",
        "Here is a prompt you can paste into a frontier LLM, along with one or more onboarding documents, and ask it to produce rows in that format:\n",
        "\n",
        "```\n",
        "You are helping to build an evaluation dataset for a Retrievalâ€‘Augmented Generation (RAG) system used to answer questions about the attached onboarding documents for a company.\n",
        "\n",
        "Task: Generate a diverse set of questionâ€“answer pairs that a new employee might ask after reading these documents. Every answer must be directly supported by the text in the documents (no outside knowledge).\n",
        "\n",
        "Output format: Return the data as CSV text with the following header:\n",
        "\n",
        "question,answer,context,category,difficulty\n",
        "\n",
        "Where:\n",
        "\n",
        "question: A realistic user question about the content.\n",
        "\n",
        "answer: A concise, correct answer grounded strictly in the documents.\n",
        "\n",
        "context: 2â€“5 sentences copied or lightly paraphrased from the documents that support the answer; keep them as a single CSV field.\n",
        "\n",
        "category: One of definition, fact, procedure, policy, example, reasoning.\n",
        "\n",
        "difficulty: One of easy, medium, hard, where hard requires reading and combining multiple parts of the documents.\n",
        "\n",
        "Requirements:\n",
        "\n",
        "Generate N rows (where N = 30 or 50, etc.).\n",
        "\n",
        "Cover all major sections of the documents.\n",
        "\n",
        "Include at least 20% multiâ€‘hop or reasoning questions that require combining information from multiple paragraphs or sections.\n",
        "\n",
        "Use double quotes around each field and escape any double quotes inside fields per CSV conventions.\n",
        "\n",
        "Now, produce the CSV only, with no explanation, code fences, or commentary.\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T4UDitwh_ZiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the evaluation dataset into a file (`Onboarding_eval.csv`)."
      ],
      "metadata": {
        "id": "CgCDP17I05sn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9adce996"
      },
      "source": [
        "This code block prepares the environment for evaluation by:\n",
        "\n",
        "1.  **Loading the Evaluation Dataset**: It loads your pre-compiled evaluation dataset from `Onboarding_eval.csv` into a pandas DataFrame. This dataset contains the questions, gold answers, contexts, categories, and difficulties for evaluating your RAG system.\n",
        "2.  **Re-initializing the Embedding Model**: It ensures that the same embedding model (`sentence-transformers/all-MiniLM-L6-v2`) used during the RAG solution's construction is re-initialized. This is crucial for consistent vector representation when loading the index.\n",
        "3.  **Loading the ChromaDB Vector Index and Query Engine**: It connects to your existing ChromaDB instance (persisted in `./chroma_db`), retrieves the 'onboarding_rag' collection, and then reconstructs the `LlamaIndex` vector store and `query_engine`. This allows the evaluation process to interact with your previously built RAG system."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# 1. Load your eval set\n",
        "df = pd.read_csv(\"Onboarding_eval.csv\")\n",
        "\n",
        "# 2. Re-initialize embedding model (required for loading index consistently)\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# 3. Load your existing ChromaDB LlamaIndex index / query engine\n",
        "persist_dir = \"./chroma_db\"  # This must match the directory where ChromaDB was persisted\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"onboarding_rag\")\n",
        "\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Load the index from the ChromaDB vector store\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store,\n",
        "    storage_context=storage_context\n",
        ")\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "Jm65OBrE87sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b675bb4c"
      },
      "source": [
        "##Metrics for Evaluation\n",
        "To assess the performance of our RAG solution, we will employ a multi-faceted evaluation approach, leveraging different metrics to capture various aspects of quality. We will primarily focus on three categories:\n",
        "\n",
        "1.  **Lexical Similarity**: This initial measure quantifies the word-for-word or character-level overlap between the RAG system's generated answers and the ground-truth answers. While straightforward, it can sometimes be too strict, penalizing semantically correct but differently phrased responses.\n",
        "2.  **Semantic Similarity**: Moving beyond exact word matches, semantic similarity assesses how closely the *meaning* of the RAG's answer aligns with the ground truth. This is often achieved using embedding models that convert text into numerical vectors, and then calculating the cosine similarity between these vectors.\n",
        "3.  **LLM-as-a-Judge**: For a more nuanced and human-like assessment, we will utilize a high-quality Large Language Model (LLM) to act as a 'judge'. This LLM will evaluate the correctness, completeness, and overall quality of the RAG's answers by comparing them against the reference answers and providing a score and explanation. This method helps overcome the limitations of purely lexical or semantic metrics by understanding context and intent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa84e53f"
      },
      "source": [
        "##Lexical Similarity\n",
        "This code block evaluates the **lexical similarity** between the `gold_answer` (ground truth) and the `model_answer` (RAG's output) using a character-level ratio.\n",
        "\n",
        "1.  **`lexical_f1` Function**: A simple function `lexical_f1` is defined. It takes two strings, `pred` (predicted answer) and `gold` (gold standard answer), converts them to lowercase, and then uses `difflib.SequenceMatcher` to calculate a ratio of similarity based on common substrings. This is a basic form of lexical similarity.\n",
        "2.  **Iterate and Query**: The code iterates through each question in the `df` DataFrame. For each question, it queries the `query_engine` (our RAG pipeline) to get the `model_answer`.\n",
        "3.  **Calculate Score**: The `lexical_f1` function is called to compare the `model_answer` and the `gold_answer`, and the resulting score is stored.\n",
        "4.  **Store Results**: The question, gold answer, model answer, category, difficulty, and the calculated lexical score are appended to a `results` list.\n",
        "5.  **Aggregate and Save**: Finally, the `results` list is converted into a pandas DataFrame (`results_df`), the mean lexical score is printed, and the DataFrame is saved to a CSV file named `Onboarding_eval_results_llamaindex.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "043db547"
      },
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def lexical_f1(pred: str, gold: str) -> float:\n",
        "    # toy similarity: character-level ratio (replace with token F1, ROUGE, etc.)\n",
        "    return SequenceMatcher(None, pred.lower(), gold.lower()).ratio()\n",
        "\n",
        "results = []\n",
        "for _, row in df.iterrows():\n",
        "    q = row[\"question\"]\n",
        "    gold = row[\"gold_answer\"] # Corrected column name from \"answer\" to \"gold_answer\"\n",
        "\n",
        "    resp = query_engine.query(q)\n",
        "    pred = resp.response\n",
        "    pred = pred.strip()\n",
        "\n",
        "    score = lexical_f1(pred, gold)\n",
        "\n",
        "    results.append({\n",
        "        \"question\": q,\n",
        "        \"gold_answer\": gold,\n",
        "        \"model_answer\": pred,\n",
        "        \"category\": row[\"category\"],\n",
        "        \"difficulty\": row[\"difficulty\"],\n",
        "        \"lexical_sim\": score,\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean lexical score:\", results_df[\"lexical_sim\"].mean())"
      ],
      "metadata": {
        "id": "butyGj8SJZCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv('Onboarding_eval_results.csv', index=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LKFfIHRDFG2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Semantic Similarity"
      ],
      "metadata": {
        "id": "nDPovELlCCSQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ea1185"
      },
      "source": [
        "This code block calculates the semantic similarity between the `gold_answer` (ground truth) and the `model_answer` (RAG's output) using a pre-trained `sentence-transformers` model (`all-MiniLM-L6-v2`).\n",
        "\n",
        "1.  **Loads Results**: It first loads the CSV file containing the model's answers (`Onboarding_eval_results_llamaindex.csv`).\n",
        "2.  **Initializes Embedding Model**: It loads a lightweight SentenceTransformer model to convert text into numerical embeddings.\n",
        "3.  **Calculates Cosine Similarity**: For each question, it encodes both the gold and model answers into embeddings and then computes the cosine similarity between them. Cosine similarity measures the angle between two vectors, indicating how similar their directions are (and thus, how semantically similar the texts are).\n",
        "4.  **Stores and Displays**: The computed `semantic_sim` score (ranging from -1 to 1, where 1 is identical) is added to the DataFrame, and the mean similarity is printed, along with a preview of the results. Finally, the updated DataFrame is saved to a new CSV file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q sentence-transformers\n",
        "\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# 1. Load the file that already has the model's answers\n",
        "df = pd.read_csv(\"/content/Onboarding_eval_results.csv\")\n",
        "\n",
        "# 2. Rename columns\n",
        "gold_col = \"gold_answer\"         # ground-truth answer\n",
        "pred_col = \"model_answer\"   # RAG / LlamaIndex answer\n",
        "\n",
        "# 3. Load a lightweight embedding model\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # small, fast[web:571]\n",
        "\n",
        "# 4. Compute semantic similarity for each row\n",
        "def semantic_sim(row):\n",
        "    emb = model.encode(\n",
        "        [str(row[gold_col]), str(row[pred_col])],\n",
        "        convert_to_tensor=True\n",
        "    )\n",
        "    return float(util.cos_sim(emb[0], emb[1]).item())  # in [-1, 1]\n",
        "\n",
        "df[\"semantic_sim\"] = df.apply(semantic_sim, axis=1)\n",
        "\n",
        "# 5. Inspect aggregate metrics and save\n",
        "print(\"Mean semantic similarity:\", df[\"semantic_sim\"].mean())\n",
        "print(df[[\"question\", gold_col, pred_col, \"semantic_sim\"]].head())\n",
        "\n",
        "df.to_csv(\"/content/Onboarding_eval_results.csv\", index=False)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SJ7d2pYoBqT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LLM-as-a-Judge"
      ],
      "metadata": {
        "id": "NZrOFdXdDZtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use another strong LLM for the evaluation. The prompt to the LLM should include the following:\n",
        "\n",
        "```\n",
        "You are evaluating the correctness of a model's answer against a gold reference answer. You will receive:\n",
        "\n",
        "**question**: the user question  \n",
        "\n",
        "**reference_answer**: the ideal gold answer\n",
        "\n",
        "**model_answer:** the answer produced by a model\n",
        "\n",
        "\n",
        "Task:\n",
        "1. Compare model_answer to reference_answer.\n",
        "2. Ignore superficial differences in wording.\n",
        "3. Focus on factual correctness and coverage of key points.\n",
        "4. Add following columns to the data\n",
        "*  \"score\": integer from 1 to 5 (1=totally incorrect, 5=fully correct and complete)\n",
        "*  \"explanation\": clear explanation of why gave the score with justification from that row. This should be based on comparing the reference_answer and the model_answer.\n",
        "*  \"change\": what should be changed in the reference_answer to make it better\n",
        "\n",
        "Save the resulting file as a csv\n",
        "```"
      ],
      "metadata": {
        "id": "y74WBTwMOPv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LLM-as-a-Judge Feedback\n",
        "In addition to providing an evaluation of the RAG solution, LLM-as-a-Judge can also be used to improve the RAG solution. For example, ask the LLM to *improve the prompt for the RAG model to enhance the performance.*"
      ],
      "metadata": {
        "id": "kjdToJ6dTrso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“º Recommended Reading\n",
        "Weaviate-Context-Engineering-ebook.pdf"
      ],
      "metadata": {
        "id": "Ll8jbGoQTdIh"
      }
    }
  ]
}