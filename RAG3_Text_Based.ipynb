{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "H100",
      "authorship_tag": "ABX9TyOpiAv2S48kJPsDg9RhMYVv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/RAG3_Text_Based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d588cfff"
      },
      "source": [
        "## Building a RAG Solution\n",
        "\n",
        "###Step 1 - Install Required Packages\n",
        "\n",
        "Before we can run any of the code in this notebook, we need to make sure all necessary software libraries are installed. This step uses `pip`, Python's package installer, to download and install several libraries that are crucial for building our RAG (Retrieval Augmented Generation) system:\n",
        "\n",
        "*   **`llama-index`**: A data framework for LLM (Large Language Model) applications, used for data ingestion, indexing, and querying.\n",
        "*   **`llama-index-llms-huggingface`**: LlamaIndex integration for Hugging Face Large Language Models.\n",
        "*   **`llama-index-embeddings-huggingface`**: LlamaIndex integration for Hugging Face embedding models.\n",
        "*   **`llama-index-vector-stores-chroma`**: LlamaIndex integration for the Chroma vector database.\n",
        "*   **`chromadb`**: A fast, in-memory vector database used to store and search our document embeddings.\n",
        "*   **`pypdf`**: A library to work with PDF files, enabling us to read and extract text from our documents.\n",
        "*   **`sentence-transformers`**: Provides state-of-the-art pre-trained models for creating text embeddings.\n",
        "*   **`torch`**: PyTorch, a powerful open-source machine learning framework, essential for running the language models.\n",
        "*   **`accelerate`**: A Hugging Face library that simplifies using PyTorch models on different hardware (like GPUs) efficiently.\n",
        "*   **`bitsandbytes`**: A library for efficient 8-bit quantization of neural networks, which helps reduce memory usage and speed up inference for large models.\n",
        "\n",
        "The `-q` flag means 'quiet' (less output), and `-U` means 'upgrade' (ensure the latest version is installed). This ensures our environment has all the tools needed for the RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: INSTALL REQUIRED PACKAGES ---\n",
        "!pip install -q -U llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-vector-stores-chroma chromadb pypdf sentence-transformers torch accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "Q_T0poqI0hun",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "736aaef4"
      },
      "source": [
        "### Step 2 - Imports\n",
        "\n",
        "This section imports all the necessary modules and classes from the installed libraries. Each import plays a specific role in building our RAG system:\n",
        "\n",
        "*   **`os`**: Provides a way to interact with the operating system, for example, to create directories for storing documents.\n",
        "*   **`torch`**: The PyTorch library, essential for running and managing machine learning models, especially the LLM and embedding models.\n",
        "*   **`llama_index.core` components**:\n",
        "    *   **`VectorStoreIndex`**: The core class from LlamaIndex for creating and querying a vector index, which stores our document embeddings.\n",
        "    *   **`SimpleDirectoryReader`**: Used to easily load documents (like PDFs) from a specified directory.\n",
        "    *   **`StorageContext`**: Manages the storage backend for the LlamaIndex, telling it where to store the index data (in our case, ChromaDB).\n",
        "    *   **`Settings`**: A global configuration object for LlamaIndex, allowing us to set default models for embeddings and LLMs, as well as document parsing strategies.\n",
        "*   **`llama_index.core.node_parser.SentenceSplitter`**: A tool to break down large documents into smaller, manageable chunks (nodes) based on sentences, which is crucial for effective retrieval.\n",
        "*   **`llama_index.embeddings.huggingface.HuggingFaceEmbedding`**: Allows us to use pre-trained embedding models from Hugging Face to convert text into numerical vector representations.\n",
        "*   **`llama_index.llms.huggingface.HuggingFaceLLM`**: Enables the integration of Large Language Models (LLMs) available on Hugging Face directly into our LlamaIndex application.\n",
        "*   **`llama_index.vector_stores.chroma.ChromaVectorStore`**: The LlamaIndex adapter to use ChromaDB as the persistent vector store for our embeddings.\n",
        "*   **`chromadb`**: The client library for Chroma, used to directly interact with the Chroma vector database (e.g., to create or get collections)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 2: IMPORTS ---\n",
        "import os\n",
        "import torch\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, Settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "2B9n36J40qii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cdea723"
      },
      "source": [
        "### Step 3 - Global Settings (Embeddings and Chunking)\n",
        "\n",
        "This step is crucial for configuring how our RAG system processes and understands documents. We set up two main global settings for LlamaIndex:\n",
        "\n",
        "1.  **Embedding Model (`Settings.embed_model`)**:\n",
        "    *   `Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")`:\n",
        "        *   **Purpose**: An embedding model converts text into numerical vectors (embeddings). These vectors capture the semantic meaning of the text, allowing the RAG system to find relevant document chunks based on the similarity of their embeddings to the query's embedding.\n",
        "        *   **Model Choice**: We're using `\"sentence-transformers/all-MiniLM-L6-v2\"`. This is a popular, efficient, and effective pre-trained model from Hugging Face designed to create good quality sentence embeddings. It's a good balance of performance and computational cost.\n",
        "\n",
        "2.  **Node Parser / Chunking Strategy (`Settings.node_parser`)**:\n",
        "    *   `Settings.node_parser = SentenceSplitter(chunk_size=500, chunk_overlap=50)`:\n",
        "        *   **Purpose**: Large documents need to be broken down into smaller, manageable pieces (called 'nodes' or 'chunks') before they can be effectively stored and retrieved. This is because LLMs have a limited context window, and providing smaller, relevant chunks improves retrieval accuracy and reduces computational load.\n",
        "        *   **`SentenceSplitter`**: This specific parser splits documents into chunks based on sentence boundaries, which often leads to more coherent and meaningful chunks compared to splitting purely by character count.\n",
        "        *   **`chunk_size=500`**: Each chunk will aim to contain approximately 500 tokens (or words/subwords, depending on the tokenizer). This is a common size, balancing detail with brevity.\n",
        "        *   **`chunk_overlap=50`**: This creates an overlap of 50 tokens between consecutive chunks. Overlap helps ensure that context isn't lost when a piece of information spans across two chunks. For example, if a sentence is at the very end of one chunk, the beginning of that sentence will also appear in the next chunk, providing continuity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 3: GLOBAL SETTINGS (Embeddings and Chunking) ---\n",
        "# Embedding model (same as your original)\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Chunk size and overlap (equivalent to your RecursiveCharacterTextSplitter)\n",
        "Settings.node_parser = SentenceSplitter(chunk_size=500, chunk_overlap=50)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qkne9bWb0qcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e068e938"
      },
      "source": [
        "### Step 4 - Load Phi-3 Mini (Local LLM)\n",
        "\n",
        "This step initializes and configures the Large Language Model (LLM) that our RAG system will use to generate answers based on the retrieved documents. We are utilizing a powerful, open-source model from Hugging Face, specifically **Phi-3-mini-4k-instruct**.\n",
        "\n",
        "*   **`model_id = \"microsoft/Phi-3-mini-4k-instruct\"`**: This line specifies the exact model we want to load from the Hugging Face model hub. Phi-3-mini is a lightweight yet capable model developed by Microsoft, designed for instruction-following tasks, making it suitable for a RAG system.\n",
        "\n",
        "*   **`llm = HuggingFaceLLM(...)`**: We instantiate the `HuggingFaceLLM` class from LlamaIndex, which acts as a wrapper to easily integrate Hugging Face models.\n",
        "    *   **`model_name=model_id`**: Points to the model we defined earlier.\n",
        "    *   **`model_kwargs={\"torch_dtype\": torch.bfloat16}`**: This sets the data type for the model's computations to `bfloat16`. Using `bfloat16` (Brain Floating Point) is a common optimization for large models, as it reduces memory usage and speeds up calculations on compatible hardware (like modern GPUs) while maintaining sufficient precision.\n",
        "    *   **`device_map=\"auto\"`**: This crucial setting tells the `accelerate` library (which `HuggingFaceLLM` uses internally) to automatically distribute the model across available devices (e.g., GPU, CPU) in the most efficient way. For large models, this helps manage memory and ensures the model runs as fast as possible.\n",
        "    *   **`max_new_tokens=256`**: This parameter limits the maximum number of tokens the LLM will generate in its response. This helps control the length of the answers and manage computational resources.\n",
        "    *   **`generate_kwargs={}`**: This is a dictionary where you can pass additional arguments directly to the model's `generate` method (e.g., `temperature`, `top_p` for controlling creativity/randomness in generation).\n",
        "    *   **`tokenizer_name=model_id`**: Specifies that the tokenizer (which converts text into numerical tokens for the model) associated with the `model_id` should be loaded.\n",
        "\n",
        "*   **`Settings.llm = llm`**: Finally, we set this configured `llm` object as the default Large Language Model for all subsequent operations within our LlamaIndex application. This means any query engine or other LlamaIndex component will automatically use this `Phi-3-mini-4k-instruct` model for generating responses."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 4: LOAD PHI-3 MINI (Local LLM) ---\n",
        "print(\"Loading Phi-3-mini-4k-instruct (this takes ~1-2 minutes)...\")\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=model_id,\n",
        "    model_kwargs={\n",
        "        \"torch_dtype\": torch.bfloat16,\n",
        "    },\n",
        "    device_map=\"auto\",\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={},\n",
        "    tokenizer_name=model_id,\n",
        ")\n",
        "Settings.llm = llm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "i4ODxW0O06lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e894af0c"
      },
      "source": [
        "### Step 5 - Ingest PDFs\n",
        "\n",
        "This crucial step is responsible for loading your company's onboarding documents (in PDF format) into the RAG system. Here's a breakdown:\n",
        "\n",
        "1.  **Create Document Directory (`os.makedirs`)**:\n",
        "    *   `os.makedirs(\"onboarding_docs\", exist_ok=True)`: This line uses Python's `os` module to create a directory named `onboarding_docs` in the current working directory. The `exist_ok=True` argument ensures that if the directory already exists, no error is raised, making the script robust to multiple runs.\n",
        "    *   **Purpose**: This folder will be where you place all the PDF documents you want your RAG assistant to be able to answer questions about.\n",
        "\n",
        "2.  **Load Documents with `SimpleDirectoryReader`**:\n",
        "    *   `reader = SimpleDirectoryReader(input_dir=\"onboarding_docs\", required_exts=[\".pdf\"])`: We initialize `SimpleDirectoryReader` from `llama_index.core`. This class is designed to easily load documents from a specified directory.\n",
        "        *   `input_dir=\"onboarding_docs\"`: Tells the reader to look for files within the `onboarding_docs` folder.\n",
        "        *   `required_exts=[\".pdf\"]`: Specifies that only files with the `.pdf` extension should be considered for loading.\n",
        "    *   `documents = reader.load_data()`: This method executes the loading process. It reads all specified PDF files in the directory and converts their content into a format that LlamaIndex can process. Each page of a PDF typically becomes a separate 'document' object in the `documents` list.\n",
        "\n",
        "3.  **Verify Document Loading**:\n",
        "    *   The subsequent `if not documents:` block (`1TJKY1251BN6`) checks if any documents were actually loaded. If the `documents` list is empty, it prints a warning, prompting the user to upload PDFs. Otherwise, it confirms how many document pages were loaded.\n",
        "\n",
        "In summary, this step sets up the document storage, reads your PDF content, and prepares it for further processing by the RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 5: INGEST PDFs ---\n",
        "os.makedirs(\"onboarding_docs\", exist_ok=True)\n",
        "print(\"üëâ Please upload your PDFs to the 'onboarding_docs' folder now (if not already there).\")\n",
        "\n",
        "reader = SimpleDirectoryReader(input_dir=\"onboarding_docs\", required_exts=[\".pdf\"])\n",
        "documents = reader.load_data()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vZs3DrYW18Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not documents:\n",
        "    print(\"‚ö†Ô∏è No files found! Please upload PDFs to the folder.\")\n",
        "else:\n",
        "    print(f\"Loaded {len(documents)} document pages.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1TJKY1251BN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a931d7e1"
      },
      "source": [
        "### Step 6 - Build or Load Chroma Vector Index\n",
        "\n",
        "This step is where our processed documents are stored and indexed, allowing for efficient retrieval later. We use ChromaDB, a lightweight vector database, to store the numerical embeddings of our document chunks.\n",
        "\n",
        "1.  **Define Persistence Directory (`persist_dir`)**:\n",
        "    *   `persist_dir = \"./chroma_db\"`: This sets a local directory named `chroma_db` where ChromaDB will store its data. This ensures that our vector index is saved to disk and can be reused between Colab sessions, avoiding the need to re-index documents every time.\n",
        "\n",
        "2.  **Initialize Chroma Client and Collection**:\n",
        "    *   `chroma_client = chromadb.PersistentClient(path=persist_dir)`: We create a `PersistentClient` for ChromaDB, pointing it to our `persist_dir`. This client will manage the interaction with the database.\n",
        "    *   `chroma_collection = chroma_client.get_or_create_collection(\"onboarding_rag\")`: We get (if it exists) or create (if it doesn't) a collection named `onboarding_rag`. A collection in ChromaDB is where our document embeddings and their associated metadata are stored.\n",
        "\n",
        "3.  **Configure LlamaIndex Storage Context**:\n",
        "    *   `vector_store = ChromaVectorStore(chroma_collection=chroma_collection)`: We create a `ChromaVectorStore` instance, linking it to our `onboarding_rag` collection. This makes ChromaDB compatible with LlamaIndex's vector store interface.\n",
        "    *   `storage_context = StorageContext.from_defaults(vector_store=vector_store)`: We then create a `StorageContext` for LlamaIndex, telling it to use our configured `ChromaVectorStore` for all storage operations.\n",
        "\n",
        "4.  **Build or Load Index Logic**:\n",
        "    *   `if chroma_collection.count() == 0:`: This condition checks if the `onboarding_rag` collection is empty. If it is (meaning no embeddings have been stored yet), we proceed to build a new index.\n",
        "        *   `index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, show_progress=True)`: If the collection is empty, a new `VectorStoreIndex` is built from the `documents` (loaded in Step 5). LlamaIndex processes these documents, creates embeddings using the `Settings.embed_model` (from Step 3), and stores them in the `chroma_collection` using the `storage_context`.\n",
        "        *   `print(\"‚úÖ New index built and persisted.\")`: Confirms that a new index was created.\n",
        "    *   `else:`: If the `chroma_collection` is not empty (i.e., it already contains embeddings from a previous run).\n",
        "        *   `index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)`: LlamaIndex loads the existing index directly from our `ChromaVectorStore`. This saves time and computational resources as the embeddings don't need to be re-generated.\n",
        "        *   `print(\"‚úÖ Loaded existing index.\")`: Confirms that an existing index was loaded.\n",
        "\n",
        "This step ensures that our document content is efficiently stored and ready for retrieval during the query process."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 6: BUILD OR LOAD CHROMA VECTOR INDEX ---\n",
        "print(\"Building Vector Index (this takes a bit of time)...\")\n",
        "persist_dir = \"./chroma_db\"\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"onboarding_rag\")\n",
        "\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Build index if not exists, otherwise load\n",
        "if chroma_collection.count() == 0:\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents,\n",
        "        storage_context=storage_context,\n",
        "        show_progress=True\n",
        "    )\n",
        "    print(\"‚úÖ New index built and persisted.\")\n",
        "else:\n",
        "    index = VectorStoreIndex.from_vector_store(\n",
        "        vector_store,\n",
        "        storage_context=storage_context\n",
        "    )\n",
        "    print(\"‚úÖ Loaded existing index.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SGtgUuiA1GDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa70bf9d"
      },
      "source": [
        "### Step 7 - Create Query Engine (RAG Pipeline)\n",
        "\n",
        "This step is where we create the **query engine**, which is the core component that allows us to interact with our RAG system. The query engine takes a user's question, retrieves relevant information from our indexed documents, and then uses the LLM to generate a coherent answer.\n",
        "\n",
        "*   **`query_engine = index.as_query_engine(similarity_top_k=3)`**:\n",
        "    *   **`index.as_query_engine()`**: This method on our previously built `VectorStoreIndex` (from Step 6) creates a query engine. This engine orchestrates the entire RAG process: it receives a query, converts it into an embedding, uses that embedding to search the `ChromaVectorStore` for the most similar document chunks, and then passes those chunks along with the original query to the LLM for answer generation.\n",
        "    *   **`similarity_top_k=3`**: This is a crucial parameter for the retrieval part of RAG. It tells the query engine to retrieve the **top 3 most similar document chunks** (or nodes) from our vector store based on the similarity of their embeddings to the query's embedding. A higher `k` means more context is provided to the LLM, potentially leading to more comprehensive answers but also increasing processing time and token usage. A lower `k` focuses on the most relevant bits.\n",
        "\n",
        "After this step, our RAG pipeline is fully configured and ready to answer questions interactively!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 7: CREATE QUERY ENGINE (RAG Pipeline) ---\n",
        "# Top-k retrieval (same as your k=3)\n",
        "query_engine = index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "print(\"‚úÖ RAG Pipeline Ready!\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "v2DKVCUl1NCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "919b78b6"
      },
      "source": [
        "### Step 8 - Test the RAG Pipeline\n",
        "\n",
        "This final step demonstrates how to use the `query_engine` we built in Step 7 to ask questions and receive answers, along with the source documents that informed the answer. This is where you can interact with your fully functional RAG system.\n",
        "\n",
        "1.  **Define a Query**:\n",
        "    *   `query = \"What is the deductible for Dental and Vision?\"`: We define a natural language question that we want our RAG assistant to answer. This query will be passed to the `query_engine`.\n",
        "\n",
        "2.  **Execute the Query**:\n",
        "    *   `response = query_engine.query(query)`: This line sends the `query` to the `query_engine`. The engine then performs the following actions internally:\n",
        "        *   Converts the query into an embedding.\n",
        "        *   Uses this embedding to retrieve the most relevant document chunks from the Chroma vector store (based on `similarity_top_k=3` set in Step 7).\n",
        "        *   Passes these retrieved chunks and the original query to the loaded LLM (Phi-3 Mini from Step 4) to generate a coherent answer.\n",
        "\n",
        "3.  **Display the Answer**:\n",
        "    *   `print(\"ü§ñ Answer:\")` and `print(response.response)`: The generated answer from the LLM is extracted from the `response` object and printed. This is the natural language answer to your question.\n",
        "\n",
        "4.  **Display Source Documents**:\n",
        "    *   `print(\"üìÑ Source Documents:\")` and the subsequent loop: This section iterates through `response.source_nodes`. Each `node` represents a chunk of text retrieved from your documents that was used by the LLM to formulate its answer.\n",
        "    *   `file_name = node.metadata.get(\"file_name\", \"Unknown file\")` and `page_label = node.metadata.get(\"page_label\", \"N/A\")`: For each source node, we extract metadata such as the original `file_name` and the `page_label` from where the chunk was extracted. This is crucial for transparency and allowing users to verify the information. If the metadata isn't available, default values are used.\n",
        "    *   This provides a clear trail of where the information came from, enhancing the trustworthiness and interpretability of the RAG system's output.\n",
        "\n",
        "By running this cell, you can see the RAG pipeline in action, demonstrating its ability to retrieve relevant information and synthesize it into an answer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 8: TEST IT ---\n",
        "query = \"What is the deductible for Dental and Vision?\"\n",
        "query = \"How much time off do I get?\"\n",
        "\n",
        "print(f\"\\n‚ùì Asking: {query}\\n\")\n",
        "\n",
        "response = query_engine.query(query)\n",
        "\n",
        "print(\"ü§ñ Answer:\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(response.response)\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "print(\"\\nüìÑ Source Documents:\")\n",
        "print(\"--------------------------------------------------\")\n",
        "if response.source_nodes:\n",
        "    for i, node in enumerate(response.source_nodes):\n",
        "        file_name = node.metadata.get(\"file_name\", \"Unknown file\")\n",
        "        page_label = node.metadata.get(\"page_label\", \"N/A\")\n",
        "        print(f\"  {i+1}. File: {file_name}, Page: {page_label}\")\n",
        "else:\n",
        "    print(\"  No source documents found.\")\n",
        "print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a-vbqkxW0hro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "970012a0"
      },
      "source": [
        "## Building an User Interface for RAG\n",
        "\n",
        "We will use `gradio` (Construct the Gradio interface using `gradio.Interface) to create the interface that provides a text input for questions and text outputs for the answer and source documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "f6216d3c"
      },
      "source": [
        "print(\"Installing Gradio...\")\n",
        "!pip install -q gradio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1963866"
      },
      "source": [
        "## Define RAG Query Function\n",
        "\n",
        "We will need to create a Python function that takes a user query, calls the existing `query_engine`, and formats the response (answer and source documents) into a displayable string for the Gradio interface.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "04375e80"
      },
      "source": [
        "def query_rag(question):\n",
        "    \"\"\"\n",
        "    Queries the RAG pipeline with the given question and formats the response.\n",
        "    \"\"\"\n",
        "    response = query_engine.query(question)\n",
        "\n",
        "    # Extract the answer\n",
        "    answer = response.response\n",
        "\n",
        "    # Format source documents\n",
        "    source_docs_str = \"\\n\\nüìÑ Source Documents:\\n--------------------------------------------------\"\n",
        "    if response.source_nodes:\n",
        "        for i, node in enumerate(response.source_nodes):\n",
        "            file_name = node.metadata.get(\"file_name\", \"Unknown file\")\n",
        "            page_label = node.metadata.get(\"page_label\", \"N/A\")\n",
        "            source_docs_str += f\"\\n  {i+1}. File: {file_name}, Page: {page_label}\"\n",
        "    else:\n",
        "        source_docs_str += \"\\n  No source documents found.\"\n",
        "    source_docs_str += \"\\n--------------------------------------------------\"\n",
        "\n",
        "    # Combine answer and source documents\n",
        "    formatted_output = f\"ü§ñ Answer:\\n--------------------------------------------------\\n{answer}\\n--------------------------------------------------{source_docs_str}\"\n",
        "\n",
        "    return formatted_output\n",
        "\n",
        "print(\"Defined query_rag function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "febe6fa6"
      },
      "source": [
        "### Build the Gradio interface\n",
        "This involves importing Gradio, defining the interface with the `query_rag` function, and specifying textboxes for input and output, along with a title.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "22b7fe79"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "print(\"Building Gradio interface...\")\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=query_rag,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Type your question here...\"),\n",
        "    outputs=gr.Textbox(label=\"RAG Response\", lines=10), # Increased lines for output\n",
        "    title=\"Onboarding RAG Assistant\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Gradio interface built.\")\n",
        "\n",
        "# To launch the interface in a new cell later:\n",
        "# iface.launch(share=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac2960f7"
      },
      "source": [
        "### Launch the Interface\n",
        "The Gradio interface has been built; the next step is to launch it to allow interactive querying.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "06f78cc0"
      },
      "source": [
        "print(\"Launching Gradio interface...\")\n",
        "\n",
        "iface.launch(share=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}