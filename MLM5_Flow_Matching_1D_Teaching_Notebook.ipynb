{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/MLM5_Flow_Matching_1D_Teaching_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t1tabZIRIspe",
      "metadata": {
        "id": "t1tabZIRIspe"
      },
      "source": [
        "# ðŸš€ Flow Matching in 1D â€” A Deterministic Alternative to Diffusion\n",
        "\n",
        "This notebook builds a **1D Flow Matching** (FM) model â€” a deterministic ODE-based generative model â€” on a **multimodal** data distribution.\n",
        "\n",
        "We will:\n",
        "1) Create a **quadâ€‘modal** 1D data distribution and visualize it.  \n",
        "2) Define a **flow matching** training objective with a **linear probability path**.  \n",
        "3) Train a tiny MLP to **predict the velocity field**\n",
        "$v_\\theta(x,t)$.  \n",
        "4) **Sample** by integrating the ODE\n",
        "$\\dot x = v_\\theta(x,t)$ from base noise to data.  \n",
        "5) Compare generated samples to the original data.\n",
        "\n",
        "> **Why Flow Matching?** Unlike diffusion (which trains via **noise prediction** and samples via a **reverse SDE/ODE**), flow matching trains by **regressing a velocity field** along a pre-defined path between a **base** distribution $p_0$ and the **data** distribution $p_1$; sampling solves a **deterministic ODE** from $t=0$ to $t=1$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F-xXLqjqIspj",
      "metadata": {
        "id": "F-xXLqjqIspj"
      },
      "source": [
        "## Setup\n",
        "We rely on PyTorch for a tiny MLP and Matplotlib for plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xMiGdn5AIspj",
      "metadata": {
        "id": "xMiGdn5AIspj"
      },
      "outputs": [],
      "source": [
        "import math, random, numpy as np\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fY4-QvaIspl",
      "metadata": {
        "id": "5fY4-QvaIspl"
      },
      "source": [
        "## Data: a **quadâ€‘modal** 1D distribution\n",
        "\n",
        "We will use the same mixture of Gaussians as in the diffusion notebook for applesâ€‘toâ€‘apples comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ex5IkRY9Ispl",
      "metadata": {
        "id": "ex5IkRY9Ispl"
      },
      "outputs": [],
      "source": [
        "# Mixture of Gaussians (quadâ€‘modal) for p1(x)\n",
        "weights = np.array([0.20, 0.30, 0.10, 0.40])  # sums to 1\n",
        "means   = np.array([-6.0, -2.0,  2.0,  6.0])\n",
        "stds    = np.array([ 0.5,  0.3,  0.4,  0.6])\n",
        "assert np.isclose(weights.sum(), 1.0)\n",
        "\n",
        "N = 50000  # data points\n",
        "comps = np.random.choice(len(weights), size=N, p=weights)\n",
        "x_data = np.random.normal(loc=means[comps], scale=stds[comps]).astype(np.float32)\n",
        "\n",
        "def plot_hist(data, title, bins=200, range_=(-10,10)):\n",
        "    plt.figure()\n",
        "    plt.hist(data, bins=bins, range=range_, density=True)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"density\")\n",
        "    plt.show()\n",
        "\n",
        "plot_hist(x_data, \"Target data distribution p1(x) (quadâ€‘modal)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z7v3bjf5Ispm",
      "metadata": {
        "id": "z7v3bjf5Ispm"
      },
      "source": [
        "## Flow Matching (FM) with a **linear path**\n",
        "\n",
        "We set a base distribution $p_0(x) = \\mathcal{N}(0,1)$, a data distribution $p_1$ (the mixture above), and define a **linear interpolant**:\n",
        "\n",
        "$x_t = (1-t)\\,x_0 + t\\,x_1, \\quad t\\in[0,1]$,\n",
        "where $x_0\\sim p_0$ and $x_1\\sim p_1$.\n",
        "\n",
        "The **conditional** velocity field along this path is simply:\n",
        "$v^*(x_t,t\\mid x_0,x_1) = \\tfrac{d}{dt} x_t = x_1 - x_0$.\n",
        "\n",
        "**Training objective (Conditional Flow Matching):**\n",
        "sample $x_0,x_1,t$, compute $x_t$, and regress a model $v_\\theta(x_t,t)$ to the target $(x_1 - x_0)$ with meanâ€‘squared error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3N845RQLIspm",
      "metadata": {
        "id": "3N845RQLIspm"
      },
      "outputs": [],
      "source": [
        "# Build on-the-fly training samples for Conditional Flow Matching (linear path)\n",
        "device = torch.device('cpu')\n",
        "\n",
        "def sample_pairs(n):\n",
        "    # base p0 ~ N(0,1)\n",
        "    x0 = torch.randn(n, 1, device=device)\n",
        "    # data p1 ~ mixture (sampled above)\n",
        "    x1_np = np.random.choice(x_data, size=n, replace=True)\n",
        "    x1 = torch.from_numpy(x1_np).float().unsqueeze(1).to(device)\n",
        "    # time t ~ Uniform[0,1]\n",
        "    t = torch.rand(n, device=device)\n",
        "    # linear path: x_t = (1-t)*x0 + t*x1\n",
        "    xt = (1.0 - t).unsqueeze(1) * x0 + t.unsqueeze(1) * x1\n",
        "    # target velocity: v* = x1 - x0\n",
        "    v_star = x1 - x0\n",
        "    return xt, t, v_star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51afd96a",
      "metadata": {
        "id": "51afd96a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Generate a batch of training data\n",
        "xt, t, vstar = sample_pairs(1024)\n",
        "\n",
        "# Convert tensors to numpy arrays and then to a pandas DataFrame\n",
        "training_data_df = pd.DataFrame({\n",
        "    'xt': xt.squeeze().numpy(),\n",
        "    't': t.squeeze().numpy(),\n",
        "    'vstar': vstar.squeeze().numpy()\n",
        "})\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "display(training_data_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R7DCOb3mIspn",
      "metadata": {
        "id": "R7DCOb3mIspn"
      },
      "source": [
        "## A tiny network to predict the velocity $v_\\theta(x,t)$\n",
        "\n",
        "We use a small MLP with a sinusoidal **time embedding**, just like in the diffusion notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "px9UmhpfIspn",
      "metadata": {
        "id": "px9UmhpfIspn"
      },
      "outputs": [],
      "source": [
        "# Sinusoidal time embedding\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, dim=32, max_period=10000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_period = max_period\n",
        "    def forward(self, t):  # t: [B] in [0,1]\n",
        "        device = t.device\n",
        "        half = self.dim // 2\n",
        "        freqs = torch.exp(-math.log(self.max_period) * torch.arange(half, device=device) / half)\n",
        "        args = (t.float().unsqueeze(1)) * freqs.unsqueeze(0)\n",
        "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=1)\n",
        "        if self.dim % 2 == 1:\n",
        "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n",
        "        return emb\n",
        "\n",
        "class VelocityNet(nn.Module):\n",
        "    def __init__(self, t_dim=32, hidden=64):\n",
        "        super().__init__()\n",
        "        self.tembed = TimeEmbedding(dim=t_dim)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(1 + t_dim, hidden),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden, 1),\n",
        "        )\n",
        "    def forward(self, x_t, t):\n",
        "        te = self.tembed(t)\n",
        "        h = torch.cat([x_t, te], dim=1)\n",
        "        return self.net(h)\n",
        "\n",
        "model = VelocityNet().to(device)\n",
        "opt = optim.AdamW(model.parameters(), lr=2e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eCFUNsvIIspn",
      "metadata": {
        "id": "eCFUNsvIIspn"
      },
      "source": [
        "## Train with Conditional Flow Matching (CFM)\n",
        "\n",
        "Minimize $\\mathbb{E}\\,\\|v_\\theta(x_t,t) - (x_1-x_0)\\|^2$ with batches of tuples $(x_t, t, x_1 - x_0)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RjIvzxDFIspo",
      "metadata": {
        "id": "RjIvzxDFIspo"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "for step in range(2500):  # quick CPU-friendly run\n",
        "    xt, t, vstar = sample_pairs(1024)\n",
        "    pred = model(xt, t)\n",
        "    loss = F.mse_loss(pred, vstar)\n",
        "    opt.zero_grad(); loss.backward(); opt.step()\n",
        "    if (step+1) % 250 == 0:\n",
        "        losses.append(float(loss.detach().cpu()))\n",
        "        print(f\"step {step+1}: loss {loss.item():.6f}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.title(\"Training loss (every 250 steps)\")\n",
        "plt.xlabel(\"checkpoint\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fNH8L_ZNIspo",
      "metadata": {
        "id": "fNH8L_ZNIspo"
      },
      "source": [
        "## Sampling by integrating the ODE\n",
        "\n",
        "After training, we generate samples by solving the ODE\n",
        "\n",
        "\n",
        "$\\frac{dx}{dt} = v_\\theta(x,t)$ from $t=0$ to $t=1$.\n",
        "\n",
        "\n",
        "We will use Euler and Heun (improved Euler) integrators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C7iUxQc9Ispo",
      "metadata": {
        "id": "C7iUxQc9Ispo"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample_flow(n_samples=20000, steps=200, method='heun'):\n",
        "    x = torch.randn(n_samples, 1)  # x(0) ~ p0\n",
        "    dt = 1.0 / steps\n",
        "    for s in range(steps):\n",
        "        t = torch.full((n_samples,), (s/steps))\n",
        "        v = model(x, t)\n",
        "        if method == 'euler':\n",
        "            x = x + dt * v\n",
        "        else:  # Heun (predictor-corrector)\n",
        "            x_pred = x + dt * v\n",
        "            t_next = torch.full((n_samples,), ((s+1)/steps))\n",
        "            v_next = model(x_pred, t_next)\n",
        "            x = x + 0.5 * dt * (v + v_next)\n",
        "    return x.squeeze(1).numpy()\n",
        "\n",
        "x_gen_euler = sample_flow(method='euler')\n",
        "x_gen_heun  = sample_flow(method='heun')\n",
        "\n",
        "plot_hist(x_gen_euler, \"Generated (Euler ODE, T=200)\")\n",
        "plot_hist(x_gen_heun,  \"Generated (Heun ODE,  T=200)\")\n",
        "plot_hist(x_data,      \"Reference data p1(x) â€” for comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HMHS0C0IIspp",
      "metadata": {
        "id": "HMHS0C0IIspp"
      },
      "source": [
        "## Visualize the learned velocity field\n",
        "\n",
        "We can examine $v_\\theta(x,t)$ over a grid of $x$ values for a few $t$ snapshots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uNOGyJnkIspp",
      "metadata": {
        "id": "uNOGyJnkIspp"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def plot_velocity_slices(xs=np.linspace(-10,10,200).astype(np.float32), ts=[0.0, 0.25, 0.5, 0.75, 1.0]):\n",
        "    xs_torch = torch.from_numpy(xs).unsqueeze(1)\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure()\n",
        "    for tt in ts:\n",
        "        t = torch.full((len(xs),), tt)\n",
        "        v = model(xs_torch, t).squeeze(1).numpy()\n",
        "        plt.plot(xs, v, label=f\"t={tt:.2f}\")\n",
        "    plt.title(\"Learned velocity slices v_Î¸(x,t)\")\n",
        "    plt.xlabel(\"x\"); plt.ylabel(\"velocity\")\n",
        "    plt.legend(); plt.show()\n",
        "\n",
        "plot_velocity_slices()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PbO9_jNdIspp",
      "metadata": {
        "id": "PbO9_jNdIspp"
      },
      "source": [
        "## Recap â€” What changed vs. diffusion?\n",
        "\n",
        "- **Objective:** Diffusion learns to **predict noise**; Flow Matching learns to **predict velocity** along a path.  \n",
        "- **Dynamics:** Diffusion uses an SDE/ODE; FM uses a **deterministic ODE**.  \n",
        "- **Training data:** We pair $x_0\\sim p_0$, $x_1\\sim p_1$, sample $t\\sim\\mathcal U[0,1]$, compute $x_t$ and supervise $(x_1 - x_0)$.  \n",
        "- **Sampling:** Integrate $\\dot x = v_\\theta(x,t)$ from $t=0$ (noise) to $t=1$ (data).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iM1Ag-v6dyNs",
      "metadata": {
        "id": "iM1Ag-v6dyNs"
      },
      "source": [
        "# Optional Tasks\n",
        "\n",
        "## 1. 2D Distribution Generation with Flow Matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UE98Dnlcd2uN",
      "metadata": {
        "id": "UE98Dnlcd2uN"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Parameters\n",
        "N = 1000  # Number of points to sample\n",
        "x_min, x_max = -4, 4\n",
        "y_min, y_max = -4, 4\n",
        "resolution = 100  # Resolution of the grid\n",
        "\n",
        "# Create the grid\n",
        "x = np.linspace(x_min, x_max, resolution)\n",
        "y = np.linspace(y_min, y_max, resolution)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "\n",
        "# Checkerboard pattern\n",
        "length = 4\n",
        "checkerboard = np.indices((length, length)).sum(axis=0) % 2\n",
        "\n",
        "# Sample points in regions where checkerboard pattern is 1\n",
        "sampled_points = []\n",
        "while len(sampled_points) < N:\n",
        "    # Randomly sample a point within the x and y range\n",
        "    x_sample = np.random.uniform(x_min, x_max)\n",
        "    y_sample = np.random.uniform(y_min, y_max)\n",
        "\n",
        "    # Determine the closest grid index\n",
        "    i = int((x_sample - x_min) / (x_max - x_min) * length)\n",
        "    j = int((y_sample - y_min) / (y_max - y_min) * length)\n",
        "\n",
        "    # Check if the sampled point is in a region where checkerboard == 1\n",
        "    if checkerboard[j, i] == 1:\n",
        "        sampled_points.append((x_sample, y_sample))\n",
        "\n",
        "# Convert to NumPy array for easier plotting\n",
        "sampled_points = np.array(sampled_points)\n",
        "\n",
        "# Plot the checkerboard pattern\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(checkerboard, extent=(x_min, x_max, y_min, y_max), origin=\"lower\", cmap=ListedColormap([\"purple\", \"yellow\"]))\n",
        "\n",
        "# Plot sampled points\n",
        "plt.scatter(sampled_points[:, 0], sampled_points[:, 1], color=\"red\", marker=\"o\")\n",
        "plt.xlabel(\"X-axis\")\n",
        "plt.ylabel(\"Y-axis\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_HXyM9QtgJeZ",
      "metadata": {
        "id": "_HXyM9QtgJeZ"
      },
      "source": [
        "## 2. Exploring Different Probability Paths in Flow Matching\n",
        "\n",
        "**Objective:** To understand how the choice of the probability path impacts the training and performance of a 1D Flow Matching model, and to relate this concept to practical considerations in generative modeling.\n",
        "\n",
        "**Background:** In this notebook, we used a simple linear probability path ($x_t = (1-t)x_0 + tx_1$) to train the Flow Matching model. However, other paths can be used, such as those derived from Optimal Transport. The choice of path influences the target velocity field that the model learns to predict.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1.  **Review the Linear Path:** Briefly review the concept of the linear path and how the target velocity $(x_1 - x_0)$ is derived. Discuss why this target is used in the Conditional Flow Matching objective.\n",
        "\n",
        "2.  **Introduce an Alternative Path (e.g., Quadratic Path):** Consider a simple non-linear path, for instance, a quadratic path:\n",
        "    $x_t = (1 - t)^2 x_0 + (1 - (1-t)^2) x_1$\n",
        "    *   Calculate the conditional velocity field $v^*(x_t, t \\mid x_0, x_1)$ for this quadratic path by taking the derivative with respect to `t`.\n",
        "    *   Discuss how this velocity field differs from the linear path's constant velocity.\n",
        "\n",
        "3.  **Modify the Code:** Adapt the `sample_pairs` function in the provided notebook to generate training data for the quadratic path. You will need to:\n",
        "    *   Implement the quadratic path formula to calculate `xt`.\n",
        "    *   Implement the newly derived conditional velocity formula to calculate the target `v_star`.\n",
        "\n",
        "4.  **Train a New Model:**\n",
        "    *   Instantiate a *new* `VelocityNet` model. It's important to train a separate model for this new path.\n",
        "    *   Train this new model using the modified `sample_pairs` function and the same training loop as before (you might need to adjust the number of training steps or learning rate).\n",
        "\n",
        "5.  **Sample and Evaluate:**\n",
        "    *   Use the trained model for the quadratic path to generate samples using the `sample_flow` function.\n",
        "    *   Compare the generated samples from the quadratic path model to the original data distribution and the samples generated by the linear path model (using histograms).\n",
        "\n",
        "6.  **Discuss the Findings (Practical Relevance):**\n",
        "    *   Compare the training convergence and the quality of the generated samples for both the linear and quadratic paths. Which path seems to work better for this specific 1D multimodal data?\n",
        "    *   Discuss the trade-offs of using different paths. Consider computational cost (is the target velocity easy to compute?), smoothness of the velocity field (does a smoother field make learning easier?), and theoretical guarantees (if any).\n",
        "    *   **Practical Connection:** How might the choice of probability path be relevant in real-world applications of generative modeling (e.g., image generation, text-to-speech)? Consider scenarios where a specific type of transformation or interpolation between noise and data might be desired or more efficient. For example, in image generation, would you want a linear interpolation in pixel space, or something that follows a more perceptually relevant path?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}