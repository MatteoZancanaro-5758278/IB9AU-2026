{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyObR58MWyHL1yBNrvO7kIa6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/MLM1_Vision_Transformers_and_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vision Transformers & Image Embeddings\n",
        "\n",
        "\n",
        "###Learning Objectives:\n",
        "1. Understand how Vision Transformers convert images to embeddings\n",
        "2. Explore CLIP's contrastive learning for text-image alignment\n",
        "3. Apply to financial document similarity search\n",
        "\n"
      ],
      "metadata": {
        "id": "JS_6M8ggmPxj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "130a9fe5"
      },
      "source": [
        "### Setting up the Environment\n",
        "\n",
        "Before we dive into the models, we need to set up our Python environment. This involves installing the necessary libraries and importing them. We'll be using `transformers` for Vision Transformer and CLIP models, `torch` for tensor operations, `pillow` for image handling, `sentence-transformers` for cosine similarity, `matplotlib` for plotting, `numpy` for numerical operations, and `requests` for fetching images from URLs.\n",
        "\n",
        "The code also checks if a GPU (`cuda`) is available, which significantly speeds up deep learning computations, and sets the `device` variable accordingly."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== SETUP ==========\n",
        "!pip -q install transformers torch pillow sentence-transformers matplotlib numpy requests\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from transformers import CLIPProcessor, CLIPModel, ViTImageProcessor, ViTModel\n",
        "from sentence_transformers import util\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n"
      ],
      "metadata": {
        "id": "wGiuKrnZmL4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c316d775"
      },
      "source": [
        "### Vision Transformer Embeddings\n",
        "\n",
        "Vision Transformers (ViT) are a type of neural network that applies the transformer architecture (originally designed for natural language processing) directly to images. Instead of using convolutional layers, ViTs split an image into fixed-size patches, linearly embed each patch, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder.\n",
        "\n",
        "The output of a ViT model includes a special `[CLS]` token's embedding, which serves as a global representation (embedding) of the entire image. Images with similar content will have similar embeddings in this high-dimensional space.\n",
        "\n",
        "This section demonstrates:\n",
        "1.  **Loading a pre-trained ViT model:** We use `google/vit-base-patch16-224-in21k`, a base-sized ViT model pre-trained on a large dataset.\n",
        "2.  **Loading sample financial images:** These images will be used to demonstrate how ViT processes different visual content.\n",
        "3.  **Processing images:** The `vit_processor` converts the images into the format expected by the ViT model.\n",
        "4.  **Generating embeddings:** The model processes the images and extracts a 768-dimensional embedding for each. The `[CLS]` token's embedding is typically used as the image representation.\n",
        "5.  **Calculating cosine similarity:** We use cosine similarity to measure how alike the generated embeddings are. A similarity value close to 1 indicates high similarity, while a value close to 0 indicates low similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iT93tGmLzj"
      },
      "source": [
        "# Vision Transformer Embeddings\n",
        "\n",
        "# Load ViT model and processor\n",
        "vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\n",
        "\n",
        "# Sample financial images (you can replace with your own)\n",
        "image_urls = [\n",
        "    \"https://cdn-icons-png.flaticon.com/512/2906/2906274.png\",  # Document\n",
        "    \"https://images.unsplash.com/photo-1554224155-6726b3ff858f?w=400\",  # Financial chart\n",
        "    \"https://images.unsplash.com/photo-1611974789855-9c2a0a7236a3?w=400\",  # Credit cards\n",
        "]\n",
        "\n",
        "def load_image(url):\n",
        "    \"\"\"Load image from URL\"\"\"\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "# Load and process images\n",
        "images = [load_image(url) for url in image_urls]\n",
        "\n",
        "# Display images\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "for idx, img in enumerate(images):\n",
        "    axes[idx].imshow(img)\n",
        "    axes[idx].set_title(f\"Image {idx+1}\")\n",
        "    axes[idx].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Get embeddings\n",
        "inputs = vit_processor(images=images, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = vit_model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # [CLS] token\n",
        "\n",
        "print(f\"Embedding shape per image: {embeddings[0].shape}\")\n",
        "print(f\"Total embeddings: {embeddings.shape[0]}\")\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity_matrix = util.cos_sim(embeddings, embeddings)\n",
        "print(\"\\nImage Similarity Matrix:\")\n",
        "print(similarity_matrix.numpy())\n",
        "\n",
        "# Create a DataFrame for embeddings\n",
        "import pandas as pd\n",
        "embedding_df = pd.DataFrame(embeddings, index=[f\"Image {i+1}\" for i in range(embeddings.shape[0])])\n",
        "print(\"\\nEmbedding Vectors (DataFrame):\")\n",
        "print(embedding_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20ee2003"
      },
      "source": [
        "### CLIP - Text-Image Alignment\n",
        "\n",
        "**Contrastive Language-Image Pre-training (CLIP)** is a neural network trained on a wide variety of (image, text) pairs. It learns to associate images with their descriptive texts. Unlike ViT, which generates embeddings only for images, CLIP generates embeddings for both images and text in a shared, multimodal embedding space.\n",
        "\n",
        "The key idea behind CLIP is **contrastive learning**: it learns to predict which text snippet best matches a given image from a random set of other text snippets. This allows it to understand the semantic relationship between images and text.\n",
        "\n",
        "In this section, we:\n",
        "1.  **Load a pre-trained CLIP model:** We use `openai/clip-vit-base-patch32`.\n",
        "2.  **Define text queries:** These are natural language descriptions related to financial concepts.\n",
        "3.  **Process images and text:** The `clip_processor` prepares both the images (from the previous section) and the text queries for the CLIP model.\n",
        "4.  **Get predictions:** The CLIP model outputs `logits_per_image`, which indicate the similarity scores between each image and each text query. These are then converted into probabilities using a softmax function.\n",
        "5.  **Display results:** The output shows the probability that each text query matches each of the three images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhXvbuO5mCn4"
      },
      "outputs": [],
      "source": [
        "#  CLIP - Text-Image Alignment\n",
        "print(\"\\n=== Part 2: CLIP Text-Image Matching ===\\n\")\n",
        "\n",
        "# Load CLIP model\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Financial domain queries\n",
        "text_queries = [\n",
        "    \"a financial document\",\n",
        "    \"a stock market chart\",\n",
        "    \"credit cards and banking\",\n",
        "    \"a business meeting\",\n",
        "    \"cryptocurrency trading\"\n",
        "]\n",
        "\n",
        "# Process images and text\n",
        "inputs = clip_processor(\n",
        "    text=text_queries,\n",
        "    images=images,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True\n",
        ").to(device)\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "    outputs = clip_model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "# Display results\n",
        "print(\"Text-to-Image Matching Probabilities:\\n\")\n",
        "print(f\"{'Query':<30} | {'Image 1':>8} | {'Image 2':>8} | {'Image 3':>8}\")\n",
        "print(\"-\" * 70)\n",
        "for idx, query in enumerate(text_queries):\n",
        "    print(f\"{query:<30} | {probs[0][idx]:.4f} | {probs[1][idx]:.4f} | {probs[2][idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76b0abb1"
      },
      "source": [
        "### FinTech Application - Document Classification\n",
        "\n",
        "One powerful application of CLIP's text-image alignment capability is zero-shot image classification. \"Zero-shot\" means the model can classify images into categories it has never explicitly seen during training, simply by comparing the image's embedding to the embedding of text descriptions of the categories.\n",
        "\n",
        "In this financial technology (FinTech) example, we simulate classifying financial documents:\n",
        "\n",
        "1.  **Define document types:** We create a list of possible financial document categories (e.g., \"bank statement\", \"investment prospectus\").\n",
        "2.  **Classify each image:** For each of our sample images, we:\n",
        "    *   Use the `clip_processor` to prepare the image and all document type queries.\n",
        "    *   Pass them to the `clip_model` to get similarity scores (`logits_per_image`).\n",
        "    *   Convert these scores to probabilities using softmax.\n",
        "    *   Identify the document type with the highest probability as the predicted class.\n",
        "\n",
        "This demonstrates how CLIP can be used to automatically categorize various financial documents based on textual descriptions of those categories, without needing a large, labeled dataset of financial images for each category."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  FinTech Application - Document Classification\n",
        "\n",
        "# Simulated document types\n",
        "\n",
        "doc_types = [\n",
        "    \"a financial document\",\n",
        "    \"a stock market chart\",\n",
        "    \"credit cards and banking\",\n",
        "    \"a business meeting\",\n",
        "    \"cryptocurrency trading\"\n",
        "]\n",
        "\n",
        "\n",
        "# Classify each image\n",
        "print(\"Document Classification Results:\\n\")\n",
        "for img_idx, img in enumerate(images):\n",
        "    inputs = clip_processor(\n",
        "        text=doc_types,\n",
        "        images=img,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "        probs = outputs.logits_per_image.softmax(dim=1)[0]\n",
        "\n",
        "    # Get top prediction\n",
        "    top_idx = probs.argmax().item()\n",
        "    confidence = probs[top_idx].item()\n",
        "\n",
        "    print(f\"Image {img_idx + 1}:\")\n",
        "    print(f\"  Predicted: {doc_types[top_idx]} (confidence: {confidence:.2%})\")\n",
        "    print(f\"  All probabilities: {dict(zip(doc_types, probs.tolist()))}\\n\")\n"
      ],
      "metadata": {
        "id": "fLdXW9ZY25i5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}