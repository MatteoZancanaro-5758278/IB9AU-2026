{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/SD2_Variational_Autoencoder_(VAE)_Illustration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "828ed2e4"
      },
      "source": [
        "## Understanding Variational Autoencoders (VAEs)\n",
        "\n",
        "This notebook demonstrates a Variational Autoencoder (VAE) applied to the MNIST dataset. Before diving into the code, let's clarify what a VAE is and how it relates to a standard Autoencoder.\n",
        "\n",
        "### Autoencoders (AE)\n",
        "\n",
        "A traditional Autoencoder is a neural network designed to learn an efficient, compressed representation (encoding) of input data. It consists of two main parts:\n",
        "1.  **Encoder**: Maps the input data to a lower-dimensional latent space representation.\n",
        "2.  **Decoder**: Reconstructs the original input data from the latent space representation.\n",
        "\n",
        "The goal of an AE is to minimize the reconstruction error, meaning the output should be as similar as possible to the input. While AEs can learn useful representations, their latent space is not continuous or easily sampled, making it difficult to generate new, meaningful data by sampling from this space.\n",
        "\n",
        "### Variational Autoencoders (VAE)\n",
        "\n",
        "A Variational Autoencoder addresses the limitations of standard Autoencoders by introducing a probabilistic approach to the latent space. Instead of mapping an input to a fixed point in the latent space, a VAE maps it to a **probability distribution** (specifically, a Gaussian distribution) in the latent space.\n",
        "\n",
        "Key differences and features of VAEs:\n",
        "*   **Probabilistic Latent Space**: The encoder outputs two vectors for each input: a mean vector (`z_mean`) and a logarithm of variance vector (`z_log_var`). These define the parameters of a Gaussian distribution.\n",
        "*   **Sampling**: Instead of directly using `z_mean` as the latent representation, a VAE samples from the distribution defined by `z_mean` and `z_log_var`. This sampling process introduces stochasticity, making the latent space smoother and more continuous.\n",
        "*   **Loss Function**: A VAE's loss function has two components:\n",
        "    1.  **Reconstruction Loss**: Measures how well the decoder reconstructs the input from the sampled latent vector (similar to AE).\n",
        "    2.  **KL Divergence Loss**: This is a regularization term that forces the learned latent distribution for each input to be close to a standard normal distribution (mean 0, variance 1). This ensures that the latent space is well-structured and continuous, allowing for meaningful sampling.\n",
        "*   **Generative Capability**: Because the latent space is structured and continuous, we can sample arbitrary points from the standard normal distribution, pass them through the decoder, and generate new, coherent data that resembles the training data.\n",
        "\n",
        "In essence, a VAE learns a compressed, continuous, and disentangled latent representation of the data, which not only allows for efficient reconstruction but also for the generation of novel data points by sampling from a simple prior distribution in the latent space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b5ad75d"
      },
      "source": [
        "### 1. Setup and Data Loading\n",
        "\n",
        "The following initializes the necessary libraries, defines global configuration parameters for our VAE model, and loads the MNIST dataset. The MNIST dataset, consisting of handwritten digits, is a classic choice for demonstrating generative models. We'll preprocess the images by normalizing pixel values and flattening them into one-dimensional vectors suitable for our dense network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "453a21ed"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "\n",
        "# --- Configuration ---\n",
        "LATENT_DIM_VAE = 2   # Use 2 for easy 2D visualization, or e.g., 32 for better quality\n",
        "IMAGE_SIZE = 784    # 28x28 pixels flattened\n",
        "EPOCHS = 10         # Number of training epochs (keep low for quick demo)\n",
        "BATCH_SIZE = 128    # Number of samples per gradient update\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- 1. Load and Preprocess MNIST Data ---\n",
        "print(\"Loading MNIST data...\")\n",
        "\n",
        "# Define a transform to flatten the images and normalize them\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Converts a PIL Image or numpy.ndarray to a FloatTensor of shape (C, H, W)\n",
        "    transforms.Lambda(lambda x: x.view(-1)), # Flatten 28x28 to 784\n",
        "    # MNIST pixel values are already 0-255, ToTensor scales them to 0-1. No explicit normalization needed beyond that\n",
        "])\n",
        "\n",
        "# Load MNIST training dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Load MNIST test dataset\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Training data samples: {len(train_dataset)}\")\n",
        "print(f\"Test data samples: {len(test_dataset)}\")\n",
        "\n",
        "# Verify data shape\n",
        "for images, labels in train_loader:\n",
        "    print(f\"Shape of images batch: {images.shape}\") # Expected: (BATCH_SIZE, IMAGE_SIZE)\n",
        "    print(f\"Shape of labels batch: {labels.shape}\") # Expected: (BATCH_SIZE)\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba4dacd"
      },
      "source": [
        "### 2. Defining VAE Components\n",
        "\n",
        "Here, we define the core components of our Variational Autoencoder:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ea49923"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Encoder ---\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_log_var = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is assumed to be flattened (batch_size, input_dim)\n",
        "        h = F.relu(self.fc1(x))\n",
        "        z_mean = self.fc_mean(h)\n",
        "        z_log_var = self.fc_log_var(h)\n",
        "\n",
        "        # Reparameterization trick\n",
        "        std = torch.exp(0.5 * z_log_var)\n",
        "        epsilon = torch.randn_like(std) # Sample from standard normal\n",
        "        z = z_mean + epsilon * std\n",
        "\n",
        "        return z_mean, z_log_var, z\n",
        "\n",
        "# --- Decoder ---\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        h = F.relu(self.fc1(z))\n",
        "        reconstruction = torch.sigmoid(self.fc2(h)) # Sigmoid for [0,1] pixel values\n",
        "        return reconstruction\n",
        "\n",
        "# --- VAE Model Class ---\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_mean, z_log_var, z = self.encoder(x)\n",
        "        reconstruction = self.decoder(z)\n",
        "        return reconstruction, z_mean, z_log_var\n",
        "\n",
        "\n",
        "hidden_dim = 128 # Example hidden dimension for both encoder and decoder\n",
        "\n",
        "# Create instances for summary output, similar to Keras summary\n",
        "encoder_temp = Encoder(IMAGE_SIZE, hidden_dim, LATENT_DIM_VAE).to(DEVICE)\n",
        "decoder_temp = Decoder(LATENT_DIM_VAE, hidden_dim, IMAGE_SIZE).to(DEVICE)\n",
        "\n",
        "print(\"\\nEncoder Architecture:\")\n",
        "print(encoder_temp)\n",
        "\n",
        "print(\"\\nDecoder Architecture:\")\n",
        "print(decoder_temp)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d03582c"
      },
      "source": [
        "### 3. Build and Train the VAE\n",
        "\n",
        "In this section, we instantiate our encoder and decoder, then combine them into the full VAE model. We compile the model with an Adam optimizer, and then train it using the preprocessed MNIST training data. During training, the VAE learns to both reconstruct the input images accurately and to maintain a well-structured, continuous latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86b7244a"
      },
      "source": [
        "print(\"\\n--- Building and Compiling VAE (PyTorch) ---\")\n",
        "# Hidden dimension is used in the Encoder and Decoder for the intermediate layer\n",
        "hidden_dim = 128\n",
        "\n",
        "# Instantiate Encoder, Decoder, and VAE\n",
        "encoder = Encoder(IMAGE_SIZE, hidden_dim, LATENT_DIM_VAE).to(DEVICE)\n",
        "decoder = Decoder(LATENT_DIM_VAE, hidden_dim, IMAGE_SIZE).to(DEVICE)\n",
        "vae = VAE(encoder, decoder).to(DEVICE)\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = optim.Adam(vae.parameters())\n",
        "\n",
        "# --- VAE Loss Function ---\n",
        "def vae_loss(reconstruction, x, z_mean, z_log_var):\n",
        "    # Reconstruction loss (Binary Cross-Entropy)\n",
        "    # F.binary_cross_entropy_with_logits is often used for raw logits, but our decoder output is sigmoid (0-1)\n",
        "    # So we use F.binary_cross_entropy\n",
        "    reconstruction_loss = F.binary_cross_entropy(reconstruction, x, reduction='sum')\n",
        "\n",
        "    # KL Divergence Loss\n",
        "    # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    # log(sigma^2) is z_log_var\n",
        "    kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
        "\n",
        "    # Total VAE loss\n",
        "    total_loss = reconstruction_loss + kl_loss\n",
        "    return total_loss, reconstruction_loss, kl_loss\n",
        "\n",
        "# --- Training Loop ---\n",
        "print(\"\\n--- Training Variational Autoencoder (PyTorch) ---\")\n",
        "history_vae = {'loss': [], 'reconstruction_loss': [], 'kl_loss': [], 'val_loss': [], 'val_reconstruction_loss': [], 'val_kl_loss': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    vae.train() # Set model to training mode\n",
        "    train_loss = 0.0\n",
        "    train_recon_loss = 0.0\n",
        "    train_kl_loss = 0.0\n",
        "\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(DEVICE) # Move data to appropriate device\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "\n",
        "        reconstruction, z_mean, z_log_var = vae(data)\n",
        "        total_batch_loss, recon_batch_loss, kl_batch_loss = vae_loss(reconstruction, data, z_mean, z_log_var)\n",
        "\n",
        "        total_batch_loss.backward() # Backpropagate\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "        train_loss += total_batch_loss.item()\n",
        "        train_recon_loss += recon_batch_loss.item()\n",
        "        train_kl_loss += kl_batch_loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataset)\n",
        "    avg_train_recon_loss = train_recon_loss / len(train_dataset)\n",
        "    avg_train_kl_loss = train_kl_loss / len(train_dataset)\n",
        "\n",
        "    history_vae['loss'].append(avg_train_loss)\n",
        "    history_vae['reconstruction_loss'].append(avg_train_recon_loss)\n",
        "    history_vae['kl_loss'].append(avg_train_kl_loss)\n",
        "\n",
        "    # --- Evaluation Loop ---\n",
        "    vae.eval() # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    val_recon_loss = 0.0\n",
        "    val_kl_loss = 0.0\n",
        "    with torch.no_grad(): # Disable gradient calculations during evaluation\n",
        "        for data, _ in test_loader:\n",
        "            data = data.to(DEVICE)\n",
        "            reconstruction, z_mean, z_log_var = vae(data)\n",
        "            total_batch_loss, recon_batch_loss, kl_batch_loss = vae_loss(reconstruction, data, z_mean, z_log_var)\n",
        "            val_loss += total_batch_loss.item()\n",
        "            val_recon_loss += recon_batch_loss.item()\n",
        "            val_kl_loss += kl_batch_loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_dataset)\n",
        "    avg_val_recon_loss = val_recon_loss / len(test_dataset)\n",
        "    avg_val_kl_loss = val_kl_loss / len(test_dataset)\n",
        "\n",
        "    history_vae['val_loss'].append(avg_val_loss)\n",
        "    history_vae['val_reconstruction_loss'].append(avg_val_recon_loss)\n",
        "    history_vae['val_kl_loss'].append(avg_val_kl_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "          f\"Train Recon Loss: {avg_train_recon_loss:.4f}, \"\n",
        "          f\"Train KL Loss: {avg_train_kl_loss:.4f}, \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
        "          f\"Val Recon Loss: {avg_val_recon_loss:.4f}, \"\n",
        "          f\"Val KL Loss: {avg_val_kl_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7036efdf"
      },
      "source": [
        "### 4. Visualize VAE Reconstructions\n",
        "\n",
        "After training, one of the first ways to evaluate a VAE is to see how well it can reconstruct images it has seen before. This function takes a few test images, passes them through the trained VAE (encoder then decoder), and displays the original alongside their reconstructed counterparts. This helps us understand the quality of the learned representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af7eab35"
      },
      "source": [
        "print(\"\\n--- Visualizing VAE Reconstructions (PyTorch) ---\")\n",
        "\n",
        "# Function to plot reconstructions\n",
        "def plot_reconstructions_vae(vae_model, test_loader, n=10):\n",
        "    \"\"\"Plots original and VAE reconstructed images using the PyTorch VAE model.\"\"\"\n",
        "    vae_model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        # Get a batch of test data\n",
        "        data_iter = iter(test_loader)\n",
        "        images, _ = next(data_iter)\n",
        "\n",
        "        # Ensure we have at least n images\n",
        "        if images.shape[0] < n:\n",
        "            print(f\"Not enough images in the batch ({images.shape[0]}) to plot {n}. Using available images.\")\n",
        "            n = images.shape[0]\n",
        "\n",
        "        # Move images to the device and get reconstructions\n",
        "        images = images.to(DEVICE)\n",
        "        reconstructed_images, _, _ = vae_model(images[:n])\n",
        "\n",
        "        # Move images back to CPU and convert to numpy for plotting\n",
        "        original_imgs_np = images[:n].cpu().numpy()\n",
        "        reconstructed_imgs_np = reconstructed_images.cpu().numpy()\n",
        "\n",
        "        plt.figure(figsize=(20, 4))\n",
        "        plt.suptitle(\"VAE: Original vs Reconstructed Images (PyTorch)\", fontsize=16)\n",
        "        for i in range(n):\n",
        "            # Display original\n",
        "            ax = plt.subplot(2, n, i + 1)\n",
        "            plt.imshow(original_imgs_np[i].reshape(28, 28), cmap='gray')\n",
        "            plt.title(\"Original\")\n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)\n",
        "\n",
        "            # Display reconstruction\n",
        "            ax = plt.subplot(2, n, i + 1 + n)\n",
        "            plt.imshow(reconstructed_imgs_np[i].reshape(28, 28), cmap='gray')\n",
        "            plt.title(\"Reconstructed\")\n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)\n",
        "        plt.show()\n",
        "\n",
        "# Plot VAE Reconstructions\n",
        "plot_reconstructions_vae(vae, test_loader, n=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6222492a"
      },
      "source": [
        "### 5. Generate New Images from Latent Space\n",
        "\n",
        "A key advantage of VAEs over traditional Autoencoders is their ability to generate entirely new data. Because the VAE's latent space is structured to resemble a standard normal distribution, we can sample random points from this distribution. When these random latent vectors are fed into the decoder, they produce novel, realistic-looking images that were not part of the training data. This demonstrates the VAE's generative power."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1dd6841"
      },
      "source": [
        "# --- VAE Specific Visualizations ---\n",
        "\n",
        "# 1. Generate new digits by sampling from the latent space prior\n",
        "def plot_generated_images(decoder_model, n=15, latent_dim=LATENT_DIM_VAE):\n",
        "    \"\"\"Generates images by sampling from the standard normal prior.\"\"\"\n",
        "    decoder_model.eval() # Set decoder to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        random_latent_vectors = torch.randn(n, latent_dim).to(DEVICE)\n",
        "        generated_images = decoder_model(random_latent_vectors)\n",
        "\n",
        "        # Move images back to CPU and convert to numpy for plotting\n",
        "        generated_images_np = generated_images.cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    plt.suptitle(\"VAE Generated Images (from random latent samples) (PyTorch)\", fontsize=16)\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(1, n, i + 1)\n",
        "        plt.imshow(generated_images_np[i].reshape(28, 28), cmap='gray')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nGenerating new images with VAE Decoder (PyTorch)...\")\n",
        "# Use the standalone decoder model accessed via vae.decoder\n",
        "plot_generated_images(vae.decoder, n=15, latent_dim=LATENT_DIM_VAE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20ac1f1c"
      },
      "source": [
        "### 6. Visualize the 2D Latent Space\n",
        "\n",
        "If we set the `LATENT_DIM_VAE` to 2, we can visually inspect the learned latent space. This plot takes a subset of the test data, encodes it, and then plots the `z_mean` values in a 2D scatter plot. Each point is colored according to its original digit class. A well-trained VAE will show distinct clusters for different digit classes, indicating that it has learned a meaningful and separable representation of the data in its latent space. This continuity allows for smooth transitions between different generated digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c876f5dc"
      },
      "source": [
        "print(\"\\nVisualizing VAE 2D Latent Space (PyTorch)...\")\n",
        "\n",
        "# 2. Visualize the learned VAE Latent Space (if LATENT_DIM_VAE == 2)\n",
        "def plot_latent_space(encoder_model, test_loader, latent_dim=LATENT_DIM_VAE):\n",
        "    \"\"\"Plots the 2D latent space (z_mean) colored by digit class for PyTorch.\"\"\"\n",
        "    if latent_dim != 2:\n",
        "        print(f\"Latent space visualization requires LATENT_DIM_VAE=2 (currently {latent_dim}). Skipping.\")\n",
        "        return\n",
        "\n",
        "    encoder_model.eval() # Set encoder to evaluation mode\n",
        "    z_mean_values = []\n",
        "    y_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(DEVICE)\n",
        "            # The encoder returns z_mean, z_log_var, z. We only need z_mean for this plot.\n",
        "            z_mean, _, _ = encoder_model(data)\n",
        "            z_mean_values.append(z_mean.cpu().numpy())\n",
        "            y_labels.append(target.cpu().numpy())\n",
        "\n",
        "    z_mean_values = np.concatenate(z_mean_values, axis=0)\n",
        "    y_labels = np.concatenate(y_labels, axis=0)\n",
        "\n",
        "    # Plot only a subset for clarity if dataset is too large\n",
        "    num_samples_plot = min(10000, len(z_mean_values))\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    scatter = plt.scatter(z_mean_values[:num_samples_plot, 0],\n",
        "                          z_mean_values[:num_samples_plot, 1],\n",
        "                          c=y_labels[:num_samples_plot],\n",
        "                          cmap='viridis', alpha=0.7, s=5)\n",
        "    plt.colorbar(scatter, label='Digit Class')\n",
        "    plt.xlabel(\"Latent Dimension 1 (z_mean)\")\n",
        "    plt.ylabel(\"Latent Dimension 2 (z_mean)\")\n",
        "    plt.title(\"VAE Latent Space (Mean Vectors - z_mean) (PyTorch)\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "if LATENT_DIM_VAE == 2:\n",
        "    plot_latent_space(vae.encoder, test_loader, latent_dim=LATENT_DIM_VAE)\n",
        "else:\n",
        "    print(f\"\\nSkipping Latent Space plot because LATENT_DIM_VAE is {LATENT_DIM_VAE} (requires 2).\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}