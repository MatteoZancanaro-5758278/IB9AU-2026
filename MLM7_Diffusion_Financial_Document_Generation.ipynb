{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/MLM7_Diffusion_Financial_Document_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Financial Document Generation with Diffusion\n",
        "\n",
        "CONTEXT: PRIVACY-PRESERVING SYNTHETIC DATA\n",
        "In FinTech, we often cannot train OCR (Optical Character Recognition) models\n",
        "on real client invoices due to GDPR/PII regulations.\n",
        "\n",
        "SOLUTION:\n",
        "We train a Diffusion model on the *structure* of invoices (layout, lines, headers)\n",
        "but without real text. We then use these 'hallucinated' documents to pre-train\n",
        "OCR systems or stress-test fraud detection pipelines.\n",
        "\n",
        "GOAL:\n",
        "Train a Conditional Diffusion Model that can generate:\n",
        "- Class 0: Invoices (Grid structure)\n",
        "- Class 1: Receipts (List structure)\n",
        "- Class 2: Charts (Visual structure)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "p9MxQ0hyCp6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from PIL import Image, ImageDraw"
      ],
      "metadata": {
        "id": "95N_OBYHCv8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. INTERNAL UTILITIES (Data Generator)\n",
        "# ==========================================\n",
        "# We define the generator here so the notebook is self-contained.\n",
        "\n",
        "def create_financial_document(doc_type=None, size=(64, 64)):\n",
        "    \"\"\"Generates a low-res synthetic financial document.\"\"\"\n",
        "    w, h = size\n",
        "    img = Image.new('L', size, color=255) # White background\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    if doc_type is None:\n",
        "        doc_type = random.choice(['invoice', 'receipt', 'chart'])\n",
        "\n",
        "    if doc_type == 'invoice':\n",
        "        # Header block\n",
        "        draw.rectangle([2, 2, w//3, 10], fill=200) # Logo\n",
        "        draw.rectangle([w//3 + 4, 4, w-5, 8], fill=0) # Title\n",
        "        # Grid Lines\n",
        "        margin = 5\n",
        "        line_height = (h - 20) // 5\n",
        "        start_y = 15\n",
        "        for i in range(5):\n",
        "            y = start_y + i * line_height\n",
        "            draw.line([margin, y, w-margin, y], fill=180, width=1)\n",
        "            draw.rectangle([margin, y+2, margin+10, y+line_height-2], fill=100) # Item\n",
        "            draw.rectangle([w-20, y+2, w-margin, y+line_height-2], fill=100) # Price\n",
        "        # Total\n",
        "        draw.rectangle([w-25, h-10, w-5, h-4], fill=0)\n",
        "\n",
        "    elif doc_type == 'receipt':\n",
        "        # Narrower content, centered\n",
        "        margin = w // 4\n",
        "        # Shop Header\n",
        "        draw.rectangle([margin, 2, w-margin, 8], fill=0)\n",
        "        # List of items\n",
        "        for y in range(12, h-15, 4):\n",
        "            draw.line([margin, y, margin + 10, y], fill=50, width=1) # Item\n",
        "            draw.line([w-margin-10, y, w-margin, y], fill=50, width=1) # Price\n",
        "        # Divider & Total\n",
        "        draw.line([margin, h-12, w-margin, h-12], fill=0, width=1)\n",
        "        draw.rectangle([margin+5, h-10, w-margin-5, h-4], fill=0)\n",
        "\n",
        "    elif doc_type == 'chart':\n",
        "        # Axes\n",
        "        draw.line([5, 5, 5, h-5], fill=0, width=1)\n",
        "        draw.line([5, h-5, w-5, h-5], fill=0, width=1)\n",
        "        # Random Bar or Line\n",
        "        if random.random() > 0.5:\n",
        "            bar_w = (w - 10) // 6\n",
        "            for i in range(5):\n",
        "                height = random.randint(5, h-10)\n",
        "                x = 10 + i * bar_w\n",
        "                draw.rectangle([x, h-5-height, x+bar_w-2, h-5], fill=150)\n",
        "        else:\n",
        "            points = []\n",
        "            for i in range(6):\n",
        "                x = 5 + i * ((w-10)//5)\n",
        "                y = h - 5 - random.randint(5, h-15)\n",
        "                points.append((x, y))\n",
        "            draw.line(points, fill=0, width=2)\n",
        "    return img\n",
        "\n",
        "class FinancialDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, num_samples=1000, size=(64, 64), transform=None):\n",
        "        self.num_samples = num_samples\n",
        "        self.size = size\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Pre-generate data\n",
        "        print(f\"Synthesizing {num_samples} documents...\")\n",
        "        for _ in range(num_samples):\n",
        "            label_str = random.choice(['invoice', 'receipt', 'chart'])\n",
        "            img = create_financial_document(label_str, size)\n",
        "            label_map = {'invoice': 0, 'receipt': 1, 'chart': 2}\n",
        "            self.data.append(img)\n",
        "            self.labels.append(label_map[label_str])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n"
      ],
      "metadata": {
        "id": "BK97YD-XDFwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. CONFIGURATION & MODEL\n",
        "# ==========================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "HPARAMS = {\n",
        "    \"img_size\": 64,\n",
        "    \"timesteps\": 500,\n",
        "    \"batch_size\": 32,\n",
        "    \"lr\": 3e-4,\n",
        "    \"epochs\": 10,\n",
        "    \"channels\": 1,\n",
        "    \"num_classes\": 3\n",
        "}\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = np.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
        "        if up:\n",
        "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
        "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
        "        time_emb = self.relu(self.time_mlp(t))\n",
        "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
        "        h = h + time_emb\n",
        "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
        "        return self.transform(h)\n",
        "\n",
        "class ConditionalUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        img_channels = HPARAMS[\"channels\"]\n",
        "        down_channels = (32, 64, 128)\n",
        "        up_channels = (128, 64, 32)\n",
        "        out_dim = img_channels\n",
        "        time_emb_dim = 32\n",
        "        classes = HPARAMS[\"num_classes\"]\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.class_emb = nn.Embedding(classes, time_emb_dim)\n",
        "        self.conv0 = nn.Conv2d(img_channels, down_channels[0], 3, padding=1)\n",
        "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], time_emb_dim) for i in range(len(down_channels)-1)])\n",
        "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], time_emb_dim, up=True) for i in range(len(up_channels)-1)])\n",
        "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
        "\n",
        "    def forward(self, x, timestep, class_label):\n",
        "        t = self.time_mlp(timestep)\n",
        "        c = self.class_emb(class_label)\n",
        "        t = t + c\n",
        "        x = self.conv0(x)\n",
        "        residuals = []\n",
        "        for down in self.downs:\n",
        "            x = down(x, t)\n",
        "            residuals.append(x)\n",
        "        for up in self.ups:\n",
        "            residual = residuals.pop()\n",
        "            x = torch.cat((x, residual), dim=1)\n",
        "            x = up(x, t)\n",
        "        return self.output(x)\n"
      ],
      "metadata": {
        "id": "KlNnmml_DUhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. TRAINING & SAMPLING\n",
        "# ==========================================\n",
        "class DiffusionManager:\n",
        "    def __init__(self, timesteps=500, start=0.0001, end=0.02):\n",
        "        self.timesteps = timesteps\n",
        "        self.betas = torch.linspace(start, end, timesteps).to(device)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
        "\n",
        "    def forward_noise(self, x_0, t):\n",
        "        noise = torch.randn_like(x_0)\n",
        "        sqrt_alpha_bar_t = self.sqrt_alphas_cumprod[t][:, None, None, None]\n",
        "        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n",
        "        return sqrt_alpha_bar_t * x_0 + sqrt_one_minus_alpha_bar_t * noise, noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, model, n_samples, class_label_idx, size):\n",
        "        model.eval()\n",
        "        x = torch.randn((n_samples, 1, size, size)).to(device)\n",
        "        labels = torch.full((n_samples,), class_label_idx, dtype=torch.long).to(device)\n",
        "        for i in reversed(range(1, self.timesteps)):\n",
        "            t = (torch.ones(n_samples) * i).long().to(device)\n",
        "            predicted_noise = model(x, t, labels)\n",
        "            alpha = self.alphas[t][:, None, None, None]\n",
        "            alpha_hat = self.alphas_cumprod[t][:, None, None, None]\n",
        "            beta = self.betas[t][:, None, None, None]\n",
        "            if i > 1:\n",
        "                noise = torch.randn_like(x)\n",
        "            else:\n",
        "                noise = torch.zeros_like(x)\n",
        "            x = (1 / torch.sqrt(alpha)) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
        "        model.train()\n",
        "        return x\n",
        "\n",
        "def train():\n",
        "    # 1. Prepare Data\n",
        "    dataset = FinancialDataset(num_samples=2000, size=(HPARAMS[\"img_size\"], HPARAMS[\"img_size\"]), transform=transforms.ToTensor())\n",
        "    dataloader = DataLoader(dataset, batch_size=HPARAMS[\"batch_size\"], shuffle=True)\n",
        "\n",
        "    # 2. Initialize\n",
        "    model = ConditionalUNet().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=HPARAMS[\"lr\"])\n",
        "    diffusion = DiffusionManager(timesteps=HPARAMS[\"timesteps\"])\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    print(f\"Starting training for {HPARAMS['epochs']} epochs...\")\n",
        "    for epoch in range(HPARAMS['epochs']):\n",
        "        pbar = tqdm(dataloader)\n",
        "        epoch_loss = 0\n",
        "        for step, (images, labels) in enumerate(pbar):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            t = torch.randint(0, HPARAMS[\"timesteps\"], (images.shape[0],), device=device).long()\n",
        "            x_noisy, noise = diffusion.forward_noise(images, t)\n",
        "            noise_pred = model(x_noisy, t, labels)\n",
        "            loss = loss_fn(noise_pred, noise)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_description(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n",
        "    return model, diffusion\n",
        "\n",
        "def visualize_results(model, diffusion):\n",
        "    print(\"\\nGenerating Class 0 (Invoices)...\")\n",
        "    invoices = diffusion.sample(model, 4, 0, HPARAMS[\"img_size\"])\n",
        "    print(\"Generating Class 1 (Receipts)...\")\n",
        "    receipts = diffusion.sample(model, 4, 1, HPARAMS[\"img_size\"])\n",
        "    print(\"Generating Class 2 (Charts)...\")\n",
        "    charts = diffusion.sample(model, 4, 2, HPARAMS[\"img_size\"])\n",
        "\n",
        "    all_images = torch.cat([invoices, receipts, charts], dim=0)\n",
        "    grid = make_grid(all_images, nrow=4, padding=2, normalize=True)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "    plt.title(\"Generated: Invoices (Top), Receipts (Mid), Charts (Bot)\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trained_model, diff_manager = train()\n",
        "    visualize_results(trained_model, diff_manager)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "BVsgcf64Cm9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. SAVE & LOAD UTILITIES (NEW)\n",
        "# ==========================================\n",
        "\n",
        "def save_model(model, filename=\"fintech_diffusion_model.pth\"):\n",
        "    \"\"\"Saves model weights to disk.\"\"\"\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    print(f\"‚úÖ Model saved to {filename}\")\n",
        "\n",
        "def load_model(filename=\"fintech_diffusion_model.pth\"):\n",
        "    \"\"\"Creates a new model instance and loads weights.\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"‚ùå Error: {filename} not found.\")\n",
        "        return None\n",
        "\n",
        "    model = ConditionalUNet().to(device)\n",
        "    model.load_state_dict(torch.load(filename, map_location=device))\n",
        "    model.eval()\n",
        "    print(f\"‚úÖ Model loaded from {filename}\")\n",
        "    return model\n",
        "\n",
        "def generate_single_document(model, diffusion, doc_type='invoice'):\n",
        "    \"\"\"Generates a single image of the requested type.\"\"\"\n",
        "    label_map = {'invoice': 0, 'receipt': 1, 'chart': 2}\n",
        "\n",
        "    if doc_type not in label_map:\n",
        "        print(f\"‚ùå Unknown type: {doc_type}. Use 'invoice', 'receipt', or 'chart'\")\n",
        "        return\n",
        "\n",
        "    print(f\"üé® Generating new {doc_type}...\")\n",
        "    sample_tensor = diffusion.sample(model, n_samples=1, class_label_idx=label_map[doc_type], size=HPARAMS[\"img_size\"])\n",
        "\n",
        "    # Convert tensor to displayable image\n",
        "    img = sample_tensor[0].cpu().permute(1, 2, 0).numpy()\n",
        "    img = (img - img.min()) / (img.max() - img.min()) # Normalize to 0-1\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(f\"Generated {doc_type.capitalize()}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_results(model, diffusion):\n",
        "    print(\"\\nGenerating Grid of Samples...\")\n",
        "    invoices = diffusion.sample(model, 4, 0, HPARAMS[\"img_size\"])\n",
        "    receipts = diffusion.sample(model, 4, 1, HPARAMS[\"img_size\"])\n",
        "    charts = diffusion.sample(model, 4, 2, HPARAMS[\"img_size\"])\n",
        "\n",
        "    all_images = torch.cat([invoices, receipts, charts], dim=0)\n",
        "    grid = make_grid(all_images, nrow=4, padding=2, normalize=True)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "    plt.title(\"Generated: Invoices (Top), Receipts (Mid), Charts (Bot)\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Train the model\n",
        "    trained_model, diff_manager = train()\n",
        "\n",
        "    # 2. Save the model\n",
        "    save_model(trained_model, \"fintech_diffusion_model.pth\")\n",
        "\n",
        "    # 3. Simulate \"Later Use\": Load the model back\n",
        "    loaded_model = load_model(\"fintech_diffusion_model.pth\")\n",
        "\n",
        "    # 4. Generate a specific new document\n",
        "    generate_single_document(loaded_model, diff_manager, doc_type='invoice')"
      ],
      "metadata": {
        "id": "YYmF8sCdP3QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Saved Model and Generate New Financial Documents"
      ],
      "metadata": {
        "id": "4jvYs9gML7tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from PIL import Image, ImageDraw\n"
      ],
      "metadata": {
        "id": "KGWb2isbKmgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Load the model back\n",
        "loaded_model = load_model(\"fintech_diffusion_model.pth\")\n",
        "\n",
        "# Generate a specific new document\n",
        "generate_single_document(loaded_model, diff_manager, doc_type='chart')"
      ],
      "metadata": {
        "id": "zBd6I2DVKgwh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}