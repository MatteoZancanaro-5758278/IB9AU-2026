{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/MLM4_Diffusion_1D_Teaching_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCeT9On8yf3g"
      },
      "source": [
        "# ðŸ§ª Diffusion Models in 1D â€” From Data to Gaussian and Back\n",
        "\n",
        "This notebook builds a **1D diffusion model** endâ€‘toâ€‘end on a **multiâ€‘modal** data distribution.  \n",
        "We will:\n",
        "1) Create a **quadâ€‘modal** data distribution and sample from it.  \n",
        "2) Implement the **forward diffusion** process (adding Gaussian noise step by step) and visualize how the data **becomes Gaussian**.  \n",
        "3) **Collect the right training tuples** from the forward process.  \n",
        "4) Train a tiny neural net to **predict noise** $\\epsilon$-prediction.  \n",
        "5) Use the network to **run the reverse diffusion** and regenerate samples from pure noise.\n",
        "\n",
        "**References for theory (recommended reading):**\n",
        "- Ho, Jain, Abbeel (2020), *Denoising Diffusion Probabilistic Models* (DDPM), arXiv:2006.11239.\n",
        "- Sohlâ€‘Dickstein et al. (2015), *Deep Unsupervised Learning using Nonequilibrium Thermodynamics*.\n",
        "- Nichol & Dhariwal (2021), *Improved DDPM* (cosine schedule).\n",
        "- Song et al. (2020), *Scoreâ€‘based Generative Modeling through SDEs*.\n",
        "\n",
        "> This notebook uses **only 1D data** so everything trains very quickly on CPU.\n"
      ],
      "id": "GCeT9On8yf3g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYnitclxyf3m"
      },
      "source": [
        "##Setup\n",
        "\n",
        "We rely on PyTorch for a tiny MLP and Matplotlib for plots."
      ],
      "id": "wYnitclxyf3m"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHcI6uESyf3m"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# If running on Colab, uncomment the next line to ensure torch is installed/updated.\n",
        "# !pip -q install torch --upgrade\n",
        "\n",
        "import math, random, numpy as np\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n"
      ],
      "id": "hHcI6uESyf3m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku31ASLJyf3o"
      },
      "source": [
        "## Build a **quadâ€‘modal** 1D data distribution\n",
        "\n",
        "We will define a mixture of four Gaussians with different means, scales, and weights.  Then we draw samples and visualize the histogram.\n"
      ],
      "id": "Ku31ASLJyf3o"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCCE0Twiyf3o"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Mixture of Gaussians (quad-modal)\n",
        "weights = np.array([0.20, 0.30, 0.10, 0.40])  # must sum to 1\n",
        "means   = np.array([-6.0, -2.0,  2.0,  6.0])\n",
        "stds    = np.array([ 0.5,  0.3,  0.4,  0.6])\n",
        "\n",
        "assert np.isclose(weights.sum(), 1.0)\n",
        "\n",
        "N = 50000  # number of data points\n",
        "comps = np.random.choice(len(weights), size=N, p=weights)\n",
        "x0 = np.random.normal(loc=means[comps], scale=stds[comps]).astype(np.float32)\n",
        "\n",
        "def plot_hist(data, title, bins=200, range_=(-10,10)):\n",
        "    plt.figure()\n",
        "    plt.hist(data, bins=bins, range=range_, density=True)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"density\")\n",
        "    plt.show()\n",
        "\n",
        "plot_hist(x0, \"Original data distribution (quad-modal)\")\n"
      ],
      "id": "WCCE0Twiyf3o"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xd4MOROyf3p"
      },
      "source": [
        "## Forward diffusion (DDPM style)\n",
        "\n",
        "\n",
        "We define a variance schedule\n",
        "$\\{\\beta_1,...,\\beta_T\\}$ and the forward Markov chain:\n",
        "\n",
        "$$\n",
        "q(x_t \\mid x_{t-1}) = \\mathcal{N}\\!\\big(\\sqrt{1-\\beta_t}\\,x_{t-1},\\, \\beta_t\\big)$$\n",
        "\n",
        "From this it follows that for any $t$:\n",
        "\n",
        "$$\n",
        "x_t = \\sqrt{\\bar\\alpha_t}\\,x_0 + \\sqrt{1-\\bar\\alpha_t}\\,\\epsilon,\\quad \\epsilon\\sim\\mathcal{N}(0,1) $$\n",
        "\n",
        "where\n",
        "$$\n",
        "\\alpha_t = 1-\\beta_t  \\  \\& \\  \n",
        " \\bar\\alpha_t=\\prod_{s=1}^t \\alpha_s\n",
        "$$\n",
        "\n",
        "We'll choose a small number of steps $T$ (e.g., 300) and a simple **linear** \\($\\beta_t\\$) schedule for clarity.\n"
      ],
      "id": "2xd4MOROyf3p"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEV5vAomyf3p"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Diffusion schedule\n",
        "T = 300\n",
        "beta_start, beta_end = 1e-4, 0.02  # simple linear schedule\n",
        "betas = torch.linspace(beta_start, beta_end, T)\n",
        "alphas = 1.0 - betas\n",
        "alpha_bars = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "def q_sample(x0, t, noise=None):\n",
        "    \"\"\"Sample x_t given x0 in closed form:\n",
        "    x_t = sqrt(alpha_bar_t) * x0 + sqrt(1 - alpha_bar_t) * epsilon\n",
        "    \"\"\"\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x0)\n",
        "    sqrt_ab = torch.sqrt(alpha_bars[t]).to(x0.device)\n",
        "    sqrt_one_minus_ab = torch.sqrt(1.0 - alpha_bars[t]).to(x0.device)\n",
        "    return sqrt_ab * x0 + sqrt_one_minus_ab * noise, noise\n"
      ],
      "id": "SEV5vAomyf3p"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(sci_mode=False)\n",
        "print(betas)"
      ],
      "metadata": {
        "id": "SHyqmV2F6ekh",
        "collapsed": true
      },
      "id": "SHyqmV2F6ekh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl7YgCVtyf3q"
      },
      "source": [
        "### Visualize the forward process\n",
        "\n",
        "We'll show the histogram of $x_t$ at several time steps to see the distribution **wash out** towards a standard Gaussian.\n"
      ],
      "id": "Zl7YgCVtyf3q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5Ez2rhcyf3q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Visualize forward diffusion histograms at selected steps\n",
        "xs = torch.from_numpy(x0).unsqueeze(1)  # shape [N,1]\n",
        "steps = [0, 10, 50, 100, 150, 200, 250, 299]  # (0 indexes t=1 as 0 in code; T-1 is the last index)\n",
        "\n",
        "for s in steps:\n",
        "    if s == 0:\n",
        "        xt = xs.clone()\n",
        "        data = xt.squeeze(1).numpy()\n",
        "        plot_hist(data, f\"x_0 (t=0)\")\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            xt, _ = q_sample(xs, s-1)\n",
        "            data = xt.squeeze(1).numpy()\n",
        "            plot_hist(data, f\"x_{s} (forward diffusion step {s})\")\n"
      ],
      "id": "h5Ez2rhcyf3q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "102mNnqdyf3r"
      },
      "source": [
        "## What data do we need for **reverse** diffusion?\n",
        "\n",
        "In DDPM training, the model learns to predict the injected noise $\\epsilon$.  \n",
        "To train it, we **collect tuples** $(x_t, t, \\epsilon)$ generated from:\n",
        "\n",
        "$$ x_t = \\sqrt{\\bar\\alpha_t}\\,x_0 + \\sqrt{1-\\bar\\alpha_t}\\,\\epsilon,\\ \\ \\epsilon\\sim\\mathcal{N}(0,1) $$\n",
        "\n",
        "We will synthesize a dataset of such tuples from our 1D data $x_0$.\n"
      ],
      "id": "102mNnqdyf3r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1wec0ifyf3r"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Build a synthetic training set of (x_t, t, epsilon)\n",
        "N_train = 20000\n",
        "x0_train = torch.from_numpy(np.random.choice(x0, size=N_train, replace=True)).float().unsqueeze(1)\n",
        "t_train = torch.randint(low=0, high=T, size=(N_train,))\n",
        "eps = torch.randn_like(x0_train)\n",
        "\n",
        "# closed-form sample of x_t and record epsilon used\n",
        "sqrt_ab = torch.sqrt(alpha_bars[t_train]).unsqueeze(1)\n",
        "sqrt_one_minus_ab = torch.sqrt(1.0 - alpha_bars[t_train]).unsqueeze(1)\n",
        "x_t_train = sqrt_ab * x0_train + sqrt_one_minus_ab * eps\n",
        "\n",
        "# peek at a few samples\n",
        "for i in range(3):\n",
        "    print(\"t={}, x_t={:.3f}, epsilon={:.3f}\".format(int(t_train[i]), float(x_t_train[i]), float(eps[i])))\n"
      ],
      "id": "O1wec0ifyf3r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write out the training data that will be used for training the reverse diffusion process into a data frame."
      ],
      "metadata": {
        "id": "b6Z6nWdg-DcI"
      },
      "id": "b6Z6nWdg-DcI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60f1344a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert tensors to numpy arrays and remove the extra dimension\n",
        "x_t_np = x_t_train.squeeze(1).numpy()\n",
        "t_np = t_train.numpy()\n",
        "eps_np = eps.squeeze(1).numpy()\n",
        "\n",
        "# Create a dictionary with the data\n",
        "training_data = {\n",
        "    't': t_np,\n",
        "    'x_t': x_t_np,\n",
        "    'epsilon': eps_np\n",
        "}\n",
        "\n",
        "# Create the DataFrame\n",
        "df_train = pd.DataFrame(training_data)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "display(df_train.head())"
      ],
      "id": "60f1344a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U6s68Fsyf3r"
      },
      "source": [
        "## A tiny network to predict $\\epsilon_\\theta(x_t, t)$\n",
        "\n",
        "We will train a very small MLP on 1D inputs to predict the noise $\\epsilon$ from $(x_t, t)$.  \n",
        "We embed $t$ with a **sinusoidal embedding** (like positional encodings).\n",
        "The training loss is $(\\mathbb{E}\\|\\epsilon - \\epsilon_\\theta(x_t,t)\\|^2)$.\n"
      ],
      "id": "8U6s68Fsyf3r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why sinusoidal embedding for time $t$?\n",
        "\n",
        "Instead of feeding the raw timestep\n",
        "$t$ (an integer like 137) into the network, we map $t$ to a vector made of sines and cosines at many frequencies:\n",
        "\n",
        "$ PE(t,2i) = sin(t/10000^{2i/d}) $,\n",
        "$ PE(t,2i+1) = cos(t/10000^{2i/d}) $,\n",
        "$(i=0,â€¦,d/2-1)$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This idea comes from the Transformer's positional encoding: each dimension is a sinusoid with a different wavelength, which helps the model infer relative offsets (how far apart two positions are) and even extrapolate to positions it never saw, because the pattern is analytic (not looked up).\n",
        "\n",
        "In diffusion models we need the network to â€œknowâ€ the noise level / time step it is denoising at. So we encode $t$ with these $sin/cos$ features and feed that vector into the MLP. we turn a single number $t$ into a rich, multi-frequency signal the network can use to condition its predictions.\n"
      ],
      "metadata": {
        "id": "I4Yt-qPsB8xi"
      },
      "id": "I4Yt-qPsB8xi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "433d5016"
      },
      "source": [
        "### Neural network that predicts the noise ($\\epsilon$).\n",
        "\n",
        "1.  **`TimeEmbedding` Class**:\n",
        "    *   This class creates a sinusoidal embedding for the time step $t$. It takes an integer time step and transforms it into a vector of sine and cosine values at different frequencies. This helps the network understand the notion of time and how far along the diffusion process it is.\n",
        "\n",
        "2.  **`EpsilonNet` Class**:\n",
        "    *   This is the main neural network model.\n",
        "    *   It uses the `TimeEmbedding` class to process the time step $t$.\n",
        "    *   The network itself is a simple Multi-Layer Perceptron (MLP) with two hidden layers and SiLU activation functions.\n",
        "    *   The input to the MLP is the concatenation of the current noisy data point $x_t$ and the time embedding vector.\n",
        "    *   The output of the MLP is a single value, which is the predicted noise $\\epsilon$.\n",
        "\n",
        "3.  **Model Initialization and Optimizer**:\n",
        "    *   `model = EpsilonNet()`: Creates an instance of the `EpsilonNet`.\n",
        "    *   `opt = optim.AdamW(model.parameters(), lr=2e-3)`: Sets up the AdamW optimizer to train the model, with a learning rate of 0.002.\n",
        "\n",
        "4.  **`batch_iter` Function**:\n",
        "    *   This function is a simple data loader. It randomly samples batches of `x_t`, `t`, and `eps` from the training data `x_t_train`, `t_train`, and `eps`.\n",
        "\n",
        "5.  **Training Loop**:\n",
        "    *   The code runs a training loop for 2000 steps.\n",
        "    *   In each step:\n",
        "        *   `batch_iter()`: Gets a batch of training data.\n",
        "        *   `model(x_b, t_b)`: Feeds the noisy data and time steps into the model to get the predicted noise.\n",
        "        *   `F.mse_loss(pred, e_b)`: Calculates the Mean Squared Error (MSE) between the predicted noise and the actual noise. This is the loss function the model tries to minimize.\n",
        "        *   `opt.zero_grad(); loss.backward(); opt.step()`: Performs a standard PyTorch training step: clears previous gradients, computes gradients of the loss with respect to model parameters, and updates the model parameters using the optimizer.\n",
        "        *   The loss is printed every 200 steps to monitor training progress.\n",
        "\n",
        "6.  **Plotting Loss**:\n",
        "    *   After training, the code plots the recorded loss values to visualize how the training progressed over time.\n",
        "\n",
        "In essence, this code defines a neural network that learns to predict the noise added at any given time step of the forward diffusion process, which is a crucial component for the reverse diffusion (sampling) process.\n",
        "\n",
        "\n",
        "The **SiLU (Sigmoid Linear Unit)** activation function is a type of activation function used in neural networks. It's defined as:\n",
        "\n",
        "$ SiLU(x) = x * \\sigma(x) $\n",
        "\n",
        "where $\\sigma(x)$ is the sigmoid function.\n",
        "\n",
        "Here's why it's used:\n",
        "\n",
        "1. Smoothness: Unlike the ReLU function, SiLU is smooth everywhere, which can help with optimization during training.\n",
        "2. Non-monotonic: It's not monotonic, which allows it to handle more complex patterns in the data.\n",
        "3. Performance: It has been shown to perform well in various neural network architectures, sometimes outperforming other common activation functions like ReLU and Swish.\n",
        "\n",
        "In simple terms, it's a function that introduces non-linearity into the network, allowing it to learn complex relationships in the data."
      ],
      "id": "433d5016"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyFChKj3yf3r"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Sinusoidal time embedding for 1D\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, dim=32, max_period=10000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_period = max_period\n",
        "    def forward(self, t):  # t: [B] integer 0..T-1\n",
        "        device = t.device\n",
        "        half = self.dim // 2\n",
        "        freqs = torch.exp(-math.log(self.max_period) * torch.arange(half, device=device) / half)\n",
        "        args = t.float().unsqueeze(1) * freqs.unsqueeze(0)\n",
        "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=1)\n",
        "        if self.dim % 2 == 1:\n",
        "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n",
        "        return emb\n",
        "\n",
        "class EpsilonNet(nn.Module):\n",
        "    def __init__(self, t_dim=32, hidden=64):\n",
        "        super().__init__()\n",
        "        self.tembed = TimeEmbedding(dim=t_dim)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(1 + t_dim, hidden),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden, 1),\n",
        "        )\n",
        "    def forward(self, x_t, t):\n",
        "        te = self.tembed(t)\n",
        "        h = torch.cat([x_t, te], dim=1)\n",
        "        return self.net(h)\n",
        "\n",
        "model = EpsilonNet()\n",
        "opt = optim.AdamW(model.parameters(), lr=2e-3)\n",
        "\n",
        "# simple training loop\n",
        "def batch_iter(bs=512):\n",
        "    n = x_t_train.shape[0]\n",
        "    idx = torch.randint(0, n, (bs,))\n",
        "    return x_t_train[idx], t_train[idx], eps[idx]\n",
        "\n",
        "losses = []\n",
        "for step in range(2000):  # quick train; increase for slightly better results\n",
        "    x_b, t_b, e_b = batch_iter()\n",
        "    pred = model(x_b, t_b)\n",
        "    loss = F.mse_loss(pred, e_b)\n",
        "    opt.zero_grad(); loss.backward(); opt.step()\n",
        "    if (step+1) % 200 == 0:\n",
        "        losses.append(float(loss.detach().cpu()))\n",
        "        print(\"step {}: loss {:.6f}\".format(step+1, loss.item()))\n",
        "\n",
        "# plot training loss\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.title(\"Training loss (every 200 steps)\")\n",
        "plt.xlabel(\"checkpoint\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.show()\n"
      ],
      "id": "qyFChKj3yf3r"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEu1toBzyf3s"
      },
      "source": [
        "##  Reverse diffusion (sampling) with the learned $\\epsilon_\\theta$\n",
        "\n",
        "In DDPM with $\\epsilon$ prediction, the mean of the reverse transition is:\n",
        "\n",
        "$$\n",
        "\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}\\!\\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t}}\\ \\epsilon_\\theta(x_t,t)\\right)\n",
        "$$\n",
        "\n",
        "We'll step from\n",
        "\n",
        "$x_T\\sim\\mathcal{N}(0,1)$ **down to** $x_0$\n",
        " using this mean and the known variance.\n"
      ],
      "id": "oEu1toBzyf3s"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLZOu3wbyf3s"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# One reverse step using predicted epsilon\n",
        "def p_sample_step(x_t, t_idx):\n",
        "    t = torch.full((x_t.shape[0],), t_idx, dtype=torch.long)\n",
        "    eps_theta = model(x_t, t)\n",
        "    a_t = (1.0 - betas[t_idx]).item()\n",
        "    ab_t = alpha_bars[t_idx].item()\n",
        "    coef = (1.0 - a_t) / math.sqrt(1.0 - ab_t)\n",
        "    mean = (1.0 / math.sqrt(a_t)) * (x_t - coef * eps_theta)\n",
        "    if t_idx > 0:\n",
        "        var = betas[t_idx].item()\n",
        "        noise = torch.randn_like(x_t)\n",
        "        x_prev = mean + math.sqrt(var) * noise\n",
        "    else:\n",
        "        x_prev = mean  # at t=0, set variance to 0 for final sample\n",
        "    return x_prev\n",
        "\n",
        "# Run the reverse process\n",
        "with torch.no_grad():\n",
        "    n_samp = 20000\n",
        "    xT = torch.randn(n_samp, 1)  # start from standard normal\n",
        "    x = xT\n",
        "    for t_idx in reversed(range(T)):\n",
        "        x = p_sample_step(x, t_idx)\n",
        "    x_gen = x.squeeze(1).cpu().numpy()\n",
        "\n",
        "plot_hist(x_gen, \"Generated samples after reverse diffusion\")\n"
      ],
      "id": "zLZOu3wbyf3s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdpr10UWyf3s"
      },
      "source": [
        "## Compare original vs. generated\n",
        "\n",
        "Let's overlay histograms (separate plots) and then visually compare the shape of the learned distribution to the original quadâ€‘modal data.\n"
      ],
      "id": "zdpr10UWyf3s"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec00KwO2yf3s"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "plot_hist(x0, \"Original data distribution (quad-modal) â€” reference\")\n",
        "plot_hist(x_gen, \"Generated distribution â€” reverse diffusion\")\n"
      ],
      "id": "Ec00KwO2yf3s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EknwgP5yf3t"
      },
      "source": [
        "## Recap â€” What we *collected* during forward diffusion\n",
        "\n",
        "To enable reverse diffusion (learning), we created tuples:\n",
        "- $x_0 \\sim q_{\\text{data}}$ (our mixtureâ€‘ofâ€‘Gaussians sample)\n",
        "- Sample $t \\sim \\{0,\\dots,T-1\\}$\n",
        "- Sample $\\epsilon \\sim \\mathcal{N}(0,1)$\n",
        "- Compute $x_t = \\sqrt{\\bar\\alpha_t}\\,x_0 + \\sqrt{1-\\bar\\alpha_t}\\,\\epsilon$\n",
        "\n",
        "**Training data:** $(x_t, t, \\epsilon)$.  \n",
        "**Model objective:** minimize $\\mathbb{E}\\|\\epsilon - \\epsilon_\\theta(x_t,t)\\|^2$.  \n",
        "**Sampling:** start from $x_T \\sim \\mathcal{N}(0,1)$, apply reverse steps using $\\mu_\\theta(x_t,t)$.\n"
      ],
      "id": "6EknwgP5yf3t"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOosTbZFyf3t"
      },
      "source": [
        "---\n",
        "\n",
        "### Appendix: Notes & tips\n",
        "- Increase **training steps** if the generated histogram looks too smooth or too noisy. Even in 1D, a little more training helps.\n",
        "\n",
        "**Further reading (primary sources):**\n",
        "- DDPM: https://arxiv.org/abs/2006.11239  \n",
        "- Sohlâ€‘Dickstein et al. (2015): https://proceedings.mlr.press/v37/sohl-dickstein15.html  \n",
        "- Improved DDPM (cosine schedule): https://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf  \n",
        "- Scoreâ€‘based SDEs: https://arxiv.org/abs/2011.13456\n"
      ],
      "id": "QOosTbZFyf3t"
    }
  ]
}